This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-05-07T02:24:16.582Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
examples/
  aginews-ai-newsletter/
    README.md
  ai-podcast-generator/
    README.md
  blog-articles/
    amazon-price-tracking/
      notebook.ipynb
      notebook.md
    deploying_web_scrapers/
      notebook.ipynb
      notebook.md
    github-actions-tutorial/
      notebook.ipynb
      notebook.md
    mastering-map-endpoint/
      mastering-map-endpoint.md
    mastering-scrape-endpoint/
      mastering-scrape-endpoint.md
  claude_stock_analyzer/
    claude_stock_analyzer.py
  claude-3.7-stock-analyzer/
    claude-3.7-stock-analyzer.py
  claude3.7-web-crawler/
    claude3.7-web-crawler.py
  claude3.7-web-extractor/
    claude-3.7-web-extractor.py
  contradiction_testing/
    web-data-contradiction-testing-using-llms.mdx
  crm_lead_enrichment/
    crm_lead_enrichment.py
  deep-research-apartment-finder/
    .env.example
    apartment_finder.py
    README.md
    requirements.txt
  deepseek-v3-company-researcher/
    .gitignore
    deepseek-v3-extract.py
    README.md
    requirements.txt
  deepseek-v3-crawler/
    .gitignore
    deepseek-v3-crawler.py
    README.md
    requirements.txt
  find_internal_link_opportunites/
    find_internal_link_opportunites.ipynb
  full_example_apps/
    README.md
  gemini-2.0-crawler/
    gemini-2.0-crawler.py
  gemini-2.0-web-extractor/
    gemini-2.0-web-extractor.py
  gemini-2.5-crawler/
    .env.example
    gemini-2.5-crawler.py
    README.md
    requirements.txt
  gemini-2.5-web-extractor/
    .env.example
    .gitignore
    gemini-2.5-web-extractor.py
    README.md
    requirements.txt
  gemini-github-analyzer/
    gemini-github-analyzer.py
  gpt-4.1-company-researcher/
    .env.example
    .gitignore
    gpt-4.1-company-researcher.py
    README.md
    requirements.txt
  gpt-4.1-web-crawler/
    .env.example
    .gitignore
    gpt-4.1-web-crawler.py
    README.md
    requirements.txt
  gpt-4.5-web-crawler/
    gpt-4.5-crawler.py
  grok_web_crawler/
    grok_web_crawler.py
  groq_web_crawler/
    groq_website_analyzer.py
    requirements.txt
  hacker_news_scraper/
    bs4_scraper.py
    firecrawl_scraper.py
    requirements.txt
  haiku_web_crawler/
    haiku_web_crawler.py
  internal_link_assistant/
    internal_link_assistant.py
  job-resource-analyzer/
    job-resources-analyzer.py
  kubernetes/
    cluster-install/
      api.yaml
      configmap.yaml
      playwright-service.yaml
      README.md
      redis.yaml
      secret.yaml
      worker.yaml
  llama-4-maverick-web-crawler/
    .env.example
    .gitignore
    llama4-maverick-web-crawler.py
    README.md
    requirements.txt
  mistral-small-3.1-crawler/
    mistral-small-3.1-crawler.py
  mistral-small-3.1-extractor/
    mistral-small-3.1-extractor.py
  o1_job_recommender/
    o1_job_recommender.py
  o1_web_crawler/
    o1_web_crawler.py
  o1_web_extractor/
    o1_web_extractor.py
  o3-mini_company_researcher/
    o3-mini_company_researcher.py
  o3-mini_web_crawler/
    o3-mini_web_crawler.py
  o3-mini-deal-finder/
    o3-mini-deal-finder.py
  o3-web-crawler/
    .env.example
    .gitignore
    o3-web-crawler.py
    README.md
    requirements.txt
  o4-mini-web-crawler/
    .env.example
    .gitignore
    o4-mini-web-crawler.py
    README.md
    requirements.txt
  openai_swarm_firecrawl/
    .env.example
    main.py
    README.md
    requirements.txt
  openai_swarm_firecrawl_web_extractor/
    .env.example
    main.py
    requirements.txt
  openai-realtime-firecrawl/
    README.md
  R1_company_researcher/
    r1_company_researcher.py
  R1_web_crawler/
    R1_web_crawler.py
  sales_web_crawler/
    .env.example
    app.py
    requirements.txt
  scrape_and_analyze_airbnb_data_e2b/
    .env.template
    .prettierignore
    airbnb_listings.json
    codeInterpreter.ts
    index.ts
    model.ts
    package.json
    prettier.config.mjs
    README.md
    scraping.ts
  simple_web_data_extraction_with_claude/
    simple_web_data_extraction_with_claude.ipynb
  sonnet_web_crawler/
    sonnet_web_crawler.py
  turning_docs_into_api_specs/
    turning_docs_into_api_specs.py
  visualize_website_topics_e2b/
    claude-visualize-website-topics.ipynb
  web_data_extraction/
    web-data-extraction-using-llms.mdx
  web_data_rag_with_llama3/
    web-data-rag--with-llama3.mdx
  website_qa_with_gemini_caching/
    website_qa_with_gemini_caching.ipynb
    website_qa_with_gemini_flash_caching.ipynb

================================================================
Repository Files
================================================================

================
File: examples/aginews-ai-newsletter/README.md
================
# AGI News ‚ú®
AGI News is a daily AI newsletter that's completely sourced by autonomous AI agents. It is live at [https://www.aginews.io/](https://www.aginews.io/)

Here is a link to the repo:

[https://github.com/ericciarla/aginews](https://github.com/ericciarla/aginews)

================
File: examples/ai-podcast-generator/README.md
================
# Generate AI podcasts based on real time news üéôÔ∏è

This example crawls the web for interesting news stories then records a podcast with your own voice.

Here is a link to the repo:

[https://github.com/ericciarla/aginews-podcast](https://github.com/ericciarla/aginews-podcast)

================
File: examples/blog-articles/amazon-price-tracking/notebook.ipynb
================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Build an Automated Amazon Price Tracking Tool in Python For Free\n",
    "## That sends alerts to your phone and keeps price history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Shall We Build in This Tutorial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot to be said about the psychology of discounts. For example, buying a discounted item even though we don't need it isn't saving money at all. That's walking into the oldest trap sellers use to increase sales. However, there are legitimate cases where waiting for a price drop on items you actually need makes perfect sense.\n",
    "\n",
    "The challenge is that e-commerce websites run flash sales and temporary discounts constantly, but these deals often disappear as quickly as they appear. Missing these brief windows of opportunity can be frustrating.\n",
    "\n",
    "That's where automation comes in. In this guide, we'll build a Python application that monitors product prices across any e-commerce website and instantly notifies you when prices drop on items you're actually interested in. Here is a sneak peak of the app:\n",
    "\n",
    "![](images/sneak-peek.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The app looks pretty dull, doesn't it? Well, no worries because it is fully functional:\n",
    "- It has a minimalistic UI to add or remove products from the tracker\n",
    "- A simple dashboard to display price history for each product\n",
    "- Controls for setting the price drop threshold in percentages\n",
    "- A notification system that sends Discord alerts when a tracked item's price drops\n",
    "- A scheduling system that updates the product prices on an interval you specify\n",
    "- Runs for free for as long as you want\n",
    "\n",
    "Even though the title says \"Amazon price tracker\" (full disclosure: I was forced to write that for SEO purposes), the app will work for any e-commerce website you can imagine (except Ebay, for some reason). \n",
    "\n",
    "So, let's get started building this Amazon price tracker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Toolstack We Will Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The app's code will be written fully in Python and its libraries:\n",
    "\n",
    "- [Streamlit](streamlit.io) for the UI\n",
    "- [Firecrawl](firecrawl.dev) for AI-based scraping of e-commerce websites\n",
    "- [SQLAlchemy](https://www.sqlalchemy.org/) for database management\n",
    "\n",
    "Apart from Python, we will use these platforms:\n",
    "\n",
    "- Discord for notifications\n",
    "- GitHub for hosting the app\n",
    "- GitHub Actions for running the app on a schedule\n",
    "- Supabase for hosting a free Postgres database instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Amazon Price Tracker App Step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this project involves multiple components working together, we'll take a top-down approach rather than building individual pieces first. This approach makes it easier to understand how everything fits together, since we'll introduce each tool only when it's needed. The benefits of this strategy will become clear as we progress through the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a dedicated environment on our machines to work on the project:\n",
    "\n",
    "```bash\n",
    "mkdir automated-price-tracker\n",
    "cd automated-price-tracker\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "These commands create a working directory and activate a virtual environment. Next, create a new script called `ui.py` for designing the user interface with Streamlit.\n",
    "\n",
    "```bash\n",
    "touch ui.py\n",
    "```\n",
    "\n",
    "Then, install Streamlit:\n",
    "\n",
    "```bash\n",
    "pip install streamlit\n",
    "```\n",
    "\n",
    "Next, create a `requirements.txt` file and add Streamlit as the first dependency:\n",
    "\n",
    "```bash\n",
    "touch requirements.txt\n",
    "echo \"streamlit\" >> requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the code will be hosted on GitHub, we need to initialize Git and create a `.gitignore` file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "git init\n",
    "touch .gitignore\n",
    "echo \".venv\" >> .gitignore  # Add the virtual env folder\n",
    "git commit -m \"Initial commit\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Add a sidebar to the UI for product input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the final product one more time:\n",
    "\n",
    "![](images/sneak-peek.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has two sections: the sidebar and the main dashboard. Since the first thing you do when launching this app is adding products, we will start building the sidebar first. Open `ui.py` and paste the following code:\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "\n",
    "# Set up sidebar\n",
    "with st.sidebar:\n",
    "    st.title(\"Add New Product\")\n",
    "    product_url = st.text_input(\"Product URL\")\n",
    "    add_button = st.button(\"Add Product\")\n",
    "\n",
    "# Main content\n",
    "st.title(\"Price Tracker Dashboard\")\n",
    "st.markdown(\"## Tracked Products\")\n",
    "```\n",
    "\n",
    "The code snippet above sets up a basic Streamlit web application with two main sections. In the sidebar, it creates a form for adding new products with a text input field for the product URL and an \"Add Product\" button. The main content area contains a dashboard title and a section header for tracked products. The code uses Streamlit's `st.sidebar` context manager to create the sidebar layout and basic Streamlit components like `st.title`, `st.text_input`, and `st.button` to build the user interface elements.\n",
    "\n",
    "To see how this app looks like, run the following command:\n",
    "\n",
    "```bash\n",
    "streamlit run ui.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add a commit to save our progress:\n",
    "\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Add a sidebar to the basic UI\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Add a feature to check if input URL is valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we want to add some restrictions to the input field like checking if the passed URL is valid. For this, create a new file called `utils.py` where we write additional utility functions for our app:\n",
    "\n",
    "```bash\n",
    "touch utils.py\n",
    "```\n",
    "\n",
    "Inside the script, paste following code:\n",
    "\n",
    "```bash\n",
    "# utils.py\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "\n",
    "def is_valid_url(url: str) -> bool:\n",
    "    try:\n",
    "        # Parse the URL\n",
    "        result = urlparse(url)\n",
    "\n",
    "        # Check if scheme and netloc are present\n",
    "        if not all([result.scheme, result.netloc]):\n",
    "            return False\n",
    "\n",
    "        # Check if scheme is http or https\n",
    "        if result.scheme not in [\"http\", \"https\"]:\n",
    "            return False\n",
    "\n",
    "        # Basic regex pattern for domain validation\n",
    "        domain_pattern = (\n",
    "            r\"^[a-zA-Z0-9]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z]{2,})+$\"\n",
    "        )\n",
    "        if not re.match(domain_pattern, result.netloc):\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception:\n",
    "        return False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function `is_valid_url()` validates URLs by checking several criteria:\n",
    "\n",
    "1. It verifies the URL has both a scheme (`http`/`https`) and domain name\n",
    "2. It ensures the scheme is specifically `http` or `https`\n",
    "3. It validates the domain name format using regex to check for valid characters and TLD\n",
    "4. It returns True only if all checks pass, False otherwise\n",
    "\n",
    "Let's use this function in our `ui.py` file. Here is the modified code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import streamlit as st\n",
    "from utils import is_valid_url\n",
    "\n",
    "\n",
    "# Set up sidebar\n",
    "with st.sidebar:\n",
    "    st.title(\"Add New Product\")\n",
    "    product_url = st.text_input(\"Product URL\")\n",
    "    add_button = st.button(\"Add Product\")\n",
    "\n",
    "    if add_button:\n",
    "        if not product_url:\n",
    "            st.error(\"Please enter a product URL\")\n",
    "        elif not is_valid_url(product_url):\n",
    "            st.error(\"Please enter a valid URL\")\n",
    "        else:\n",
    "            st.success(\"Product is now being tracked!\")\n",
    "\n",
    "# Main content\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what's new:\n",
    "\n",
    "1. We added URL validation using the `is_valid_url()` function from `utils.py`\n",
    "2. When the button is clicked, we perform validation:\n",
    "   - Check if URL is empty\n",
    "   - Validate URL format using `is_valid_url()`\n",
    "3. User feedback is provided through error/success messages:\n",
    "   - Error shown for empty URL\n",
    "   - Error shown for invalid URL format \n",
    "   - Success message when URL passes validation\n",
    "\n",
    "Rerun the Streamlit app again and see if our validation works. Then, return to your terminal to commit the changes we've made:\n",
    "\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Add a feature to check URL validity\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Scrape the input URL for product details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a valid URL is entered and the add button is clicked, we need to implement product scraping functionality instead of just showing a success message. The system should:\n",
    "\n",
    "1. Immediately scrape the product URL to extract key details:\n",
    "   - Product name\n",
    "   - Current price\n",
    "   - Main product image\n",
    "   - Brand name\n",
    "   - Other relevant attributes\n",
    "\n",
    "2. Store these details in a database to enable:\n",
    "   - Regular price monitoring\n",
    "   - Historical price tracking\n",
    "   - Price change alerts\n",
    "   - Product status updates\n",
    "\n",
    "For the scraper, we will use [Firecrawl](firecrawl.dev), an AI-based scraping API for extracting webpage data without HTML parsing. This solution provides several advantages:\n",
    "\n",
    "1. No website HTML code analysis required for element selection\n",
    "2. Resilient to HTML structure changes through AI-based element detection\n",
    "3. Universal compatibility with product webpages due to structure-agnostic approach \n",
    "4. Reliable website blocker bypass via robust API infrastructure\n",
    "\n",
    "First, create a new file called `scraper.py`:\n",
    "\n",
    "```bash\n",
    "touch scraper.py\n",
    "```\n",
    "\n",
    "Then, install these three libraries:\n",
    "\n",
    "```bash\n",
    "pip install firecrawl-py pydantic python-dotenv\n",
    "echo \"firecrawl-py\\npydantic\\npython-dotenv\\n\" >> requirements.txt  # Add them to dependencies\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`firecrawl-py` is the Python SDK for Firecrawl scraping engine, `pydantic` is a data validation library that helps enforce data types and structure through Python class definitions, and `python-dotenv` is a library that loads environment variables from a `.env` file into your Python application.\n",
    "\n",
    "With that said, head over to the Firecrawl website and [sign up for a free account](https://www.firecrawl.dev/) (the free plan will work fine). You will be given an API key, which you should copy. \n",
    "\n",
    "Then, create a `.env` file in your terminal and add the API key as an environment variable:\n",
    "\n",
    "```bash\n",
    "touch .env\n",
    "echo \"FIRECRAWL_API_KEY='YOUR-API-KEY-HERE' >> .env\"\n",
    "echo \".env\" >> .gitignore  # Ignore .env files in Git\n",
    "```\n",
    "\n",
    "The `.env` file is used to securely store sensitive configuration values like API keys that shouldn't be committed to version control. By storing the Firecrawl API key in `.env` and adding it to `.gitignore`, we ensure it stays private while still being accessible to our application code. This is a security best practice to avoid exposing credentials in source control.\n",
    "\n",
    "Now, we can start writing the `scraper.py`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from firecrawl import FirecrawlApp\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "app = FirecrawlApp()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `load_dotenv()` function reads the `.env` file you have in your working directory and loads the environment variables inside, including the Firecrawl API key. When you create an instance of `FirecrawlApp` class, the API key is automatically detected to establish a connection between your script and the scraping engine in the form of the `app` variable.\n",
    "\n",
    "Now, we create a Pydantic class (usually called a model) that defines the details we want to scrape from each product:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Product(BaseModel):\n",
    "    \"\"\"Schema for creating a new product\"\"\"\n",
    "\n",
    "    url: str = Field(description=\"The URL of the product\")\n",
    "    name: str = Field(description=\"The product name/title\")\n",
    "    price: float = Field(description=\"The current price of the product\")\n",
    "    currency: str = Field(description=\"Currency code (USD, EUR, etc)\")\n",
    "    main_image_url: str = Field(description=\"The URL of the main image of the product\")\n",
    "```\n",
    "\n",
    "Pydantic models may be completely new to you, so let's break down the `Product` model:\n",
    "\n",
    "- The `url` field stores the product page URL we want to track\n",
    "- The `name` field stores the product title/name that will be scraped\n",
    "- The `price` field stores the current price as a float number\n",
    "- The `currency` field stores the 3-letter currency code (e.g. USD, EUR)\n",
    "- The `main_image_url` field stores the URL of the product's main image\n",
    "\n",
    "Each field is typed and has a description that documents its purpose. The `Field` class from Pydantic allows us to add metadata like descriptions to each field. These descriptions are especially important for Firecrawl since it uses them to automatically locate the relevant HTML elements containing the data we want. \n",
    "\n",
    "Now, let's create a function to call the engine to scrape URL's based on the schema above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def scrape_product(url: str):\n",
    "    extracted_data = app.scrape_url(\n",
    "        url,\n",
    "        params={\n",
    "            \"formats\": [\"extract\"],\n",
    "            \"extract\": {\"schema\": Product.model_json_schema()},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Add the scraping date to the extracted data\n",
    "    extracted_data[\"extract\"][\"timestamp\"] = datetime.utcnow()\n",
    "\n",
    "    return extracted_data[\"extract\"]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    product = \"https://www.amazon.com/gp/product/B002U21ZZK/\"\n",
    "\n",
    "    print(scrape_product(product))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above defines a function called `scrape_product` that takes a URL as input and uses it to scrape product information. Here's how it works:\n",
    "\n",
    "The function calls `app.scrape_url` with two parameters:\n",
    "1. The product URL to scrape\n",
    "2. A params dictionary that configures the scraping:\n",
    "   - It specifies we want to use the \"extract\" format\n",
    "   - It provides our `Product` Pydantic model schema as the extraction template as a JSON object\n",
    "\n",
    "The scraper will attempt to find and extract data that matches our Product schema fields - the URL, name, price, currency, and image URL.\n",
    "\n",
    "The function returns just the \"extract\" portion of the scraped data, which contains the structured product information. `extract` returns a dictionary to which we add the date of the scraping as it will be important later on.\n",
    "\n",
    "Let's test the script by running it:\n",
    "\n",
    "```bash\n",
    "python scraper.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get an output like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'url': 'https://www.amazon.com/dp/B002U21ZZK', \n",
    "    'name': 'MOVA Globe Earth with Clouds 4.5\"', \n",
    "    'price': 212, \n",
    "    'currency': 'USD', \n",
    "    'main_image_url': 'https://m.media-amazon.com/images/I/41bQ3Y58y3L._AC_.jpg', \n",
    "    'timestamp': '2024-12-05 13-20'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that a [MOVA Globe](https://www.amazon.com/dp/B002U21ZZK) costs $212 USD on Amazon at the time of writing this article. You can test the script for any other website that contains the information we are looking (except Ebay):\n",
    "\n",
    "- Price\n",
    "- Product name/title\n",
    "- Main image URL\n",
    "\n",
    "One key advantage of using Firecrawl is that it returns data in a consistent dictionary format across all websites. Unlike HTML-based scrapers like BeautifulSoup or Scrapy which require custom code for each site and can break when website layouts change, Firecrawl uses AI to understand and extract the requested data fields regardless of the underlying HTML structure. \n",
    "\n",
    "Finish this step by committing the new changes to Git:\n",
    "\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Implement a Firecrawl scraper for products\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Storing new products in a PostgreSQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to check product prices regularly, we need to have an online database. In this case, Postgres is the best option since it's reliable, scalable, and has great support for storing time-series data like price histories.\n",
    "\n",
    "There are many platforms for hosting Postgres instances but the one I find the easiest and fastest to set up is Supabase. So, please head over to [the Supabase website](https://supabase.com) and create your free account. During the sign-up process, you will be given a password, which you should save somewhere safe on your machine. \n",
    "\n",
    "\n",
    "Then, in a few minutes, your free Postgres instance comes online. To connect to this instance, click on Home in the left sidebar and then, \"Connect\":\n",
    "\n",
    "![](images/supabase_connect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be shown your database connection string with a placeholder for the password you copied. You should paste this string in your `.env` file with your password added to the `.env` file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "echo POSTGRES_URL=\"THE-SUPABASE-URL-STRING-WITH-YOUR-PASSWORD-ADDED\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the easiest way to interact with this database is through SQLAlchemy. Let's install it:\n",
    "\n",
    "```bash\n",
    "pip install \"sqlalchemy==2.0.35\" psycopg2-binary\n",
    "echo \"psycopg2-binary\\nsqlalchemy==2.0.35\" >> requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: [SQLAlchemy](https://sqlalchemy.org) is a Python SQL toolkit and Object-Relational Mapping (ORM) library that lets us interact with databases using Python code instead of raw SQL. For our price tracking project, it provides essential features like database connection management, schema definition through Python classes, and efficient querying capabilities. This makes it much easier to store and retrieve product information and price histories in our Postgres database.\n",
    "\n",
    "After the installation, create a new `database.py` file for storing database-related functions:\n",
    "\n",
    "```bash\n",
    "touch database.py\n",
    "```\n",
    "\n",
    "Let's populate this script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sqlalchemy import create_engine, Column, String, Float, DateTime, ForeignKey\n",
    "from sqlalchemy.orm import sessionmaker, relationship, declarative_base\n",
    "from datetime import datetime\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Product(Base):\n",
    "    __tablename__ = \"products\"\n",
    "\n",
    "    url = Column(String, primary_key=True)\n",
    "    prices = relationship(\n",
    "        \"PriceHistory\", back_populates=\"product\", cascade=\"all, delete-orphan\"\n",
    "    )\n",
    "\n",
    "\n",
    "class PriceHistory(Base):\n",
    "    __tablename__ = \"price_histories\"\n",
    "\n",
    "    id = Column(String, primary_key=True)\n",
    "    product_url = Column(String, ForeignKey(\"products.url\"))\n",
    "    name = Column(String, nullable=False)\n",
    "    price = Column(Float, nullable=False)\n",
    "    currency = Column(String, nullable=False)\n",
    "    main_image_url = Column(String)\n",
    "    timestamp = Column(DateTime, nullable=False)\n",
    "    product = relationship(\"Product\", back_populates=\"prices\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The code above defines two SQLAlchemy models for our price tracking database:\n",
    "\n",
    "The Product model represents items we want to track, with the product URL as the primary key. It has a one-to-many relationship with price histories (which means each product in `products` can have multiple price history entry in `price_histories`).\n",
    "\n",
    "The `PriceHistory` model stores individual price points over time. Each record contains:\n",
    "- A unique ID as primary key\n",
    "- The product URL as a foreign key linking to the `Product`\n",
    "- The product name\n",
    "- The price value and currency\n",
    "- The main product image URL\n",
    "- A timestamp of when the price was recorded\n",
    "\n",
    "The relationship between `Product` and `PriceHistory` is bidirectional, allowing easy navigation between related records. The `cascade` setting ensures price histories are deleted when their product is deleted.\n",
    "\n",
    "These models provide the structure for storing and querying our price tracking data in a PostgreSQL database using SQLAlchemy's ORM capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a `Database` class with a singe `add_product` method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Database:\n",
    "    def __init__(self, connection_string):\n",
    "        self.engine = create_engine(connection_string)\n",
    "        Base.metadata.create_all(self.engine)\n",
    "        self.Session = sessionmaker(bind=self.engine)\n",
    "\n",
    "    def add_product(self, url):\n",
    "        session = self.Session()\n",
    "        try:\n",
    "            # Create the product entry\n",
    "            product = Product(url=url)\n",
    "            session.merge(product)  # merge will update if exists, insert if not\n",
    "            session.commit()\n",
    "        finally:\n",
    "            session.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `Database` class above provides core functionality for managing product data in our PostgreSQL database. It takes a connection string in its constructor to establish the database connection using SQLAlchemy.\n",
    "\n",
    "The `add_product` method allows us to store new product URLs in the database. It uses SQLAlchemy's `merge` functionality which intelligently handles both inserting new products and updating existing ones, preventing duplicate entries.\n",
    "\n",
    "The method carefully manages database sessions, ensuring proper resource cleanup by using `try`/`finally` blocks. This prevents resource leaks and maintains database connection stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this method inside the sidebar of our UI. Switch to `ui.py` and make the following adjustments:\n",
    "\n",
    "First, update the imports to load the Database class and initialize it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import streamlit as st\n",
    "\n",
    "from utils import is_valid_url\n",
    "from database import Database\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "with st.spinner(\"Loading database...\"):\n",
    "    db = Database(os.getenv(\"POSTGRES_URL\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code integrates the `Database` class into the Streamlit UI by importing required dependencies and establishing a database connection. The database URL is loaded securely from environment variables using `python-dotenv`. The `Database` class creates or updates the tables we specified in `database.py` after being initialized.\n",
    "\n",
    "The database initialization process is wrapped in a Streamlit spinner component to maintain responsiveness while establishing the connection. This provides visual feedback during the connection setup period, which typically requires a brief initialization time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, in the sidebar code, we only need to add a single line of code to add the product to the database if the URL is valid:\n",
    "\n",
    "```python\n",
    "# Set up sidebar\n",
    "with st.sidebar:\n",
    "    st.title(\"Add New Product\")\n",
    "    product_url = st.text_input(\"Product URL\")\n",
    "    add_button = st.button(\"Add Product\")\n",
    "\n",
    "    if add_button:\n",
    "        if not product_url:\n",
    "            st.error(\"Please enter a product URL\")\n",
    "        elif not is_valid_url(product_url):\n",
    "            st.error(\"Please enter a valid URL\")\n",
    "        else:\n",
    "            db.add_product(product_url)  # This is the new line\n",
    "            st.success(\"Product is now being tracked!\")\n",
    "```\n",
    "\n",
    "In the final `else` block that runs when the product URL is valid, we call the `add_product` method to store the product in the database.\n",
    "\n",
    "Let's commit everything:\n",
    "\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Add a Postgres database integration for tracking product URLs\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Storing price histories for new products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after the product is added to the `products` table, we want to add its details and its scraped price to the `price_histories` table. \n",
    "\n",
    "First, switch to `database.py` and add a new method for creating entries in the `PriceHistories` table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Database:\n",
    "    ...  # the rest of the class\n",
    "\n",
    "    def add_price(self, product_data):\n",
    "        session = self.Session()\n",
    "        try:\n",
    "            price_history = PriceHistory(\n",
    "                id=f\"{product_data['url']}_{product_data['timestamp']}\",\n",
    "                product_url=product_data[\"url\"],\n",
    "                name=product_data[\"name\"],\n",
    "                price=product_data[\"price\"],\n",
    "                currency=product_data[\"currency\"],\n",
    "                main_image_url=product_data[\"main_image_url\"],\n",
    "                timestamp=product_data[\"timestamp\"],\n",
    "            )\n",
    "            session.add(price_history)\n",
    "            session.commit()\n",
    "        finally:\n",
    "            session.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `add_price` method takes a dictionary containing product data (which is returned by our scraper) and creates a new entry in the `PriceHistory` table. The entry's ID is generated by combining the product URL with a timestamp. The method stores essential product information like name, price, currency, image URL, and the timestamp of when the price was recorded. It uses SQLAlchemy's session management to safely commit the new price history entry to the database.\n",
    "\n",
    "Now, we need to add this functionality to the sidebar as well. In `ui.py`, add a new import statement that loads the `scrape_product` function from `scraper.py`:\n",
    "\n",
    "```python\n",
    "...  # The rest of the imports\n",
    "from scraper import scrape_product\n",
    "```\n",
    "\n",
    "Then, update the `else` block in the sidebar again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with st.sidebar:\n",
    "    st.title(\"Add New Product\")\n",
    "    product_url = st.text_input(\"Product URL\")\n",
    "    add_button = st.button(\"Add Product\")\n",
    "\n",
    "    if add_button:\n",
    "        if not product_url:\n",
    "            st.error(\"Please enter a product URL\")\n",
    "        elif not is_valid_url(product_url):\n",
    "            st.error(\"Please enter a valid URL\")\n",
    "        else:\n",
    "            db.add_product(product_url)\n",
    "            with st.spinner(\"Added product to database. Scraping product data...\"):\n",
    "                product_data = scrape_product(product_url)\n",
    "                db.add_price(product_data)\n",
    "            st.success(\"Product is now being tracked!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when a user enters a product URL and clicks the \"Add Product\" button, several things happen:\n",
    "\n",
    "1. The URL is validated to ensure it's not empty and is properly formatted.\n",
    "2. If valid, the URL is added to the products table via `add_product()`.\n",
    "3. The product page is scraped immediately to get current price data.\n",
    "4. This initial price data is stored in the price history table via `add_price()`.\n",
    "5. The user sees loading spinners and success messages throughout the process.\n",
    "\n",
    "This gives us a complete workflow for adding new products to track, including capturing their initial price point. The UI provides clear feedback at each step and handles errors gracefully.\n",
    "\n",
    "Check that everything is working the way we want it and then, commit the new changes:\n",
    "\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Add a feature to track product prices after they are added\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Displaying each product's price history in the main dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the final product shown in the introduction once again:\n",
    "\n",
    "![](images/sneak-peek.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the sidebar, the main dashboard shows each product's price history visualized with a Plotly line plot where the X axis is the timestamp while the Y axis is the prices. Each line plot is wrapped in a Streamlit component that includes buttons for removing the product from the database or visiting its source URL. \n",
    "\n",
    "In this step, we will implement the plotting feature and leave the two buttons for a later section. First, add a new method to the `Database` class for retrieving the price history for each product:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Database:\n",
    "    ...  # The rest of the code\n",
    "\n",
    "    def get_price_history(self, url):\n",
    "        \"\"\"Get price history for a product\"\"\"\n",
    "        session = self.Session()\n",
    "        try:\n",
    "            return (\n",
    "                session.query(PriceHistory)\n",
    "                .filter(PriceHistory.product_url == url)\n",
    "                .order_by(PriceHistory.timestamp.desc())\n",
    "                .all()\n",
    "            )\n",
    "        finally:\n",
    "            session.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method queries the price histories table based on product URL, orders the rows in descending order (oldest first) and returns the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, add another method for retrieving all products from the `products` table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Database:\n",
    "    ...\n",
    "    \n",
    "    def get_all_products(self):\n",
    "        session = self.Session()\n",
    "        try:\n",
    "            return session.query(Product).all()\n",
    "        finally:\n",
    "            session.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that every time our Streamlit app is opened, the main dashboard queries all existing products from the database and render their price histories with line charts in dedicated components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the line charts, we need Plotly and Pandas, so install them in your environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install pandas plotly\n",
    "echo \"pandas\\nplotly\" >> requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterward, import them at the top of `ui.py` along with other existing imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, switch to `ui.py` and paste the following snippet of code after the Main content section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Main content\n",
    "st.title(\"Price Tracker Dashboard\")\n",
    "st.markdown(\"## Tracked Products\")\n",
    "\n",
    "# Get all products\n",
    "products = db.get_all_products()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, after the page title and subtitle is shown, we are retrieving all products from the database. Let's loop over them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create a card for each product\n",
    "for product in products:\n",
    "    price_history = db.get_price_history(product.url)\n",
    "    if price_history:\n",
    "        # Create DataFrame for plotting\n",
    "        df = pd.DataFrame(\n",
    "            [\n",
    "                {\"timestamp\": ph.timestamp, \"price\": ph.price, \"name\": ph.name}\n",
    "                for ph in price_history\n",
    "            ]\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each product, we get their price history with `db.get_price_history` and then, convert this data into a dataframe with three columns:\n",
    "\n",
    "- Timestamp\n",
    "- Price\n",
    "- Product name\n",
    "\n",
    "This makes plotting easier with Plotly. Next, we create a Streamlit expander component for each product:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create a card for each product\n",
    "for product in products:\n",
    "    price_history = db.get_price_history(product.url)\n",
    "    if price_history:\n",
    "        ...\n",
    "        # Create a card-like container for each product\n",
    "        with st.expander(df[\"name\"][0], expanded=False):\n",
    "            st.markdown(\"---\")\n",
    "            col1, col2 = st.columns([1, 3])\n",
    "\n",
    "            with col1:\n",
    "                if price_history[0].main_image_url:\n",
    "                    st.image(price_history[0].main_image_url, width=200)\n",
    "                st.metric(\n",
    "                    label=\"Current Price\",\n",
    "                    value=f\"{price_history[0].price} {price_history[0].currency}\",\n",
    "                )\n",
    "```\n",
    "\n",
    "The expander shows the product name as its title and contains:\n",
    "\n",
    "1. A divider line\n",
    "2. Two columns:\n",
    "   - Left column: Product image (if available) and current price metric\n",
    "   - Right column (shown in next section)\n",
    "\n",
    "The price is displayed using Streamlit's metric component which shows the current price and currency.\n",
    "\n",
    "Here is the rest of the code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "            ...\n",
    "            \n",
    "            with col2:\n",
    "                # Create price history plot\n",
    "                fig = px.line(\n",
    "                    df,\n",
    "                    x=\"timestamp\",\n",
    "                    y=\"price\",\n",
    "                    title=None,\n",
    "                )\n",
    "                fig.update_layout(\n",
    "                    xaxis_title=None,\n",
    "                    yaxis_title=\"Price ($)\",\n",
    "                    showlegend=False,\n",
    "                    margin=dict(l=0, r=0, t=0, b=0),\n",
    "                    height=300,\n",
    "                )\n",
    "                fig.update_xaxes(tickformat=\"%Y-%m-%d %H:%M\", tickangle=45)\n",
    "                fig.update_yaxes(tickprefix=\"$\", tickformat=\".2f\")\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the right column, we create an interactive line plot using Plotly Express to visualize the price history over time. The plot shows price on the y-axis and timestamp on the x-axis. The layout is customized to remove the title, adjust axis labels and formatting, and optimize the display size. The timestamps are formatted to show date and time, with angled labels for better readability. Prices are displayed with 2 decimal places and a dollar sign prefix. The plot is rendered using Streamlit's `plotly_chart` component and automatically adjusts its width to fill the container.\n",
    "\n",
    "After this step, the UI must be fully functional and ready to track products. For example, here is what mine looks like after adding a couple of products:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/finished.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But notice how the price history chart doesn't show anything. That's because we haven't populated it by checking the product price in regular intervals. Let's do that in the next couple of steps. For now, commit the latest changes we've made:\n",
    "\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Display product price histories for each product in the dashboard\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "Let's take a brief moment to summarize the steps we took so far and what's next. So far, we've built a Streamlit interface that allows users to add product URLs and displays their current prices and basic information. We've implemented the database schema, created functions to scrape product data, and designed a clean UI with price history visualization. The next step is to set up automated price checking to populate our history charts and enable proper price tracking over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Adding new price entries for existing products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to write a script that adds new price entries in the `price_histories` table for each product in `products` table. We call this script `check_prices.py`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "from database import Database\n",
    "from dotenv import load_dotenv\n",
    "from firecrawl import FirecrawlApp\n",
    "from scraper import scrape_product\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "db = Database(os.getenv(\"POSTGRES_URL\"))\n",
    "app = FirecrawlApp()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the top, we are importing the functions and packages and initializing the database and a Firecrawl app. Then, we define a simple `check_prices` function:\n",
    "\n",
    "```python\n",
    "def check_prices():\n",
    "    products = db.get_all_products()\n",
    "\n",
    "    for product in products:\n",
    "        # Retrieve updated product data\n",
    "        updated_product = scrape_product(product.url)\n",
    "\n",
    "        # Add the price to the database\n",
    "        db.add_price(updated_product)\n",
    "        print(f\"Added new price entry for {updated_product['name']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_prices()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function body, we retrieve all products URLs, retrieve their new price data with `scrape_product` function from `scraper.py` and then, add a new price entry for the product with `db.add_price`. \n",
    "\n",
    "If you run the function once and refresh the Streamlit app, you must see a line chart appear for each product you are tracking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/linechart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's commit the changes in this step:\n",
    "\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Add a script for checking prices of existing products\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Check prices regularly with GitHub actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate various software workflows directly from your GitHub repository. In our case, it's particularly useful because we can set up automated price checks to run the `check_prices.py` script at regular intervals (e.g., daily or hourly) without manual intervention. This ensures we consistently track price changes and maintain an up-to-date database of historical prices for our tracked products.\n",
    "\n",
    "So, the first step is creating a new GitHub repository for our project and pushing existing code to it:\n",
    "\n",
    "```bash\n",
    "git remote add origin https://github.com/yourusername/price-tracker.git\n",
    "git push origin main\n",
    "```\n",
    "\n",
    "Then, return to your terminal and create this directory structure:\n",
    "\n",
    "```bash\n",
    "mkdir -p .github/workflows\n",
    "touch .github/workflows/check_prices.yml\n",
    "```\n",
    "\n",
    "The first command creates a new directory structure `.github/workflows` using the `-p` flag to create parent directories if they don't exist.\n",
    "\n",
    "The second command creates an empty YAML file called `check_prices.yml` inside the workflows directory. GitHub Actions looks for workflow files in this specific location - any YAML files in the `.github/workflows` directory will be automatically detected and processed as workflow configurations. These YAML files define when and how your automated tasks should run, what environment they need, and what commands to execute. In our case, this file will contain instructions for GitHub Actions to periodically run our price checking script. Let's write it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "name: Price Check\n",
    "\n",
    "on:\n",
    "  schedule:\n",
    "    # Runs every 3 minutes\n",
    "    - cron: \"*/3 * * * *\"\n",
    "  workflow_dispatch: # Allows manual triggering\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this first part of the YAML file:\n",
    "\n",
    "The `name: Price Check` line gives our workflow a descriptive name that will appear in the GitHub Actions interface.\n",
    "\n",
    "The `on:` section defines when this workflow should be triggered. We've configured two triggers:\n",
    "\n",
    "1. A schedule using cron syntax `*/3 * * * *` which runs the workflow every 3 minutes. The five asterisks represent minute, hour, day of month, month, and day of week respectively. The `*/3` means \"every 3rd minute\". The 3-minute interval is for debugging purposes, we will need to choose a wider interval later on to respect the free limits of GitHub actions. \n",
    "\n",
    "2. `workflow_dispatch` enables manual triggering of the workflow through the GitHub Actions UI, which is useful for testing or running the check on-demand.\n",
    "\n",
    "Now, let's add the rest:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "jobs:\n",
    "  check-prices:\n",
    "    runs-on: ubuntu-latest\n",
    "\n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v4\n",
    "\n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: \"3.10\"\n",
    "          cache: \"pip\"\n",
    "\n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r automated_price_tracking/requirements.txt\n",
    "\n",
    "      - name: Run price checker\n",
    "        env:\n",
    "          FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\n",
    "          POSTGRES_URL: ${{ secrets.POSTGRES_URL }}\n",
    "        run: python automated_price_tracking/check_prices.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this second part of the YAML file:\n",
    "\n",
    "The `jobs:` section defines the actual work to be performed. We have one job named `check-prices` that runs on an Ubuntu virtual machine (`runs-on: ubuntu-latest`).\n",
    "\n",
    "Under `steps:`, we define the sequence of actions:\n",
    "\n",
    "1. First, we checkout our repository code using the standard `actions/checkout@v4` action\n",
    "\n",
    "2. Then we set up Python 3.10 using `actions/setup-python@v5`, enabling pip caching to speed up dependency installation\n",
    "\n",
    "3. Next, we install our Python dependencies by upgrading `pip` and installing requirements from our `requirements.txt` file. At this point, it is essential that you were keeping a complete dependency file based on the installs we made in the project. \n",
    "\n",
    "4. Finally, we run our price checker script, providing two environment variables:\n",
    "   - `FIRECRAWL_API_KEY`: For accessing the web scraping service\n",
    "   - `POSTGRES_URL`: For connecting to our database\n",
    "\n",
    "Both variables must be stored in our GitHub repository as secrets for this workflow file to run without errors. So, navigate to the repository you've created for the project and open its Settings. Under \"Secrets and variables\" > \"Actions\", click on \"New repository secret\" button to add the environment variables we have in the `.env` file one-by-one. \n",
    "\n",
    "Then, return to your terminal, commit the changes and push:\n",
    "\n",
    "```bash\n",
    "git add . \n",
    "git commit -m \"Add a workflow to check prices regularly\"\n",
    "git push origin main\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, navigate to your GitHub repository again and click on the \"Actions\" tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/actions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, you can run the workflow manually (click \"Run workflow\" and refresh the page). If it is executed successfully, you can return to the Streamlit app and refresh to see the new price added to the chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Setting up Discord for notifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know our scheduling workflow works, the first order of business is setting a wider check interval in the workflow file. Even though our first workflow run was manually, the rest happen automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "on:\n",
    "  schedule:\n",
    "    # Runs every 6 hours\n",
    "    - cron: \"0 0,6,12,18 * * *\"\n",
    "  workflow_dispatch: # Allows manual triggering\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the workflow file, change the cron field to the syntax you see above, which runs the workflow at the first minute of 12am, 6am, 12pm and 6pm UTC. Then, commit and push the changes:\n",
    "\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Set a wider check interval in the workflow file\"\n",
    "git push origin main\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the interesting part. Each time the workflow is run, we want to compare the current price of the product to its original price when we started tracking it. If the difference between these two prices is below a certain threshold like 5%, this means there is a discount happening for the product and we want to send a notification. \n",
    "\n",
    "The easiest way to set this up is by using Discord webhooks. So, if you haven't got one already, go to Discord.com and create a new account (optionally, download the desktop app as well). Then, log in to your account and you will find a \"Plus\" button in the bottom-left corner. Click on it to create your own Discord server:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/discord.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pressing \"Plus\", choose \"Create my own\" and \"For me and my friends\". Then, give a new name to your server and you will be presented with an empty channel:\n",
    "\n",
    "![](images/new-server.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right click on \"general\" and choose \"Edit channel\". Switch to the integrations tab and click on \"Create webhook\". Discord immediately generates a new webhook with a random name and you should copy its URL. \n",
    "\n",
    "![](images/webhook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Webhooks are automated messages sent from apps to other apps in real-time. They work like a notification system - when something happens in one app, it automatically sends data to another app through a unique URL. In our case, we'll use Discord webhooks to automatically notify us when there's a price drop. Whenever our price tracking script detects a significant discount, it will send a message to our Discord channel through the webhook URL, ensuring we never miss a good deal.\n",
    "\n",
    "After copying the webhook URL, you should save it as environment variable to your `.env` file:\n",
    "\n",
    "```python\n",
    "echo \"DISCORD_WEBHOOK_URL='THE-URL-YOU-COPIED'\" >> .env\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a new file called `notifications.py` and paste the following contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "async def send_price_alert(\n",
    "    product_name: str, old_price: float, new_price: float, url: str\n",
    "):\n",
    "    \"\"\"Send a price drop alert to Discord\"\"\"\n",
    "    drop_percentage = ((old_price - new_price) / old_price) * 100\n",
    "\n",
    "    message = {\n",
    "        \"embeds\": [\n",
    "            {\n",
    "                \"title\": \"Price Drop Alert! üéâ\",\n",
    "                \"description\": f\"**{product_name}**\\nPrice dropped by {drop_percentage:.1f}%!\\n\"\n",
    "                f\"Old price: ${old_price:.2f}\\n\"\n",
    "                f\"New price: ${new_price:.2f}\\n\"\n",
    "                f\"[View Product]({url})\",\n",
    "                \"color\": 3066993,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            await session.post(os.getenv(\"DISCORD_WEBHOOK_URL\"), json=message)\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending Discord notification: {e}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `send_price_alert` function above is responsible for sending price drop notifications to Discord using webhooks. Let's break down what's new:\n",
    "\n",
    "1. The function takes 4 parameters:\n",
    "   - `product_name`: The name of the product that dropped in price\n",
    "   - `old_price`: The previous price before the drop\n",
    "   - `new_price`: The current lower price\n",
    "   - `url`: Link to view the product\n",
    "\n",
    "2. It calculates the percentage drop in price using the formula: `((old_price - new_price) / old_price) * 100`\n",
    "\n",
    "3. The notification is formatted as a Discord embed - a rich message format that includes:\n",
    "   - A title with a celebration emoji\n",
    "   - A description showing the product name, price drop percentage, old and new prices\n",
    "   - A link to view the product\n",
    "   - A green color (3066993 in decimal)\n",
    "\n",
    "4. The message is sent asynchronously using `aiohttp` to post to the Discord webhook URL stored in the environment variables\n",
    "\n",
    "5. Error handling is included to catch and print any issues that occur during the HTTP request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This provides a clean way to notify users through Discord whenever we detect a price drop for tracked products.\n",
    "\n",
    "To check the notification system works, add this main block to the end of the script:\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(send_price_alert(\"Test Product\", 100, 90, \"https://www.google.com\"))\n",
    "```\n",
    "\n",
    "`asyncio.run()` is used here because `send_price_alert` is an async function that needs to be executed in an event loop. `asyncio.run()` creates and manages this event loop, allowing the async HTTP request to be made properly. Without it, we wouldn't be able to use the `await` keyword inside `send_price_alert`.\n",
    "\n",
    "\n",
    "To run the script, install `aiohttp`:\n",
    "\n",
    "```python\n",
    "pip install aiohttp\n",
    "echo \"aiohttp\\n\" >> requirements.txt\n",
    "python notifications.py\n",
    "```\n",
    "\n",
    "If all is well, you should get a Discord message in your server that looks like this:\n",
    "\n",
    "![](images/alert.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's commit the changes we have again:\n",
    "\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Set up Discord alert system\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Sending Discord alerts when prices drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the only step left is adding a price comparison logic to `check_prices.py`. In other words, we want to use the `send_price_alert` function if the new scraped price is lower than the original. This requires a revamped `check_prices.py` script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import asyncio\n",
    "from database import Database\n",
    "from dotenv import load_dotenv\n",
    "from firecrawl import FirecrawlApp\n",
    "from scraper import scrape_product\n",
    "from notifications import send_price_alert\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "db = Database(os.getenv(\"POSTGRES_URL\"))\n",
    "app = FirecrawlApp()\n",
    "\n",
    "# Threshold percentage for price drop alerts (e.g., 5% = 0.05)\n",
    "PRICE_DROP_THRESHOLD = 0.05\n",
    "\n",
    "\n",
    "async def check_prices():\n",
    "    products = db.get_all_products()\n",
    "    product_urls = set(product.url for product in products)\n",
    "\n",
    "    for product_url in product_urls:\n",
    "        # Get the price history\n",
    "        price_history = db.get_price_history(product_url)\n",
    "        if not price_history:\n",
    "            continue\n",
    "\n",
    "        # Get the earliest recorded price\n",
    "        earliest_price = price_history[-1].price\n",
    "\n",
    "        # Retrieve updated product data\n",
    "        updated_product = scrape_product(product_url)\n",
    "        current_price = updated_product[\"price\"]\n",
    "\n",
    "        # Add the price to the database\n",
    "        db.add_price(updated_product)\n",
    "        print(f\"Added new price entry for {updated_product['name']}\")\n",
    "\n",
    "        # Check if price dropped below threshold\n",
    "        if earliest_price > 0:  # Avoid division by zero\n",
    "            price_drop = (earliest_price - current_price) / earliest_price\n",
    "            if price_drop >= PRICE_DROP_THRESHOLD:\n",
    "                await send_price_alert(\n",
    "                    updated_product[\"name\"], earliest_price, current_price, product_url\n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(check_prices())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the key changes in this enhanced version of `check_prices.py`:\n",
    "\n",
    "1. New imports and setup\n",
    "   - Added `asyncio` for `async`/`await` support\n",
    "   - Imported `send_price_alert` from `notifications.py`\n",
    "   - Defined `PRICE_DROP_THRESHOLD = 0.05` (5% threshold for alerts)\n",
    "\n",
    "2. Async function conversion\n",
    "   - Converted `check_prices()` to async function\n",
    "   - Gets unique product URLs using set comprehension to avoid duplicates\n",
    "   \n",
    "3. Price history analysis\n",
    "   - Retrieves full price history for each product\n",
    "   - Gets `earliest_price` from `history[-1]` (works because we ordered by timestamp DESC)\n",
    "   - Skips products with no price history using `continue`\n",
    "   \n",
    "4. Price drop detection logic\n",
    "   - Calculates drop percentage: `(earliest_price - current_price) / earliest_price`\n",
    "   - Checks if drop exceeds 5% threshold\n",
    "   - Sends Discord alert if threshold exceeded using `await send_price_alert()`\n",
    "   \n",
    "5. Async main block\n",
    "   - Uses `asyncio.run()` to execute async `check_prices()` in event loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I tested this new version of the script, I immediately got an alert:\n",
    "\n",
    "![](images/new-alert.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's commit everything and push to GitHub so that our workflow is supercharged with our notification system:\n",
    "\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Add notification system to price drops\"\n",
    "git push origin main\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations for making it to the end of this extremely long tutorial! We've just covered how to implement an end-to-end Python project you can proudly showcase on your portfolio. We built a complete price tracking system that scrapes product data from e-commerce websites, stores it in a Postgres database, analyzes price histories, and sends automated Discord notifications when prices drop significantly. Along the way, we learned about web scraping with Firecrawl, database management with SQLAlchemy, asynchronous programming with asyncio, building interactive UIs with Streamlit, automating with GitHub actions and integrating external webhooks.\n",
    "\n",
    "However, the project is far from perfect. Since we took a top-down approach to building this app, our project code is scattered across multiple files and doesn't conform to programming best practices most of the time. For this reason, I've recreated the same project in a much more sophisticated matter with production-level features. [This new version on GitHub](https://github.com/BexTuychiev/automated-price-tracking) implements proper database session management, faster operations and overall smoother user experience. \n",
    "\n",
    "If you decide to stick with the basic version, you can find the full project code and the notebook from the official Firecrawl GitHub repository example projects. I also recommend that you deploy your Streamlit app to Streamlit Cloud so that you have a function app accessible everywhere you go. \n",
    "\n",
    "Here are some more guides from our blog if you are interested:\n",
    "\n",
    "- [How to Run Web Scrapers on Schedule](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025)\n",
    "- [More about using Firecrawl's `scrape_url` function](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint)\n",
    "- [Scraping entire websites with Firecrawl in a single command - the /crawl endpoint](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl)\n",
    "\n",
    "Thank you for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

================
File: examples/blog-articles/amazon-price-tracking/notebook.md
================
---
title: How to Build an Automated Amazon Price Tracking Tool in Python For Free
description: Learn how to build a free automated price tracking tool in Python that monitors Amazon and other e-commerce sites, sends Discord alerts for price drops, and maintains price history using Firecrawl, Streamlit, and GitHub Actions.
slug: amazon-price-tracker-in-python-for-free
date: Dec 6, 2024
author: bex_tuychiev
image: /images/blog/amazon-price-tracking/amazon-price-tracker-in-python-for-free.jpg
categories: [tutorials]
keywords: [amazon price tracker, amazon price history tracker, amazon price tracker app, amazon web scraper, amazon web scraper python, ecommerce web scraping, web scraping python]
---

## That sends alerts to your phone and keeps price history

## What Shall We Build in This Tutorial?

There is a lot to be said about the psychology of discounts. For example, buying a discounted item we don't need isn't saving money at all - it's falling for one of the oldest sales tactics. However, there are legitimate cases where waiting for a price drop on items you actually need makes perfect sense.

The challenge is that e-commerce websites run flash sales and temporary discounts constantly, but these deals often disappear as quickly as they appear. Missing these brief windows of opportunity can be frustrating.

That's where automation comes in. In this guide, we'll build a Python application that monitors product prices across any e-commerce website and instantly notifies you when prices drop on items you're actually interested in. Here is a sneak peek of the app:

![Screenshot of a minimalist price tracking application showing product listings, price history charts, and notification controls for monitoring e-commerce deals using Firecrawl](amazon-price-tracking-images/sneak-peek.png)

The app has a simple appearance but provides complete functionality:

- It has a minimalistic UI to add or remove products from the tracker
- A simple dashboard to display price history for each product
- Controls for setting the price drop threshold in percentages
- A notification system that sends Discord alerts when a tracked item's price drops
- A scheduling system that updates the product prices on an interval you specify
- Runs for free for as long as you want

Even though the title says "Amazon price tracker" (full disclosure: I was forced to write that for SEO purposes), the app will work for any e-commerce website you can imagine (except Ebay, for some reason).

So, let's get started building this Amazon price tracker.

## The Toolstack We Will Use

The app will be built using Python and these libraries::

- [Streamlit](streamlit.io) for the UI
- [Firecrawl](firecrawl.dev) for AI-based scraping of e-commerce websites
- [SQLAlchemy](https://www.sqlalchemy.org/) for database management

In addition to Python, we will use these platforms:

- Discord for notifications
- GitHub for hosting the app
- GitHub Actions for running the app on a schedule
- Supabase for hosting a free Postgres database instance

## Building an Amazon Price Tracker App Step-by-step

Since this project involves multiple components working together, we'll take a top-down approach rather than building individual pieces first. This approach makes it easier to understand how everything fits together, since we'll introduce each tool only when it's needed. The benefits of this strategy will become clear as we progress through the tutorial.

### Step 1: Setting up the environment

First, let's create a dedicated environment on our machines to work on the project:

```bash
mkdir automated-price-tracker
cd automated-price-tracker
python -m venv .venv
source .venv/bin/activate
```

These commands create a working directory and activate a virtual environment. Next, create a new script called `ui.py` for designing the user interface with Streamlit.

```bash
touch ui.py
```

Then, install Streamlit:

```bash
pip install streamlit
```

Next, create a `requirements.txt` file and add Streamlit as the first dependency:

```bash
touch requirements.txt
echo "streamlit\n" >> requirements.txt
```

Since the code will be hosted on GitHub, we need to initialize Git and create a `.gitignore` file:

```bash
git init
touch .gitignore
echo ".venv" >> .gitignore  # Add the virtual env folder
git commit -m "Initial commit"
```

### Step 2: Add a sidebar to the UI for product input

Let's take a look at the final product one more time:

![A screenshot of an Amazon price tracker web application showing a sidebar for adding product URLs and a main dashboard displaying tracked products with price history charts. Created with streamlit and firecrawl](amazon-price-tracking-images/sneak-peek.png)

It has two sections: the sidebar and the main dashboard. Since the first thing you do when launching this app is adding products, we will start building the sidebar first. Open `ui.py` and paste the following code:

```python
import streamlit as st

# Set up sidebar
with st.sidebar:
    st.title("Add New Product")
    product_url = st.text_input("Product URL")
    add_button = st.button("Add Product")

# Main content
st.title("Price Tracker Dashboard")
st.markdown("## Tracked Products")
```

The code snippet above sets up a basic Streamlit web application with two main sections. In the sidebar, it creates a form for adding new products with a text input field for the product URL and an "Add Product" button. The main content area contains a dashboard title and a section header for tracked products. The code uses Streamlit's `st.sidebar` context manager to create the sidebar layout and basic Streamlit components like `st.title`, `st.text_input`, and `st.button` to build the user interface elements.

To see how this app looks like, run the following command:

```bash
streamlit run ui.py
```

Now, let's add a commit to save our progress:

```bash
git add .
git commit -m "Add a sidebar to the basic UI"
```

### Step 3: Add a feature to check if input URL is valid

In the next step, we want to add some restrictions to the input field like checking if the passed URL is valid. For this, create a new file called `utils.py` where we write additional utility functions for our app:

```bash
touch utils.py
```

Inside the script, paste following code:

```bash
# utils.py
from urllib.parse import urlparse
import re


def is_valid_url(url: str) -> bool:
    try:
        # Parse the URL
        result = urlparse(url)

        # Check if scheme and netloc are present
        if not all([result.scheme, result.netloc]):
            return False

        # Check if scheme is http or https
        if result.scheme not in ["http", "https"]:
            return False

        # Basic regex pattern for domain validation
        domain_pattern = (
            r"^[a-zA-Z0-9]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z]{2,})+$"
        )
        if not re.match(domain_pattern, result.netloc):
            return False

        return True

    except Exception:
        return False
```

The above function `is_valid_url()` validates URLs by checking several criteria:

1. It verifies the URL has both a scheme (`http`/`https`) and domain name
2. It ensures the scheme is specifically `http` or `https`
3. It validates the domain name format using regex to check for valid characters and TLD
4. It returns True only if all checks pass, False otherwise

Let's use this function in our `ui.py` file. Here is the modified code:

```python
import streamlit as st
from utils import is_valid_url


# Set up sidebar
with st.sidebar:
    st.title("Add New Product")
    product_url = st.text_input("Product URL")
    add_button = st.button("Add Product")

    if add_button:
        if not product_url:
            st.error("Please enter a product URL")
        elif not is_valid_url(product_url):
            st.error("Please enter a valid URL")
        else:
            st.success("Product is now being tracked!")

# Main content
...
```

Here is what's new:

1. We added URL validation using the `is_valid_url()` function from `utils.py`
2. When the button is clicked, we perform validation:
   - Check if URL is empty
   - Validate URL format using `is_valid_url()`
3. User feedback is provided through error/success messages:
   - Error shown for empty URL
   - Error shown for invalid URL format
   - Success message when URL passes validation

Rerun the Streamlit app again and see if our validation works. Then, return to your terminal to commit the changes we've made:

```bash
git add .
git commit -m "Add a feature to check URL validity"
```

### Step 4: Scrape the input URL for product details

When a valid URL is entered and the add button is clicked, we need to implement product scraping functionality instead of just showing a success message. The system should:

1. Immediately scrape the product URL to extract key details:
   - Product name
   - Current price
   - Main product image
   - Brand name
   - Other relevant attributes

2. Store these details in a database to enable:
   - Regular price monitoring
   - Historical price tracking
   - Price change alerts
   - Product status updates

For the scraper, we will use [Firecrawl](firecrawl.dev), an AI-based scraping API for extracting webpage data without HTML parsing. This solution provides several advantages:

1. No website HTML code analysis required for element selection
2. Resilient to HTML structure changes through AI-based element detection
3. Universal compatibility with product webpages due to structure-agnostic approach
4. Reliable website blocker bypass via robust API infrastructure

First, create a new file called `scraper.py`:

```bash
touch scraper.py
```

Then, install these three libraries:

```bash
pip install firecrawl-py pydantic python-dotenv
echo "firecrawl-py\npydantic\npython-dotenv\n" >> requirements.txt  # Add them to dependencies
```

`firecrawl-py` is the Python SDK for Firecrawl scraping engine, `pydantic` is a data validation library that helps enforce data types and structure through Python class definitions, and `python-dotenv` is a library that loads environment variables from a `.env` file into your Python application.

With that said, head over to the Firecrawl website and [sign up for a free account](https://www.firecrawl.dev/) (the free plan will work fine). You will be given an API key, which you should copy.

Then, create a `.env` file in your terminal and add the API key as an environment variable:

```bash
touch .env
echo "FIRECRAWL_API_KEY='YOUR-API-KEY-HERE' >> .env"
echo ".env" >> .gitignore  # Ignore .env files in Git
```

The `.env` file is used to securely store sensitive configuration values like API keys that shouldn't be committed to version control. By storing the Firecrawl API key in `.env` and adding it to `.gitignore`, we ensure it stays private while still being accessible to our application code. This is a security best practice to avoid exposing credentials in source control.

Now, we can start writing the `scraper.py`:

```python
from firecrawl import FirecrawlApp
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from datetime import datetime

load_dotenv()

app = FirecrawlApp()
```

Here, `load_dotenv()` function reads the `.env` file you have in your working directory and loads the environment variables inside, including the Firecrawl API key. When you create an instance of `FirecrawlApp` class, the API key is automatically detected to establish a connection between your script and the scraping engine in the form of the `app` variable.

Now, we create a Pydantic class (usually called a model) that defines the details we want to scrape from each product:

```python
class Product(BaseModel):
    """Schema for creating a new product"""

    url: str = Field(description="The URL of the product")
    name: str = Field(description="The product name/title")
    price: float = Field(description="The current price of the product")
    currency: str = Field(description="Currency code (USD, EUR, etc)")
    main_image_url: str = Field(description="The URL of the main image of the product")
```

Pydantic models may be completely new to you, so let's break down the `Product` model:

- The `url` field stores the product page URL we want to track
- The `name` field stores the product title/name that will be scraped
- The `price` field stores the current price as a float number
- The `currency` field stores the 3-letter currency code (e.g. USD, EUR)
- The `main_image_url` field stores the URL of the product's main image

Each field is typed and has a description that documents its purpose. The `Field` class from Pydantic allows us to add metadata like descriptions to each field. These descriptions are especially important for Firecrawl since it uses them to automatically locate the relevant HTML elements containing the data we want.

Now, let's create a function to call the engine to scrape URL's based on the schema above:

```python
def scrape_product(url: str):
    extracted_data = app.scrape_url(
        url,
        params={
            "formats": ["extract"],
            "extract": {"schema": Product.model_json_schema()},
        },
    )

    # Add the scraping date to the extracted data
    extracted_data["extract"]["timestamp"] = datetime.utcnow()

    return extracted_data["extract"]


if __name__ == "__main__":
    product = "https://www.amazon.com/gp/product/B002U21ZZK/"

    print(scrape_product(product))
```

The code above defines a function called `scrape_product` that takes a URL as input and uses it to scrape product information. Here's how it works:

The function calls `app.scrape_url` with two parameters:

1. The product URL to scrape
2. A params dictionary that configures the scraping:
   - It specifies we want to use the "extract" format
   - It provides our `Product` Pydantic model schema as the extraction template as a JSON object

The scraper will attempt to find and extract data that matches our Product schema fields - the URL, name, price, currency, and image URL.

The function returns just the "extract" portion of the scraped data, which contains the structured product information. `extract` returns a dictionary to which we add the date of the scraping as it will be important later on.

Let's test the script by running it:

```bash
python scraper.py
```

You should get an output like this:

```python
{
    'url': 'https://www.amazon.com/dp/B002U21ZZK', 
    'name': 'MOVA Globe Earth with Clouds 4.5"', 
    'price': 212, 
    'currency': 'USD', 
    'main_image_url': 'https://m.media-amazon.com/images/I/41bQ3Y58y3L._AC_.jpg', 
    'timestamp': '2024-12-05 13-20'
}
```

The output shows that a [MOVA Globe](https://www.amazon.com/dp/B002U21ZZK) costs $212 USD on Amazon at the time of writing this article. You can test the script for any other website that contains the information we are looking (except Ebay):

- Price
- Product name/title
- Main image URL

One key advantage of using Firecrawl is that it returns data in a consistent dictionary format across all websites. Unlike HTML-based scrapers like BeautifulSoup or Scrapy which require custom code for each site and can break when website layouts change, Firecrawl uses AI to understand and extract the requested data fields regardless of the underlying HTML structure.

Finish this step by committing the new changes to Git:

```bash
git add .
git commit -m "Implement a Firecrawl scraper for products"
```

### Step 5: Storing new products in a PostgreSQL database

If we want to check product prices regularly, we need to have an online database. In this case, Postgres is the best option since it's reliable, scalable, and has great support for storing time-series data like price histories.

There are many platforms for hosting Postgres instances but the one I find the easiest and fastest to set up is Supabase. So, please head over to [the Supabase website](https://supabase.com) and create your free account. During the sign-up process, you will be given a password, which you should save somewhere safe on your machine.

Then, in a few minutes, your free Postgres instance comes online. To connect to this instance, click on Home in the left sidebar and then, "Connect":

![Screenshot of Supabase dashboard showing database connection settings and credentials for connecting to a PostgreSQL database instance](amazon-price-tracking-images/supabase_connect.png)

You will be shown your database connection string with a placeholder for the password you copied. You should paste this string in your `.env` file with your password added to the `.env` file:

```bash
echo POSTGRES_URL="THE-SUPABASE-URL-STRING-WITH-YOUR-PASSWORD-ADDED"
```

Now, the easiest way to interact with this database is through SQLAlchemy. Let's install it:

```bash
pip install "sqlalchemy==2.0.35" psycopg2-binary
echo "psycopg2-binary\nsqlalchemy==2.0.35\n" >> requirements.txt
```

> Note: [SQLAlchemy](https://sqlalchemy.org) is a Python SQL toolkit and Object-Relational Mapping (ORM) library that lets us interact with databases using Python code instead of raw SQL. For our price tracking project, it provides essential features like database connection management, schema definition through Python classes, and efficient querying capabilities. This makes it much easier to store and retrieve product information and price histories in our Postgres database.

After the installation, create a new `database.py` file for storing database-related functions:

```bash
touch database.py
```

Let's populate this script:

```python
from sqlalchemy import create_engine, Column, String, Float, DateTime, ForeignKey
from sqlalchemy.orm import sessionmaker, relationship, declarative_base
from datetime import datetime

Base = declarative_base()


class Product(Base):
    __tablename__ = "products"

    url = Column(String, primary_key=True)
    prices = relationship(
        "PriceHistory", back_populates="product", cascade="all, delete-orphan"
    )


class PriceHistory(Base):
    __tablename__ = "price_histories"

    id = Column(String, primary_key=True)
    product_url = Column(String, ForeignKey("products.url"))
    name = Column(String, nullable=False)
    price = Column(Float, nullable=False)
    currency = Column(String, nullable=False)
    main_image_url = Column(String)
    timestamp = Column(DateTime, nullable=False)
    product = relationship("Product", back_populates="prices")

```

The code above defines two SQLAlchemy models for our price tracking database:

The `Product` model acts as a registry of all items we want to track. It's kept simple with just the URL as we don't want to duplicate data that changes over time.
  
The `PriceHistory` model stores the actual price data points and product details at specific moments in time. This separation allows us to:

- Track how product details (name, price, image) change over time
- Maintain a clean historical record for each product
- Efficiently query price trends without loading unnecessary data

Each record in `PriceHistory` contains:

- A unique ID as primary key
- The product URL as a foreign key linking to the `Product`
- The product name
- The price value and currency
- The main product image URL
- A timestamp of when the price was recorded

The relationship between `Product` and `PriceHistory` is bidirectional, allowing easy navigation between related records. The `cascade` setting ensures price histories are deleted when their product is deleted.

These models provide the structure for storing and querying our price tracking data in a PostgreSQL database using SQLAlchemy's ORM capabilities.

Now, we define a `Database` class with a singe `add_product` method:

```python
class Database:
    def __init__(self, connection_string):
        self.engine = create_engine(connection_string)
        Base.metadata.create_all(self.engine)
        self.Session = sessionmaker(bind=self.engine)

    def add_product(self, url):
        session = self.Session()
        try:
            # Create the product entry
            product = Product(url=url)
            session.merge(product)  # merge will update if exists, insert if not
            session.commit()
        finally:
            session.close()
```

The `Database` class above provides core functionality for managing product data in our PostgreSQL database. It takes a connection string in its constructor to establish the database connection using SQLAlchemy.

The `add_product` method allows us to store new product URLs in the database. It uses SQLAlchemy's `merge` functionality which intelligently handles both inserting new products and updating existing ones, preventing duplicate entries.

The method carefully manages database sessions, ensuring proper resource cleanup by using `try`/`finally` blocks. This prevents resource leaks and maintains database connection stability.

Let's use this method inside the sidebar of our UI. Switch to `ui.py` and make the following adjustments:

First, update the imports to load the Database class and initialize it:

```python
import os
import streamlit as st

from utils import is_valid_url
from database import Database
from dotenv import load_dotenv

load_dotenv()

with st.spinner("Loading database..."):
    db = Database(os.getenv("POSTGRES_URL"))
```

The code integrates the `Database` class into the Streamlit UI by importing required dependencies and establishing a database connection. The database URL is loaded securely from environment variables using `python-dotenv`. The `Database` class creates or updates the tables we specified in `database.py` after being initialized.

The database initialization process is wrapped in a Streamlit spinner component to maintain responsiveness while establishing the connection. This provides visual feedback during the connection setup period, which typically requires a brief initialization time.

Then, in the sidebar code, we only need to add a single line of code to add the product to the database if the URL is valid:

```python
# Set up sidebar
with st.sidebar:
    st.title("Add New Product")
    product_url = st.text_input("Product URL")
    add_button = st.button("Add Product")

    if add_button:
        if not product_url:
            st.error("Please enter a product URL")
        elif not is_valid_url(product_url):
            st.error("Please enter a valid URL")
        else:
            db.add_product(product_url)  # This is the new line
            st.success("Product is now being tracked!")
```

In the final `else` block that runs when the product URL is valid, we call the `add_product` method to store the product in the database.

Let's commit everything:

```bash
git add .
git commit -m "Add a Postgres database integration for tracking product URLs"
```

### Step 6: Storing price histories for new products

Now, after the product is added to the `products` table, we want to add its details and its scraped price to the `price_histories` table.

First, switch to `database.py` and add a new method for creating entries in the `PriceHistories` table:

```python
class Database:
    ...  # the rest of the class

    def add_price(self, product_data):
        session = self.Session()
        try:
            price_history = PriceHistory(
                id=f"{product_data['url']}_{product_data['timestamp']}",
                product_url=product_data["url"],
                name=product_data["name"],
                price=product_data["price"],
                currency=product_data["currency"],
                main_image_url=product_data["main_image_url"],
                timestamp=product_data["timestamp"],
            )
            session.add(price_history)
            session.commit()
        finally:
            session.close()
```

The `add_price` method takes a dictionary containing product data (which is returned by our scraper) and creates a new entry in the `PriceHistory` table. The entry's ID is generated by combining the product URL with a timestamp. The method stores essential product information like name, price, currency, image URL, and the timestamp of when the price was recorded. It uses SQLAlchemy's session management to safely commit the new price history entry to the database.

Now, we need to add this functionality to the sidebar as well. In `ui.py`, add a new import statement that loads the `scrape_product` function from `scraper.py`:

```python
...  # The rest of the imports
from scraper import scrape_product
```

Then, update the `else` block in the sidebar again:

```python
with st.sidebar:
    st.title("Add New Product")
    product_url = st.text_input("Product URL")
    add_button = st.button("Add Product")

    if add_button:
        if not product_url:
            st.error("Please enter a product URL")
        elif not is_valid_url(product_url):
            st.error("Please enter a valid URL")
        else:
            db.add_product(product_url)
            with st.spinner("Added product to database. Scraping product data..."):
                product_data = scrape_product(product_url)
                db.add_price(product_data)
            st.success("Product is now being tracked!")
```

Now when a user enters a product URL and clicks the "Add Product" button, several things happen:

1. The URL is validated to ensure it's not empty and is properly formatted.
2. If valid, the URL is added to the products table via `add_product()`.
3. The product page is scraped immediately to get current price data.
4. This initial price data is stored in the price history table via `add_price()`.
5. The user sees loading spinners and success messages throughout the process.

This gives us a complete workflow for adding new products to track, including capturing their initial price point. The UI provides clear feedback at each step and handles errors gracefully.

Check that everything is working the way we want it and then, commit the new changes:

```bash
git add .
git commit -m "Add a feature to track product prices after they are added"
```

### Step 7: Displaying each product's price history in the main dashboard

Let's take a look at the final product shown in the introduction once again:

![Screenshot of a minimalist price tracking dashboard showing product price history charts, add/remove product controls, and notification settings for monitoring e-commerce deals and price drops](amazon-price-tracking-images/sneak-peek.png)

Apart from the sidebar, the main dashboard shows each product's price history visualized with a Plotly line plot where the X axis is the timestamp while the Y axis is the prices. Each line plot is wrapped in a Streamlit component that includes buttons for removing the product from the database or visiting its source URL.

In this step, we will implement the plotting feature and leave the two buttons for a later section. First, add a new method to the `Database` class for retrieving the price history for each product:

```python
class Database:
    ...  # The rest of the code

    def get_price_history(self, url):
        """Get price history for a product"""
        session = self.Session()
        try:
            return (
                session.query(PriceHistory)
                .filter(PriceHistory.product_url == url)
                .order_by(PriceHistory.timestamp.desc())
                .all()
            )
        finally:
            session.close()
```

The method queries the price histories table based on product URL, orders the rows in descending order (oldest first) and returns the results.

Then, add another method for retrieving all products from the `products` table:

```python
class Database:
    ...
    
    def get_all_products(self):
        session = self.Session()
        try:
            return session.query(Product).all()
        finally:
            session.close()
```

The idea is that every time our Streamlit app is opened, the main dashboard queries all existing products from the database and render their price histories with line charts in dedicated components.

To create the line charts, we need Plotly and Pandas, so install them in your environment:

```bash
pip install pandas plotly
echo "pandas\nplotly\n" >> requirements.txt
```

Afterward, import them at the top of `ui.py` along with other existing imports:

```python
import pandas as pd
import plotly.express as px
```

Then, switch to `ui.py` and paste the following snippet of code after the Main content section:

```python
# Main content
st.title("Price Tracker Dashboard")
st.markdown("## Tracked Products")

# Get all products
products = db.get_all_products()
```

Here, after the page title and subtitle is shown, we are retrieving all products from the database. Let's loop over them:

```python
# Create a card for each product
for product in products:
    price_history = db.get_price_history(product.url)
    if price_history:
        # Create DataFrame for plotting
        df = pd.DataFrame(
            [
                {"timestamp": ph.timestamp, "price": ph.price, "name": ph.name}
                for ph in price_history
            ]
        )
```

For each product, we get their price history with `db.get_price_history` and then, convert this data into a dataframe with three columns:

- Timestamp
- Price
- Product name

This makes plotting easier with Plotly. Next, we create a Streamlit expander component for each product:

```python
# Create a card for each product
for product in products:
    price_history = db.get_price_history(product.url)
    if price_history:
        ...
        # Create a card-like container for each product
        with st.expander(df["name"][0], expanded=False):
            st.markdown("---")
            col1, col2 = st.columns([1, 3])

            with col1:
                if price_history[0].main_image_url:
                    st.image(price_history[0].main_image_url, width=200)
                st.metric(
                    label="Current Price",
                    value=f"{price_history[0].price} {price_history[0].currency}",
                )
```

The expander shows the product name as its title and contains:

1. A divider line
2. Two columns:
   - Left column: Product image (if available) and current price metric
   - Right column (shown in next section)

The price is displayed using Streamlit's metric component which shows the current price and currency.

Here is the rest of the code:

```python
            ...
            
            with col2:
                # Create price history plot
                fig = px.line(
                    df,
                    x="timestamp",
                    y="price",
                    title=None,
                )
                fig.update_layout(
                    xaxis_title=None,
                    yaxis_title="Price",
                    showlegend=False,
                    margin=dict(l=0, r=0, t=0, b=0),
                    height=300,
                )
                fig.update_xaxes(tickformat="%Y-%m-%d %H:%M", tickangle=45)
                fig.update_yaxes(tickprefix=f"{price_history[0].currency} ", tickformat=".2f")
                st.plotly_chart(fig, use_container_width=True)
```

In the right column, we create an interactive line plot using Plotly Express to visualize the price history over time. The plot shows price on the y-axis and timestamp on the x-axis. The layout is customized to remove the title, adjust axis labels and formatting, and optimize the display size. The timestamps are formatted to show date and time, with angled labels for better readability. Prices are displayed with 2 decimal places and a dollar sign prefix. The plot is rendered using Streamlit's `plotly_chart` component and automatically adjusts its width to fill the container.

After this step, the UI must be fully functional and ready to track products. For example, here is what mine looks like after adding a couple of products:

![Screenshot of a price tracking dashboard showing multiple product listings with price history charts, product images, and current prices for Amazon items](amazon-price-tracking-images/finished.png)

But notice how the price history chart doesn't show anything. That's because we haven't populated it by checking the product price in regular intervals. Let's do that in the next couple of steps. For now, commit the latest changes we've made:

```bash
git add .
git commit -m "Display product price histories for each product in the dashboard"
```

------------

Let's take a brief moment to summarize the steps we took so far and what's next. So far, we've built a Streamlit interface that allows users to add product URLs and displays their current prices and basic information. We've implemented the database schema, created functions to scrape product data, and designed a clean UI with price history visualization. The next step is to set up automated price checking to populate our history charts and enable proper price tracking over time.

### Step 8: Adding new price entries for existing products

Now, we want to write a script that adds new price entries in the `price_histories` table for each product in `products` table. We call this script `check_prices.py`:

```python
import os
from database import Database
from dotenv import load_dotenv
from firecrawl import FirecrawlApp
from scraper import scrape_product

load_dotenv()

db = Database(os.getenv("POSTGRES_URL"))
app = FirecrawlApp()
```

At the top, we are importing the functions and packages and initializing the database and a Firecrawl app. Then, we define a simple `check_prices` function:

```python
def check_prices():
    products = db.get_all_products()

    for product in products:
        try:
            updated_product = scrape_product(product.url)
            db.add_price(updated_product)
            print(f"Added new price entry for {updated_product['name']}")
        except Exception as e:
            print(f"Error processing {product.url}: {e}")


if __name__ == "__main__":
    check_prices()
```

In the function body, we retrieve all products URLs, retrieve their new price data with `scrape_product` function from `scraper.py` and then, add a new price entry for the product with `db.add_price`.

If you run the function once and refresh the Streamlit app, you must see a line chart appear for each product you are tracking:

![Screenshot of a price tracking dashboard showing a line chart visualization of product price history over time, with price on the y-axis and dates on the x-axis](amazon-price-tracking-images/linechart.png)

Let's commit the changes in this step:

```bash
git add .
git commit -m "Add a script for checking prices of existing products"
```

### Step 9: Check prices regularly with GitHub actions

GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate various software workflows directly from your GitHub repository. In our case, it's particularly useful because we can set up automated price checks to run the `check_prices.py` script at regular intervals (e.g., daily or hourly) without manual intervention. This ensures we consistently track price changes and maintain an up-to-date database of historical prices for our tracked products.

So, the first step is creating a new GitHub repository for our project and pushing existing code to it:

```bash
git remote add origin https://github.com/yourusername/price-tracker.git
git push origin main
```

Then, return to your terminal and create this directory structure:

```bash
mkdir -p .github/workflows
touch .github/workflows/check_prices.yml
```

The first command creates a new directory structure `.github/workflows` using the `-p` flag to create parent directories if they don't exist.

The second command creates an empty YAML file called `check_prices.yml` inside the workflows directory. GitHub Actions looks for workflow files in this specific location - any YAML files in the `.github/workflows` directory will be automatically detected and processed as workflow configurations. These YAML files define when and how your automated tasks should run, what environment they need, and what commands to execute. In our case, this file will contain instructions for GitHub Actions to periodically run our price checking script. Let's write it:

```yaml
name: Price Check

on:
  schedule:
    # Runs every 3 minutes
    - cron: "*/3 * * * *"
  workflow_dispatch: # Allows manual triggering
```

Let's break down this first part of the YAML file:

The `name: Price Check` line gives our workflow a descriptive name that will appear in the GitHub Actions interface.

The `on:` section defines when this workflow should be triggered. We've configured two triggers:

1. A schedule using cron syntax `*/3 * * * *` which runs the workflow every 3 minutes. The five asterisks represent minute, hour, day of month, month, and day of week respectively. The `*/3` means "every 3rd minute". The 3-minute interval is for debugging purposes, we will need to choose a wider interval later on to respect the free limits of GitHub actions.

2. `workflow_dispatch` enables manual triggering of the workflow through the GitHub Actions UI, which is useful for testing or running the check on-demand.

Now, let's add the rest:

```yaml
jobs:
  check-prices:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run price checker
        env:
          FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}
          POSTGRES_URL: ${{ secrets.POSTGRES_URL }}
        run: python check_prices.py
```

Let's break down this second part of the YAML file:

The `jobs:` section defines the actual work to be performed. We have one job named `check-prices` that runs on an Ubuntu virtual machine (`runs-on: ubuntu-latest`).

Under `steps:`, we define the sequence of actions:

1. First, we checkout our repository code using the standard `actions/checkout@v4` action

2. Then we set up Python 3.10 using `actions/setup-python@v5`, enabling pip caching to speed up dependency installation

3. Next, we install our Python dependencies by upgrading `pip` and installing requirements from our `requirements.txt` file. At this point, it is essential that you were keeping a complete dependency file based on the installs we made in the project.

4. Finally, we run our price checker script, providing two environment variables:
   - `FIRECRAWL_API_KEY`: For accessing the web scraping service
   - `POSTGRES_URL`: For connecting to our database

Both variables must be stored in our GitHub repository as secrets for this workflow file to run without errors. So, navigate to the repository you've created for the project and open its Settings. Under "Secrets and variables" > "Actions", click on "New repository secret" button to add the environment variables we have in the `.env` file one-by-one.

Then, return to your terminal, commit the changes and push:

```bash
git add . 
git commit -m "Add a workflow to check prices regularly"
git push origin main
```

Next, navigate to your GitHub repository again and click on the "Actions" tab:

![Screenshot of GitHub Actions interface showing workflow runs and manual trigger button for automated price tracking application](amazon-price-tracking-images/actions.png)

From there, you can run the workflow manually (click "Run workflow" and refresh the page). If it is executed successfully, you can return to the Streamlit app and refresh to see the new price added to the chart.

### Step 10: Setting up Discord for notifications

Now that we know our scheduling workflow works, the first order of business is setting a wider check interval in the workflow file. Even though our first workflow run was manually, the rest happen automatically.

```bash
on:
  schedule:
    # Runs every 6 hours
    - cron: "0 0,6,12,18 * * *"
  workflow_dispatch: # Allows manual triggering
```

The cron syntax `0 0,6,12,18 * * *` can be broken down as follows:

- First `0`: Run at minute 0
- `0,6,12,18`: Run at hours 0 (midnight), 6 AM, 12 PM (noon), and 6 PM
- First `*`: Run every day of the month
- Second `*`: Run every month
- Third `*`: Run every day of the week

So this schedule will check prices four times daily: at midnight, 6 AM, noon, and 6 PM (UTC time). This spacing helps stay within GitHub Actions' free tier limits while still catching most price changes.

Now, commit and push the changes:

```bash
git add .
git commit -m "Set a wider check interval in the workflow file"
git push origin main
```

Now comes the interesting part. Each time the workflow is run, we want to compare the current price of the product to its original price when we started tracking it. If the difference between these two prices exceeds a certain threshold like 5%, this means there is a discount happening for the product and we want to send a notification.

The easiest way to set this up is by using Discord webhooks. So, if you don't have one already, go to Discord.com and create a new account (optionally, download the desktop app as well). Then, setting up Discord notifications requires a few careful steps:

1. **Create a discord server**
   - Click the "+" button in the bottom-left corner of Discord
   - Choose "Create My Own" ‚Üí "For me and my friends"
   - Give your server a name (e.g., "Price Alerts")

2. **Create a channel for alerts**
   - Your server comes with a #general channel by default
   - You can use this or create a new channel called #price-alerts
   - Right-click the channel you want to use

3. **Set up the webhook**
   - Select "Edit Channel" from the right-click menu
   - Go to the "Integrations" tab
   - Click "Create Webhook"
   - Give it a name like "Price Alert Bot"
   - The webhook URL will be generated automatically
   - Click "Copy Webhook URL" - this is your unique notification endpoint

4. **Secure the webhook URL**
   - Never share or commit your webhook URL directly
   - Add it to your `.env` file as `DISCORD_WEBHOOK_URL`
   - Add it to your GitHub repository secrets
   - The URL should look something like: `https://discord.com/api/webhooks/...`

This webhook will serve as a secure endpoint that our price tracker can use to send notifications directly to your Discord channel.

Webhooks are automated messages sent from apps to other apps in real-time. They work like a notification system - when something happens in one app, it automatically sends data to another app through a unique URL. In our case, we'll use Discord webhooks to automatically notify us when there's a price drop. Whenever our price tracking script detects a significant discount, it will send a message to our Discord channel through the webhook URL, ensuring we never miss a good deal.

After copying the webhook URL, you should save it as environment variable to your `.env` file:

```python
echo "DISCORD_WEBHOOK_URL='THE-URL-YOU-COPIED'" >> .env
```

Now, create a new file called `notifications.py` and paste the following contents:

```python
from dotenv import load_dotenv
import os
import aiohttp
import asyncio

load_dotenv()


async def send_price_alert(
    product_name: str, old_price: float, new_price: float, url: str
):
    """Send a price drop alert to Discord"""
    drop_percentage = ((old_price - new_price) / old_price) * 100

    message = {
        "embeds": [
            {
                "title": "Price Drop Alert! üéâ",
                "description": f"**{product_name}**\nPrice dropped by {drop_percentage:.1f}%!\n"
                f"Old price: ${old_price:.2f}\n"
                f"New price: ${new_price:.2f}\n"
                f"[View Product]({url})",
                "color": 3066993,
            }
        ]
    }

    try:
        async with aiohttp.ClientSession() as session:
            await session.post(os.getenv("DISCORD_WEBHOOK_URL"), json=message)
    except Exception as e:
        print(f"Error sending Discord notification: {e}")
```

The `send_price_alert` function above is responsible for sending price drop notifications to Discord using webhooks. Let's break down what's new:

1. The function takes 4 parameters:
   - `product_name`: The name of the product that dropped in price
   - `old_price`: The previous price before the drop
   - `new_price`: The current lower price
   - `url`: Link to view the product

2. It calculates the percentage drop in price using the formula: `((old_price - new_price) / old_price) * 100`

3. The notification is formatted as a Discord embed - a rich message format that includes:
   - A title with a celebration emoji
   - A description showing the product name, price drop percentage, old and new prices
   - A link to view the product
   - A green color (3066993 in decimal)

4. The message is sent asynchronously using `aiohttp` to post to the Discord webhook URL stored in the environment variables

5. Error handling is included to catch and print any issues that occur during the HTTP request

This provides a clean way to notify users through Discord whenever we detect a price drop for tracked products.

To check the notification system works, add this main block to the end of the script:

```python
if __name__ == "__main__":
    asyncio.run(send_price_alert("Test Product", 100, 90, "https://www.google.com"))
```

`asyncio.run()` is used here because `send_price_alert` is an async function that needs to be executed in an event loop. `asyncio.run()` creates and manages this event loop, allowing the async HTTP request to be made properly. Without it, we wouldn't be able to use the `await` keyword inside `send_price_alert`.

To run the script, install `aiohttp`:

```python
pip install aiohttp
echo "aiohttp\n" >> requirements.txt
python notifications.py
```

If all is well, you should get a Discord message in your server that looks like this:

![Screenshot of a Discord notification showing a price drop alert with product details, original price, new discounted price and percentage savings](amazon-price-tracking-images/alert.png)

Let's commit the changes we have:

```bash
git add .
git commit -m "Set up Discord alert system"
```

Also, don't forget to add the Discord webhook URL to your GitHub repository secrets!

### Step 11: Sending Discord alerts when prices drop

Now, the only step left is adding a price comparison logic to `check_prices.py`. In other words, we want to use the `send_price_alert` function if the new scraped price is lower than the original. This requires a revamped `check_prices.py` script:

```python
import os
import asyncio
from database import Database
from dotenv import load_dotenv
from firecrawl import FirecrawlApp
from scraper import scrape_product
from notifications import send_price_alert

load_dotenv()

db = Database(os.getenv("POSTGRES_URL"))
app = FirecrawlApp()

# Threshold percentage for price drop alerts (e.g., 5% = 0.05)
PRICE_DROP_THRESHOLD = 0.05


async def check_prices():
    products = db.get_all_products()
    product_urls = set(product.url for product in products)

    for product_url in product_urls:
        # Get the price history
        price_history = db.get_price_history(product_url)
        if not price_history:
            continue

        # Get the earliest recorded price
        earliest_price = price_history[-1].price

        # Retrieve updated product data
        updated_product = scrape_product(product_url)
        current_price = updated_product["price"]

        # Add the price to the database
        db.add_price(updated_product)
        print(f"Added new price entry for {updated_product['name']}")

        # Check if price dropped below threshold
        if earliest_price > 0:  # Avoid division by zero
            price_drop = (earliest_price - current_price) / earliest_price
            if price_drop >= PRICE_DROP_THRESHOLD:
                await send_price_alert(
                    updated_product["name"], earliest_price, current_price, product_url
                )


if __name__ == "__main__":
    asyncio.run(check_prices())
```

Let's examine the key changes in this enhanced version of `check_prices.py`:

1. New imports and setup
   - Added `asyncio` for `async`/`await` support
   - Imported `send_price_alert` from `notifications.py`
   - Defined `PRICE_DROP_THRESHOLD = 0.05` (5% threshold for alerts)

2. Async function conversion
   - Converted `check_prices()` to async function
   - Gets unique product URLs using set comprehension to avoid duplicates

3. Price history analysis
   - Retrieves full price history for each product
   - Gets `earliest_price` from `history[-1]` (works because we ordered by timestamp DESC)
   - Skips products with no price history using `continue`

4. Price drop detection logic
   - Calculates drop percentage: `(earliest_price - current_price) / earliest_price`
   - Checks if drop exceeds 5% threshold
   - Sends Discord alert if threshold exceeded using `await send_price_alert()`

5. Async main block
   - Uses `asyncio.run()` to execute async `check_prices()` in event loop

When I tested this new version of the script, I immediately got an alert:

![Screenshot of a Discord notification showing a price drop alert for an Amazon product, displaying the original and discounted prices with percentage savings](amazon-price-tracking-images/new-alert.png)

Before we supercharge our workflow with the new notification system, you should add this line of code to your `check_prices.yml` workflow file to read the Discord webhook URL from your GitHub secrets:

```python
...
    - name: Run price checker
        env:
          FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}
          POSTGRES_URL: ${{ secrets.POSTGRES_URL }}
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
        run: python automated_price_tracking/check_prices.py
```

Finally, let's commit everything and push to GitHub so that our workflow is supercharged with our notification system:

```bash
git add .
git commit -m "Add notification system to price drops"
git push origin main
```

## Limitations of Free Tier Tools Used in the Tutorial

Before wrapping up, let's quickly review the limitations of the free tools we used in this tutorial:

- GitHub Actions: Limited to 2,000 minutes per month for free accounts. Consider increasing the cron interval to stay within limits.
- Supabase: Free tier includes 500MB database storage and limited row count. Monitor usage if tracking many products.
- Firecrawl: Free API tier allows 500 requests per month. This means that at 6 hour intervals, you can track up to four products in the free plan.
- Streamlit Cloud: Free hosting tier has some memory/compute restrictions and goes to sleep after inactivity.

While these limitations exist, they're quite generous for personal use and learning. The app will work well for tracking a reasonable number of products with daily price checks.

## Conclusion and Next Steps

Congratulations for making it to the end of this extremely long tutorial! We've just covered how to implement an end-to-end Python project you can proudly showcase on your portfolio. We built a complete price tracking system that scrapes product data from e-commerce websites, stores it in a Postgres database, analyzes price histories, and sends automated Discord notifications when prices drop significantly. Along the way, we learned about web scraping with Firecrawl, database management with SQLAlchemy, asynchronous programming with asyncio, building interactive UIs with Streamlit, automating with GitHub actions and integrating external webhooks.

However, the project is far from perfect. Since we took a top-down approach to building this app, our project code is scattered across multiple files and often doesn't follow programming best practices. For this reason, I've recreated the same project in a much more sophisticated manner with production-level features. [This new version on GitHub](https://github.com/BexTuychiev/automated-price-tracking) implements proper database session management, faster operations and overall smoother user experience. Also, this version includes buttons for removing products from the database and visiting them through the app.

If you decide to stick with the basic version, you can find the full project code and notebook in [the official Firecrawl GitHub repository's example projects](https://github.com/mendableai/firecrawl/tree/main/examples/automated_price_tracking). I also recommend that you [deploy your Streamlit app to Streamlit Cloud](https://share.streamlit.io) so that you have a functional app accessible everywhere you go.

Here are some further improvements you might consider for the app:

- Improve the price comparison logic: the app compares the current price to the oldest recorded price, which might not be ideal. You may want to compare against recent price trends instead.
- No handling of currency conversion if products use different currencies.
- The Discord notification system doesn't handle rate limits or potential webhook failures gracefully.
- No error handling for Firecrawl scraper - what happens if the scraping fails?
- No consistent usage of logging to help track issues in production.
- No input URL sanitization before scraping.

Some of these features are implemented in [the advanced version of the project](https://github.com/BexTuychiev/automated-price-tracking), so definitely check it out!

Here are some more guides from our blog if you are interested:

- [How to Run Web Scrapers on Schedule](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025)
- [More about using Firecrawl's `scrape_url` function](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint)
- [Scraping entire websites with Firecrawl in a single command - the /crawl endpoint](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl)

Thank you for reading!

================
File: examples/blog-articles/deploying_web_scrapers/notebook.ipynb
================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Deploy Python Web Scrapers Online in 2025 For Free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping projects may start on your machine, but unless you're willing to ship your laptop to random strangers on the Internet, you'll need cloud services.\n",
    "\n",
    "![](images/meme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many compelling reasons to move them to the cloud and make them more reliable. This guide explores several methods for automating and deploying web scrapers in 2025, focusing on free solutions.\n",
    "\n",
    "Here is a general outline of concepts we will cover:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setting up automated scraping with GitHub Actions\n",
    "- Deploying to PaaS platforms (Heroku, PythonAnywhere)\n",
    "- Implementing serverless solutions (AWS Lambda, Google Cloud Functions)\n",
    "- Container-based deployments with Docker and Kubernetes\n",
    "- Best practices for monitoring, security, and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Move Web Scrapers to the Cloud?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number one reason to deploy scrapers to the cloud is reliability. Cloud-based scrapers run 24/7, without cigarette breaks and the best part, without depending on your local machine. \n",
    "\n",
    "Cloud-based scrapers also handle large-scale data operations more easily and gracefully, often juggling multiple scraping tasks. And, if you are a bit more aggressive in your request frequencies and get a dreaded IP ban, cloud services can give access to other IP addresses and geographic locations. \n",
    "\n",
    "Moreover, you are not limited by your laptop's specs because cloud gives you dedicated resources. While these resources may gouge a hole in your pocket for large-scale scraping operations, many platforms offer generous free tiers, as we will explore soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Choose the Right Deployment Method For Your Scrapers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are about to list 6-7 deployment methods in this article, so you might get a decision fatigue. To prevent that, give this section a cursory read as it helps you choose the right one based on your scale requirements, technical complexity, and budget. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll dive into three deployment tiers that match your scale - from lightweight solutions to heavy-duty scraping powerhouses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small scale (1-1000 requests/day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Best Options**: \n",
    "  - GitHub Actions\n",
    "  - PythonAnywhere\n",
    "  - Heroku (Free Tier)\n",
    "- **Why**: These platforms offer sufficient resources for basic scraping needs without cost\n",
    "- **Limitations**: Daily request caps and runtime restrictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medium Scale (1000-10000 requests/day)\n",
    "- **Best Options**:\n",
    "  - AWS Lambda\n",
    "  - Google Cloud Functions\n",
    "  - Docker containers on basic VPS\n",
    "- **Why**: Better handling of concurrent requests and flexible scaling\n",
    "- **Considerations**: Cost begins to factor in, but still manageable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large Scale (10000+ requests/day)\n",
    "- **Best Options**:\n",
    "  - Kubernetes clusters\n",
    "  - Multi-region serverless deployments\n",
    "  - Specialized scraping platforms\n",
    "- **Why**: Robust infrastructure for high-volume operations\n",
    "- **Trade-offs**: Higher complexity and cost vs. reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's categorize the methods based on how fast you can get them up and running. \n",
    "\n",
    "#### Low Complexity Solutions\n",
    "- **GitHub Actions**\n",
    "  - Pros: Simple setup, version control integration\n",
    "  - Cons: Limited customization\n",
    "- **PythonAnywhere**\n",
    "  - Pros: User-friendly interface\n",
    "  - Cons: Resource constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Medium Complexity Solutions\n",
    "- **Serverless (AWS Lambda/Google Functions)**\n",
    "  - Pros: Managed infrastructure, auto-scaling\n",
    "  - Cons: Learning curve for configuration\n",
    "- **Docker Containers**\n",
    "  - Pros: Consistent environments\n",
    "  - Cons: Container management overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### High Complexity Solutions\n",
    "- **Kubernetes**\n",
    "  - Pros: Ultimate flexibility and scalability\n",
    "  - Cons: Significant operational overhead\n",
    "- **Custom Infrastructure**\n",
    "  - Pros: Complete control\n",
    "  - Cons: Requires DevOps expertise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Budget Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of cost, all methods in the article has the following generous free tiers or next-to-nothing cheap starting plans:\n",
    "\n",
    "- GitHub Actions: 2000 minutes/month\n",
    "- Heroku: $5 and up\n",
    "- AWS Lambda: starting at 0.2$ per 1 million requests\n",
    "- Google Cloud Functions: 2 million invocations/month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These limits are based on various cost factors like compute time, data transfer speeds, storage requirements and additional services like databases or monitoring.\n",
    "\n",
    "Here is a little decision matrix to distill all this information:\n",
    "\n",
    "| Factor          | Small Project | Medium Project | Large Project |\n",
    "|-----------------|---------------|----------------|---------------|\n",
    "| Best Platform   | GitHub Actions| AWS Lambda     | Kubernetes   |\n",
    "| Monthly Cost    | $0           | $10-50         | $100+        |\n",
    "| Setup Time      | 1-2 hours    | 1-2 days       | 1-2 weeks    |\n",
    "| Maintenance     | Minimal      | Moderate       | Significant  |\n",
    "| Scalability     | Limited      | Good           | Excellent    |\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start small with simpler platforms and gather data on your scraping needs by measuring actual usage patterns. From there, you can gradually scale and potentially implement hybrid approaches that balance complexity and benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article assumes familiarity with web scraping fundamentals like HTML parsing, CSS selectors, HTTP requests, and handling dynamic content. You should also be comfortable with Python basics including functions, loops, and working with external libraries. Basic knowledge of command line tools and git version control will be essential for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting with any deployment method, you'll need to create accounts on these platforms:\n",
    "\n",
    "1. **[GitHub account](github.com)** (Required)\n",
    "   - Needed for version control and GitHub Actions\n",
    "\n",
    "2. **Cloud Platform Account** (Choose at least one)\n",
    "   - [AWS account](https://signin.aws.amazon.com/signup?request_type=register) (for Lambda)\n",
    "   - [Google Cloud account](https://cloud.google.com/free) (for Cloud Functions)\n",
    "   - [Heroku account](https://signup.heroku.com/)\n",
    "   - [PythonAnywhere account](https://www.pythonanywhere.com/)\n",
    "\n",
    "3. **[Docker Hub account](https://hub.docker.com/)** (Optional)\n",
    "   - Only needed if using container-based deployments\n",
    "   - Required for storing and sharing Docker images\n",
    "\n",
    "4. **[Firecrawl account](https://firecrawl.dev)** (Optional)\n",
    "   - Only needed if you decide to use an AI-based scraper (more on Firecrawl soon).  \n",
    "\n",
    "Note: Most cloud platforms require a credit card for verification, even when using free tiers. However, they won't charge you unless you exceed free tier limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a basic scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate deployment concepts effectively, we'll start by building a basic web scraper using Firecrawl, a modern scraping API that simplifies many common challenges.\n",
    "\n",
    "[Firecrawl](https://docs.firecrawl.dev) offers several key advantages compared to traditional Python web scraping libraries:\n",
    "\n",
    "- Dead simple to use with only a few dependencies\n",
    "- Handles complex scraping challenges automatically (proxies, anti-bot mechanisms, dynamic JS content)\n",
    "- Converts web content into clean, LLM-ready markdown format\n",
    "- Supports multiple output formats (markdown, structured data, screenshots, HTML)\n",
    "- Reliable extraction with retry mechanisms and error handling\n",
    "- Supports custom actions (click, scroll, input, wait) before data extraction\n",
    "- Geographic location customization for avoiding IP bans\n",
    "- Built-in rate limiting and request management\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "As an example, we will build a simple scraper for [ProductHunt](https://www.producthunt.com/). Specifically, we will scrape the \"Yesterday's Top Products\" list from the homepage:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![](images/ph-homepage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scraper extracts the following information from each product:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![](images/ph-sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get building:\n",
    "\n",
    "```bash\n",
    "$ mkdir product-hunt-scraper\n",
    "$ cd product-hunt-scraper\n",
    "$ touch scraper.py .env\n",
    "$ python -m venv venv\n",
    "$ source venv/bin/activate\n",
    "$ pip install pydantic firecrawl-py\n",
    "$ echo \"FIRECRAWL_API_KEY='your-api-key-here' >> .env\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These commands set up a working directory for the scraper, along with a virtual environment and the main script. The last part also saves your Firecrawl API key, which you can get through their free plan by [signing up for an account](firecrawl.dev).\n",
    "\n",
    "\n",
    "Let's work on the code now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from firecrawl import FirecrawlApp\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import a few packages and the `FirecrawlApp` class - we use it to establish a connection with Firecrawl's scraping engine. Then, we define a Pydantic class outlining the details we want to scrape from each product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product(BaseModel):\n",
    "    name: str = Field(description=\"The name of the product\")\n",
    "    description: str = Field(description=\"A short description of the product\")\n",
    "    url: str = Field(description=\"The URL of the product\")\n",
    "    \n",
    "    topics: list[str] = Field(\n",
    "        description=\"A list of topics the product belongs to. Can be found below the product description.\"\n",
    "    )\n",
    "    \n",
    "    n_upvotes: int = Field(description=\"The number of upvotes the product has\")\n",
    "    n_comments: int = Field(description=\"The number of comments the product has\")\n",
    "    \n",
    "    rank: int = Field(\n",
    "        description=\"The rank of the product on Product Hunt's Yesterday's Top Products section.\"\n",
    "    )\n",
    "    logo_url: str = Field(description=\"The URL of the product's logo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field descriptions in this class play a crucial role in guiding the LLM scraping engine. By providing natural language descriptions for each field, we tell the LLM exactly what information to look for and where to find it on the page. For example, when we say \"A list of topics under the product description\", the LLM understands both the content we want (topics) and its location (below the description). \n",
    "\n",
    "This natural language approach allows Firecrawl to intelligently parse the page's HTML structure and extract the right information without requiring explicit CSS selectors or XPaths. The LLM analyzes the semantic meaning of our descriptions and matches them to the appropriate elements on the page.\n",
    "\n",
    "This approach offers two practical advantages: it reduces initial development time since you don't need to manually inspect HTML structures, and it provides long-lasting resilience against HTML changes. Since the LLM understands the semantic meaning of the elements rather than relying on specific selectors, it can often continue working even when class names or IDs are updated. This makes it suitable for scenarios where long-term maintenance is a consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting back to code, we write another Pydantic class for scraping a collection of Products from the 'Yesterday's Top Products' list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YesterdayTopProducts(BaseModel):\n",
    "    products: list[Product] = Field(\n",
    "        description=\"A list of top products from yesterday on Product Hunt.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `YesterdayTopProducts` parent class is essential - without it, Firecrawl would only scrape a single product instead of the full list. This happens because Firecrawl strictly adheres to the provided schema structure, ensuring consistent and reliable output on every scraping run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a function that scrapes ProductHunt based on the schema we just define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.producthunt.com\"\n",
    "\n",
    "\n",
    "def get_yesterday_top_products():\n",
    "    app = FirecrawlApp()\n",
    "\n",
    "    data = app.scrape_url(\n",
    "        BASE_URL,\n",
    "        params={\n",
    "            \"formats\": [\"extract\"],\n",
    "            \"extract\": {\n",
    "                \"schema\": YesterdayTopProducts.model_json_schema(),\n",
    "                \n",
    "                \"prompt\": \"Extract the top products listed under the 'Yesterday's Top Products' section. There will be exactly 5 products.\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return data[\"extract\"][\"products\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function initializes a Firecrawl app, which reads your Firecrawl API key stored in an `.env` file and scrapes the URL. Notice the parameters being passed to the `scrape_url()` method:\n",
    "\n",
    "- `formats` specifies how the data should be scraped and extracted. Firecrawl supports other formats like markdown, HTML, screenshots or links.\n",
    "- `schema`: The JSON schema produced by the Pydantic class\n",
    "- `prompt`: A general prompt guiding the underlying LLM on what to do. Providing a prompt usually improves the performance.\n",
    "\n",
    "In the end, the function returns the extracted products, which will be a list of dictionaries. \n",
    "\n",
    "The final step is writing a function to save this data to a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_yesterday_top_products():\n",
    "    products = get_yesterday_top_products()\n",
    "    \n",
    "    date_str = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    filename = f\"ph_top_products_{date_str}.json\"\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(products, f)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    save_yesterday_top_products()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function runs the previous one and saves the returned data to a JSON file identifiable with the following day's date. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just built a scraper that resiliently extracts data from ProductHunt. To keep things simple, we implemented everything in a single script with straightforward data persistence. In production environments with larger data flows and more complex websites, your project would likely span multiple directories and files.\n",
    "\n",
    "Nevertheless, the deployment methods we'll explore work for nearly any type of project. You can always restructure your own projects to follow this pattern of having a single entry point that coordinates all the intermediate scraping stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Web Scrapers With GitHub Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Actions is a powerful CI/CD platform built into GitHub that allows you to automate workflows, including running scrapers on a schedule. It provides a simple way to deploy and run automated tasks without managing infrastructure, making it an ideal choice for web scraping projects.\n",
    "\n",
    "To get started, initialize Git and commit the work we've completed so far in your working directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "$ git init\n",
    "$ touch .gitignore\n",
    "$ echo \".env\" >> .gitignore  # Remove the .env file from Git indexing\n",
    "$ git add .\n",
    "$ git commit -m \"Initial commit\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create an empty GitHub repository, copy its link and set it as the remote for your local repo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ git remote add origin your-repo-link\n",
    "$ git push\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating workflow files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow files are YAML configuration files that tell GitHub Actions how to automate tasks in your repository. For our web scraping project, these files will define when and how to run our scraper.\n",
    "\n",
    "These files live in the `.github/workflows` directory of your repository and contain instructions for:\n",
    "- When to trigger the workflow (like on a schedule or when code is pushed)\n",
    "- What environment to use (Python version, dependencies to install)\n",
    "- The actual commands to run your scraper\n",
    "- What to do with the scraped data\n",
    "\n",
    "Each workflow file acts like a recipe that GitHub Actions follows to execute your scraper automatically. This automation is perfect for web scraping since we often want to collect data on a regular schedule without manual intervention.\n",
    "\n",
    "For our scraper, we need a single workflow file that executes the `scraper.py` file. Let's set it up:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ mkdir -p .github/workflows\n",
    "$ touch .github/workflows/ph-scraper.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the newly-created file and paste the following contents:\n",
    "\n",
    "\n",
    "```yml\n",
    "name: Product Hunt Scraper\n",
    "\n",
    "on:\n",
    "  schedule:\n",
    "    - cron: '0 1 * * *'  # Runs at 1 AM UTC daily\n",
    "  workflow_dispatch:  # Allows manual trigger\n",
    "\n",
    "permissions:\n",
    "  contents: write\n",
    "\n",
    "jobs:\n",
    "  scrape:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "    - name: Checkout repository\n",
    "      uses: actions/checkout@v3\n",
    "      with:\n",
    "          persist-credentials: true\n",
    "      \n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v4\n",
    "      with:\n",
    "        python-version: '3.10'\n",
    "        \n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install -r requirements.txt\n",
    "        \n",
    "    - name: Run scraper\n",
    "      env:\n",
    "        FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\n",
    "      run: python scraper.py\n",
    "        \n",
    "    - name: Commit and push if changes\n",
    "      run: |\n",
    "        git config --local user.email \"github-actions[bot]@users.noreply.github.com\"\n",
    "        git config --local user.name \"github-actions[bot]\"\n",
    "        git add *.json\n",
    "        git diff --quiet && git diff --staged --quiet || git commit -m \"Update ProductHunt data [skip ci]\"\n",
    "        git push\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The YAML file defines a GitHub Actions workflow for automated web scraping:\n",
    "\n",
    "name: Specifies the workflow name that appears in GitHub Actions UI\n",
    "\n",
    "on: Defines how workflow triggers:\n",
    "- schedule: Uses cron syntax to run daily at 1 AM UTC\n",
    "- workflow_dispatch: Enables manual workflow triggering\n",
    "\n",
    "jobs: Contains the workflow jobs:\n",
    "- scrape: Main job that runs on ubuntu-latest\n",
    "  - steps: Sequential actions to execute:\n",
    "    1. Checkout repository using `actions/checkout`\n",
    "    2. Setup Python 3.10 environment\n",
    "    3. Install project dependencies from `requirements.txt`\n",
    "    4. Run the scraper with environment variables\n",
    "    5. Commit and push any changes to the repository\n",
    "\n",
    "The workflow automatically handles repository interaction, dependency management, and data updates while providing both scheduled and manual execution options. You can read the [official GitHub guide on workflow file syntax](https://docs.github.com/en/actions/writing-workflows/workflow-syntax-for-github-actions#about-yaml-syntax-for-workflows) to learn more.\n",
    "\n",
    "For this workflow file to sun successfully, we need to take a couple of additional steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a `requirements.txt` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the steps in the workflow file is installing the dependencies for our project using a `requirements.txt` file, which is a standard format for listing packages used in your project. \n",
    "\n",
    "For simple projects, you can create this file manually and adding each package on a new line like:\n",
    "\n",
    "```text\n",
    "pydantic\n",
    "firecrawl-py\n",
    "```\n",
    "\n",
    "However, if you have a large project with multiple files and dozens of dependencies, you need an automated method. The simplest one I can suggest is using `pipreqs` package:\n",
    "\n",
    "```bash\n",
    "$ pip install pipreqs\n",
    "$ pipreqs . \n",
    "```\n",
    "\n",
    "`pipreqs` is a lightweight package that scans all Python scripts in your project and adds them to a new `requirements.txt` file with their used versions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you notice, the workflow file has a step that executes `scraper.py`:\n",
    "\n",
    "```yml\n",
    "- name: Run scraper\n",
    "      env:\n",
    "        FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\n",
    "      run: python scraper.py\n",
    "```\n",
    "\n",
    "The workflow retrieves environment variables using the `secrets.SECRET_NAME` syntax. Since the `.env` file containing your Firecrawl API key isn't uploaded to GitHub for security reasons, you'll need to store the key in your GitHub repository secrets.\n",
    "\n",
    "To add your API key as a secret:\n",
    "\n",
    "1. Navigate to your GitHub repository\n",
    "2. Click on \"Settings\" \n",
    "3. Select \"Secrets and variables\" then \"Actions\"\n",
    "4. Click \"New repository secret\"\n",
    "5. Enter \"FIRECRAWL_API_KEY\" as the name\n",
    "6. Paste your API key as the value\n",
    "7. Click \"Add secret\"\n",
    "\n",
    "This allows the workflow to securely access your API key during execution without exposing it in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the workflow, we need to commit all new changes and push them to GitHub:\n",
    "\n",
    "```python\n",
    "$ git add .\n",
    "$ git commit -m \"Descriptive commit message\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes our workflow visible to GitHub.\n",
    "\n",
    "At the top of the workflow file, we set a schedule for the workflow file to run at 1 AM using `cron` syntax:\n",
    "\n",
    "```yaml\n",
    "on:\n",
    "  schedule:\n",
    "    - cron: '0 1 * * *'  # Runs at 1 AM UTC daily\n",
    "  workflow_dispatch:  # Allows manual trigger\n",
    "```\n",
    "\n",
    "The `cron` syntax consists of 5 fields representing minute (0-59), hour (0-23), day of month (1-31), month (1-12), and day of week (0-6, where 0 is Sunday). Each field can contain specific values, ranges (1-5), lists (1,3,5), or asterisks (*) meaning \"every\". For example, `0 1 * * *` means \"at minute 0 of hour 1 (1 AM UTC) on every day of every month\". Here are some more patterns:\n",
    "\n",
    "- `0 */2 * * *`: Every 2 hours\n",
    "- `0 9-17 * * 1-5`: Every hour from 9 AM to 5 PM on weekdays\n",
    "- `*/15 * * * *`: Every 15 minutes\n",
    "- `0 0 * * 0`: Every Sunday at midnight\n",
    "- `0 0 1 * *`: First day of every month at midnight\n",
    "- `30 18 * * 1,3,5`: Monday, Wednesday, Friday at 6:30 PM\n",
    "\n",
    "So, once the workflow file is pushed to GitHub, the scraper is scheduler to run. However, the `workflow_dispatch` parameter in the file allows us to run the scraper manually for debugging.\n",
    "\n",
    "![](images/workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to the Actions tab of your GitHub repository, click on the workflow name and press \"Run workflow\". In about a minute (if the workflow is successful), you will see the top five products from yesterday on ProductHunt saved as a JSON file to your repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you want to interrupt the scraping schedule, click on the three buttons in the top-right corner of the workflow page and disable it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Web Scrapers With Heroku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Heroku](heroku.com) is a Platform-as-a-Service (PaaS) that makes deploying applications straightforward, even for beginners. While it removed its generous free tier in 2022, its basic $5 \"dyno\" plan still has some free features we can take advantage of for the purposes of this tutorial. \n",
    "\n",
    "### Setting up Heroku\n",
    "\n",
    "First, install the Heroku CLI and login to your account:\n",
    "\n",
    "```bash\n",
    "$ brew install heroku/brew/heroku  # macOS\n",
    "$ curl https://cli-assets.heroku.com/install.sh | sh  # Linux\n",
    "$ heroku login  # Opens your web browser\n",
    "```\n",
    "\n",
    "Then, create a new Heroku app and set it as a remote for your repository:\n",
    "\n",
    "```bash\n",
    "$ heroku create ph-scraper-your-name  # Make the app name unique\n",
    "$ heroku git:remote -a ph-scraper-your-name\n",
    "```\n",
    "\n",
    "After this step, if you visit [dashboard.heroku.com](dashboard.heroku.com), your app must be visible. \n",
    "\n",
    "### Configuring the Application\n",
    "\n",
    "Heroku requires a few additional files to run your application. First, create a `Procfile` that tells Heroku what command to run:\n",
    "\n",
    "```bash\n",
    "$ touch Procfile\n",
    "$ echo \"worker: python scraper.py\" > Procfile\n",
    "```\n",
    "\n",
    "Next, create a `runtime.txt` to specify the Python version:\n",
    "\n",
    "```bash\n",
    "$ touch runtime.txt\n",
    "$ echo \"python-3.10.12\" > runtime.txt\n",
    "```\n",
    "\n",
    "### Environment Variables\n",
    "\n",
    "Instead of using a `.env` file, Heroku requires you to set your environment variables directly using the Heroku CLI:\n",
    "\n",
    "```bash\n",
    "$ heroku config:set FIRECRAWL_API_KEY='your-api-key-here'\n",
    "```\n",
    "\n",
    "You can verify the variables are set correctly with:\n",
    "\n",
    "```bash\n",
    "$ heroku config\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling Scraper Runs\n",
    "\n",
    "Heroku uses an add-on called [\"Scheduler\"](https://elements.heroku.com/addons/scheduler) for running periodic tasks. Install it with:\n",
    "\n",
    "```bash\n",
    "$ heroku addons:create scheduler:standard\n",
    "```\n",
    "\n",
    "Then open the scheduler dashboard:\n",
    "\n",
    "```bash\n",
    "$ heroku addons:open scheduler\n",
    "```\n",
    "\n",
    "In the web interface, add a new job with the command `python scraper.py` and set your desired frequency (daily, hourly, or every 10 minutes).\n",
    "\n",
    "![](images/heroku-scheduler.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment and Monitoring\n",
    "\n",
    "Now, to launch everything, you need to deploy your application by committing and pushing the local changes to Heroku:\n",
    "\n",
    "```bash\n",
    "$ git add .\n",
    "$ git commit -m \"Add Heroku-related files\"\n",
    "$ git push heroku main\n",
    "```\n",
    "\n",
    "You can periodically monitor the health of your application with the following command:\n",
    "\n",
    "```bash\n",
    "$ heroku logs --tail\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platform Limitations\n",
    "\n",
    "The basic $5 dyno has some important limitations to consider:\n",
    "\n",
    "- Sleeps after 30 minutes of inactivity\n",
    "- Limited to 512MB RAM\n",
    "- Shares CPU with other applications\n",
    "- Maximum of 23 hours active time per day\n",
    "\n",
    "For most small to medium scraping projects, these limitations aren't problematic. However, if you need more resources, you can upgrade to Standard ($25/month) or Performance ($250/month) dynos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Persistence\n",
    "\n",
    "Since Heroku's filesystem is temporary, you'll need to modify the scraper to store data externally. Here's a quick example using AWS S3:\n",
    "\n",
    "```python\n",
    "import boto3  # pip install boto3\n",
    "from datetime import datetime\n",
    "\n",
    "def save_yesterday_top_products():\n",
    "    products = get_yesterday_top_products()\n",
    "    \n",
    "    # Initialize S3 client\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Create filename with date\n",
    "    date_str = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    filename = f\"ph_top_products_{date_str}.json\"\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3.put_object(\n",
    "        Bucket='your-bucket-name',\n",
    "        Key=filename,\n",
    "        Body=json.dumps(products)\n",
    "    )\n",
    "```\n",
    "\n",
    "For this to work, you must already have an AWS account and an existing S3 bucket. Also, you must set your AWS credentials as Heroku secrets through the Heroku CLI:\n",
    "\n",
    "```bash\n",
    "$ heroku config:set AWS_ACCESS_KEY_ID='your-key'\n",
    "$ heroku config:set AWS_SECRET_ACCESS_KEY='your-secret'\n",
    "```\n",
    "\n",
    "Once you do, add `boto3` to the list of dependencies in your `requirements.txt` file:\n",
    "\n",
    "```\n",
    "$ echo \"boto3\" >> requirements.txt\n",
    "```\n",
    "\n",
    "Finally, commit and push the changes:\n",
    "\n",
    "```bash\n",
    "$ git add .\n",
    "$ git commit -m \"Switch data persistence to S3\"\n",
    "$ git push heroku main\n",
    "$ git push origin main\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can confirm that the app is functioning properly by setting the schedule frequency to 10 minutes and checking your S3 bucket for the JSON file containing the top five products from ProductHunt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Heroku Apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop your app, you can use a few different methods:\n",
    "\n",
    "1. Pause the dyno:\n",
    "\n",
    "```bash\n",
    "$ heroku ps:scale worker=0\n",
    "```\n",
    "\n",
    "This stops the worker dyno without deleting the app. To resume later:\n",
    "\n",
    "```bash\n",
    "$ heroku ps:scale worker=1\n",
    "```\n",
    "\n",
    "2. Disable the scheduler:\n",
    "\n",
    "```bash\n",
    "$ heroku addons:destroy scheduler\n",
    "```\n",
    "\n",
    "Or visit the Heroku dashboard and remove the scheduler add-on manually.\n",
    "\n",
    "3. Delete the entire app:\n",
    "\n",
    "```bash\n",
    "$ heroku apps:destroy --app your-app-name --confirm your-app-name\n",
    "```\n",
    "‚ö†Ô∏è Warning: This permanently deletes your app and all its data.\n",
    "\n",
    "4. Maintenance mode\n",
    "\n",
    "```bash\n",
    "$ heroku maintenance:on\n",
    "```\n",
    "\n",
    "This puts the app in maintenance mode. To disable:\n",
    "\n",
    "```bash\n",
    "$ heroku maintenance:off\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "To learn more about Heroku and how to run Python applications on its servers, please refer to [their Python support documentation](https://devcenter.heroku.com/categories/python-support)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Web Scrapers With PythonAnywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PythonAnywhere](https://www.pythonanywhere.com/) is a cloud-based Python development environment that offers an excellent platform for hosting web scrapers. It provides a free tier that includes:\n",
    "\n",
    "- Daily scheduled tasks\n",
    "- Web-based console access\n",
    "- 512MB storage\n",
    "- Basic CPU and memory allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up PythonAnywhere\n",
    "\n",
    "First, create a free account at [pythonanywhere.com](https://www.pythonanywhere.com). Once logged in, follow these steps:\n",
    "\n",
    "Open a Bash console from your PythonAnywhere dashboard and execute these commands to clone the GitHub repository we've been building:\n",
    "```bash\n",
    "\n",
    "$ git clone https://github.com/your-username/your-repo.git\n",
    "$ cd your-repo\n",
    "$ python3 -m venv venv\n",
    "$ source venv/bin/activate\n",
    "$ pip install -r requirements.txt\n",
    "\n",
    "# Recreate your .env file\n",
    "$ touch .env\n",
    "$ echo \"FIRECRAWL_API_KEY='your-api-key-here'\" >> .env\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling the Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PA free tier includes a scheduler with a daily frequency. To enable it, follow these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to the \"Tasks\" tab in your PythonAnywhere dashboard accessible via https://www.pythonanywhere.com/user/your-username/\n",
    "2. Add a new scheduled task.\n",
    "3. Set the timing using the provided interface.\n",
    "4. Enter the command to run your scraper:\n",
    "\n",
    "```bash\n",
    "cd /home/your-username/your-repo && source venv/bin/activate && python scraper.py\n",
    "```\n",
    "\n",
    "The command changes the working directory to the project location, activates the virtual environment and executes the scraper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/pa-scheduler.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage Options\n",
    "\n",
    "PythonAnywhere's filesystem is persistent, unlike Heroku, so you can store JSON files directly. However, for better scalability, consider using cloud storage:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "def save_yesterday_top_products():\n",
    "    \"\"\"\n",
    "    Change back to JSON-based storage.\n",
    "    \"\"\"\n",
    "    products = get_yesterday_top_products()\n",
    "    \n",
    "    # Local storage (works on PythonAnywhere)\n",
    "    date_str = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    filename = f\"data/ph_top_products_{date_str}.json\"\n",
    "    \n",
    "    # Create data directory if it doesn't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(products, f)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platform benefits & limitations\n",
    "\n",
    "PythonAnywhere offers several advantages for web scraping:\n",
    "\n",
    "- **Always-on environment**: Unlike Heroku's free tier, PythonAnywhere doesn't sleep\n",
    "- **Persistent storage**: Files remain stored between runs\n",
    "- **Simple interface**: User-friendly web console and file editor\n",
    "- **Built-in scheduler**: No need for additional add-ons\n",
    "- **Free SSL**: HTTPS requests work out of the box\n",
    "- **Multiple Python Versions**: Support for different Python versions\n",
    "\n",
    "The free tier has some restrictions:\n",
    "\n",
    "- Limited to 1 daily task\n",
    "- CPU/RAM throttling\n",
    "- 512MB storage limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping or Modifying Tasks\n",
    "\n",
    "To manage your scraper:\n",
    "\n",
    "1. **Pause**: Disable the scheduled task in the Tasks tab\n",
    "2. **Modify schedule**: Edit timing in the Tasks interface\n",
    "3. **Delete**: Remove the task completely\n",
    "4. **Update code**: Pull latest changes from git repository from any PythonAnywhere bash console:\n",
    "\n",
    "```bash\n",
    "$ cd your-repo\n",
    "$ git pull origin main\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our scraper and deployment methods are far from perfect. In this section, we will cover some best practices and tips to optimize its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling & Monitoring\n",
    "\n",
    "Proper error handling and monitoring are crucial for maintaining a reliable web scraper. Below, we will implement a few mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Implement a robust retry mechanism:\n",
    "\n",
    "```python\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=10),\n",
    "    reraise=True\n",
    ")\n",
    "def get_yesterday_top_products():\n",
    "    try:\n",
    "        app = FirecrawlApp()\n",
    "        data = app.scrape_url(\n",
    "            BASE_URL,\n",
    "            params={\n",
    "                \"formats\": [\"extract\"],\n",
    "                \"extract\": {\n",
    "                    \"schema\": YesterdayTopProducts.model_json_schema(),\n",
    "                    \"prompt\": \"Extract the top products listed under the 'Yesterday's Top Products' section.\"\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        return data[\"extract\"][\"products\"]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Scraping failed: {str(e)}\")\n",
    "        raise\n",
    "```\n",
    "\n",
    "Above, we are implementing a retry mechanism using the tenacity library. It will retry the scraping operation up to 3 times with exponential backoff between attempts. The wait time starts at 4 seconds and increases exponentially up to 10 seconds between retries. If all retries fail, it will raise the last exception. Any errors are logged before being re-raised to trigger the retry mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement comprehensive logging\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configure logging with both file and console handlers.\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Create formatters\n",
    "    detailed_formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    simple_formatter = logging.Formatter('%(levelname)s: %(message)s')\n",
    "\n",
    "    # File handler\n",
    "    file_handler = logging.FileHandler(\n",
    "        f'logs/scraper_{datetime.now().strftime(\"%Y%m%d\")}.log'\n",
    "    )\n",
    "    file_handler.setFormatter(detailed_formatter)\n",
    "    \n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(simple_formatter)\n",
    "\n",
    "    # Add handlers\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = setup_logging()\n",
    "```\n",
    "\n",
    "The logging setup above configures comprehensive logging for our web scraper, which is essential for monitoring, debugging and maintaining the scraper in production. It creates two logging handlers - one that writes detailed logs to dated files (including timestamps and log levels), and another that outputs simplified logs to the console. This dual logging approach helps us track scraper execution both in real-time via console output and historically through log files. Having proper logging is crucial for diagnosing issues, monitoring performance, and ensuring the reliability of our web scraping system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up monitoring alerts\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "def send_alert(message, webhook_url):\n",
    "    \"\"\"Send alerts to Slack/Discord/etc.\"\"\"\n",
    "    payload = {\"text\": message}\n",
    "    try:\n",
    "        requests.post(webhook_url, json=payload)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to send alert: {str(e)}\")\n",
    "\n",
    "def monitor_scraping_health(products):\n",
    "    \"\"\"Monitor scraping health and send alerts if needed.\"\"\"\n",
    "    if not products:\n",
    "        send_alert(\n",
    "            \"‚ö†Ô∏è Warning: No products scraped from ProductHunt\",\n",
    "            os.getenv(\"WEBHOOK_URL\")\n",
    "        )\n",
    "        return False\n",
    "        \n",
    "    if len(products) < 5:\n",
    "        send_alert(\n",
    "            f\"‚ö†Ô∏è Warning: Only {len(products)} products scraped (expected 5)\",\n",
    "            os.getenv(\"WEBHOOK_URL\")\n",
    "        )\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "```\n",
    "\n",
    "The monitoring functions above help ensure our scraper is working properly. The `send_alert()` function sends notifications to messaging platforms like Slack or Discord when issues occur, requiring a webhook URL configured in environment variables. The `monitor_scraping_health()` function checks if we're getting the expected amount of scraped data and triggers alerts if not. Learn more about setting up webhooks in [Discord](https://discord.com/developers/docs/resources/webhook) and [Slack](https://api.slack.com/messaging/webhooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Management\n",
    "\n",
    "Proper data management is crucial for a production web scraper. This includes validating the scraped data to ensure quality and consistency, as well as implementing efficient storage mechanisms to handle large volumes of data. Let's look at the key components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement data validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class ProductValidator:\n",
    "    @staticmethod\n",
    "    def validate_product(product: dict) -> Optional[str]:\n",
    "        \"\"\"Validate product data and return error message if invalid.\"\"\"\n",
    "        required_fields = ['name', 'description', 'url', 'topics']\n",
    "        \n",
    "        for field in required_fields:\n",
    "            if not product.get(field):\n",
    "                return f\"Missing required field: {field}\"\n",
    "                \n",
    "        if not isinstance(product.get('n_upvotes'), int):\n",
    "            return \"Invalid upvote count\"\n",
    "            \n",
    "        if not product.get('url').startswith('http'):\n",
    "            return \"Invalid URL format\"\n",
    "            \n",
    "        return None\n",
    "\n",
    "def validate_products(products: list) -> list:\n",
    "    \"\"\"Validate and filter products.\"\"\"\n",
    "    valid_products = []\n",
    "    \n",
    "    for product in products:\n",
    "        error = ProductValidator.validate_product(product)\n",
    "        if error:\n",
    "            logger.warning(f\"Invalid product data: {error}\")\n",
    "            continue\n",
    "        valid_products.append(product)\n",
    "    \n",
    "    return valid_products\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class like `ProductValidator` is important for ensuring data quality and consistency in our web scraping pipeline. It validates product data against required fields and format specifications before storage. This validation step helps prevent corrupted or incomplete data from entering our system, making downstream processing more reliable. The class provides static methods to validate individual products and entire product lists, checking for required fields like name and description, proper URL formatting, and valid upvote counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement efficient storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "import json\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self, base_dir: str = \"data\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    def save_products(self, products: list, compress: bool = True):\n",
    "        \"\"\"Save products with optional compression.\"\"\"\n",
    "        date_str = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "        \n",
    "        if compress:\n",
    "            filename = self.base_dir / f\"ph_products_{date_str}.json.gz\"\n",
    "            with gzip.open(filename, 'wt', encoding='utf-8') as f:\n",
    "                json.dump(products, f)\n",
    "        else:\n",
    "            filename = self.base_dir / f\"ph_products_{date_str}.json\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(products, f)\n",
    "\n",
    "    def load_products(self, date_str: str) -> list:\n",
    "        \"\"\"Load products for a specific date.\"\"\"\n",
    "        gz_file = self.base_dir / f\"ph_products_{date_str}.json.gz\"\n",
    "        json_file = self.base_dir / f\"ph_products_{date_str}.json\"\n",
    "        \n",
    "        if gz_file.exists():\n",
    "            with gzip.open(gz_file, 'rt', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        elif json_file.exists():\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataManager` class extends our plain storage function from the previous sections. It provides a robust and organized way to handle data persistence for our web scraper. The class implements both compressed and uncompressed storage options using `gzip`, which helps optimize disk space usage while maintaining data accessibility. By organizing data by date and providing consistent file naming conventions, it enables easy tracking and retrieval of historical product data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's a wrap! We've covered several ways to deploy web scrapers in 2025, from simple GitHub Actions to more complex setups with Heroku and PythonAnywhere. Each method has its own sweet spot:\n",
    "\n",
    "- GitHub Actions: Great for simple scrapers that run daily/weekly\n",
    "- Heroku: Perfect for more frequent scraping with its flexible scheduler\n",
    "- PythonAnywhere: Solid choice for beginners with its user-friendly interface\n",
    "\n",
    "Remember, start small and scale up as needed. No need to jump straight into complex setups if GitHub Actions does the job. The best deployment method is the one that matches your specific needs and technical comfort level. üï∑Ô∏è\n",
    "\n",
    "Here are some related resources that might interest you:\n",
    "- [GitHub Actions documentation](https://docs.github.com/en/actions)\n",
    "- [Firecrawl documentation](docs.firecrawl.dev)\n",
    "- [Comprehensive guide on Firecrawl's `scrape_url` method](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint)\n",
    "- [How to generate sitemaps in Python using Firecrawl](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint)\n",
    "\n",
    "Thank you for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

================
File: examples/blog-articles/deploying_web_scrapers/notebook.md
================
---
title: "How to Deploy Python Web Scrapers Online in 2025 For Free"
description: Learn how to deploy Python web scrapers online for free in 2025 using GitHub Actions, Heroku, PythonAnywhere and more. Step-by-step guide with code examples for automating web scraping tasks in the cloud with best practices for monitoring, security and optimization.
slug: deploy-web-scrapers-for-free-2025
date: Dec 1, 2024
author: bex_tuychiev
image: /images/blog/deploying-web-scrapers/deploy-web-scrapers-for-free-2025.jpg
categories: [tutorials]
keywords: [web scraping deployment, python automation, cloud deployment, github actions, heroku deployment, web scraping best practices, github actions tutorial, heroku tutorial]
---

## Introduction

Web scraping projects may start on your machine, but unless you're willing to ship your laptop to random strangers on the Internet, you'll need cloud services.

![Funny meme illustrating why cloud deployment is better than local workflows](deploying-web-scrapers-images/meme.png)

There are many compelling reasons to move web scrapers to the cloud and make them more reliable. This guide explores several methods for automating and deploying web scrapers in 2025, focusing on free solutions.

Here is a general outline of concepts we will cover:

- Setting up automated scraping with GitHub Actions
- Deploying to PaaS platforms (Heroku, PythonAnywhere)
- Best practices for monitoring, security, and optimization

## Why Move Web Scrapers to the Cloud?

The number one reason to deploy scrapers to the cloud is reliability. Cloud-based scrapers run 24/7, without cigarette breaks and the best part, without depending on your local machine.

Cloud-based scrapers also handle large-scale data operations more easily and gracefully, often juggling multiple scraping tasks. And, if you are a bit more aggressive in your request frequencies and get a dreaded IP ban, cloud services can give access to other IP addresses and geographic locations.

Moreover, you are not limited by your laptop's specs because cloud gives you dedicated resources. While these resources may gouge a hole in your pocket for large-scale scraping operations, many platforms offer generous free tiers, as we will explore soon.

## How to Choose the Right Deployment Method For Your Scrapers

We are about to list three deployment methods in this article, so you might get a decision fatigue. To prevent that, give this section a cursory read as it helps you choose the right one based on your scale requirements, technical complexity, and budget.

### Scale requirements

In this section, we'll dive into three deployment tiers that match your scale - from lightweight solutions to heavy-duty scraping powerhouses.

#### Small scale (1-1000 requests/day)

- **Best Options**:
  - GitHub Actions
  - PythonAnywhere
  - Heroku (Free Tier)
- **Why**: These platforms offer sufficient resources for basic scraping needs without cost
- **Limitations**: Daily request caps and runtime restrictions

#### Medium Scale (1000-10000 requests/day)

- **Best Options**:
  - AWS Lambda
  - Google Cloud Functions
  - Docker containers on basic VPS
- **Why**: Better handling of concurrent requests and flexible scaling
- **Considerations**: Cost begins to factor in, but still manageable

#### Large Scale (10000+ requests/day)

- **Best Options**:
  - Kubernetes clusters
  - Multi-region serverless deployments
  - Specialized scraping platforms
- **Why**: Robust infrastructure for high-volume operations
- **Trade-offs**: Higher complexity and cost vs. reliability

### Technical complexity

Now, let's categorize the methods based on how fast you can get them up and running.

#### Low Complexity Solutions

- **GitHub Actions**
  - Pros: Simple setup, version control integration
  - Cons: Limited customization
- **PythonAnywhere**
  - Pros: User-friendly interface
  - Cons: Resource constraints

#### Medium Complexity Solutions

- **Serverless (AWS Lambda/Google Functions)**
  - Pros: Managed infrastructure, auto-scaling
  - Cons: Learning curve for configuration
- **Docker Containers**
  - Pros: Consistent environments
  - Cons: Container management overhead

#### High Complexity Solutions

- **Kubernetes**
  - Pros: Ultimate flexibility and scalability
  - Cons: Significant operational overhead
- **Custom Infrastructure**
  - Pros: Complete control
  - Cons: Requires DevOps expertise

### Budget Considerations

In terms of cost, all methods in the article has the following generous free tiers or next-to-nothing cheap starting plans:

- GitHub Actions: 2000 minutes/month
- Heroku: $5 and up
- AWS Lambda: starting at 0.2$ per 1 million requests
- Google Cloud Functions: 2 million invocations/month

These limits are based on various cost factors like compute time, data transfer speeds, storage requirements and additional services like databases or monitoring.

Here is a little decision matrix to distill all this information:

| Factor          | Small Project | Medium Project | Large Project |
|-----------------|---------------|----------------|---------------|
| Best Platform   | GitHub Actions| AWS Lambda     | Kubernetes   |
| Monthly Cost    | $0           | $10-50         | $100+        |
| Setup Time      | 1-2 hours    | 1-2 days       | 1-2 weeks    |
| Maintenance     | Minimal      | Moderate       | Significant  |
| Scalability     | Limited      | Good           | Excellent    |

-------------

Start small with simpler platforms and gather data on your scraping needs by measuring actual usage patterns. From there, you can gradually scale and potentially implement hybrid approaches that balance complexity and benefits.

## Prerequisites

This article assumes familiarity with web scraping fundamentals like HTML parsing, CSS selectors, HTTP requests, and handling dynamic content. You should also be comfortable with Python basics including functions, loops, and working with external libraries. Basic knowledge of command line tools and git version control will be essential for deployment.

### Required accounts

Before starting with any deployment method, you'll need to create accounts on these platforms:

1. **[GitHub account](github.com)** (Required)
   - Needed for version control and GitHub Actions
2. **Cloud Platform Account** (Choose at least one)
   - [Heroku account](https://signup.heroku.com/)
   - [PythonAnywhere account](https://www.pythonanywhere.com/)

3. **[Firecrawl account](https://firecrawl.dev)** (Optional)
   - Only needed if you decide to use an AI-based scraper (more on Firecrawl soon).  

Note: Most cloud platforms require a credit card for verification, even when using free tiers. However, they won't charge you unless you exceed free tier limits.

### Building a basic scraper

To demonstrate deployment concepts effectively, we'll start by building a basic web scraper using Firecrawl, a modern scraping API that simplifies many common challenges.

[Firecrawl](https://docs.firecrawl.dev) offers several key advantages compared to traditional Python web scraping libraries:

- Dead simple to use with only a few dependencies
- Handles complex scraping challenges automatically (proxies, anti-bot mechanisms, dynamic JS content)
- Converts web content into clean, LLM-ready markdown format
- Supports multiple output formats (markdown, structured data, screenshots, HTML)
- Reliable extraction with retry mechanisms and error handling
- Supports custom actions (click, scroll, input, wait) before data extraction
- Geographic location customization for avoiding IP bans
- Built-in rate limiting and request management

As an example, we will build a simple scraper for [ProductHunt](https://www.producthunt.com/). Specifically, we will scrape the "Yesterday's Top Products" list from the homepage:

![Screenshot of Product Hunt homepage showing Yesterday's Top Products section with product cards displaying titles, descriptions, upvotes and comments](deploying-web-scrapers-images/ph-homepage.png)

The scraper extracts the following information from each product:

![Sample Product Hunt product card showing key data fields including product name, description, upvotes, comments, topics and logo that our Firecrawl scraper will extract](deploying-web-scrapers-images/ph-sample.png)

Let's get building:

```bash
mkdir product-hunt-scraper
cd product-hunt-scraper
touch scraper.py .env
python -m venv venv
source venv/bin/activate
pip install pydantic firecrawl-py
echo "FIRECRAWL_API_KEY='your-api-key-here' >> .env"
```

These commands set up a working directory for the scraper, along with a virtual environment and the main script. The last part also saves your Firecrawl API key, which you can get through their free plan by [signing up for an account](firecrawl.dev).

Let's work on the code now:

```python
import json

from firecrawl import FirecrawlApp
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from datetime import datetime

load_dotenv()
```

First, we import a few packages and the `FirecrawlApp` class - we use it to establish a connection with Firecrawl's scraping engine. Then, we define a Pydantic class outlining the details we want to scrape from each product:

```python
class Product(BaseModel):
    name: str = Field(description="The name of the product")
    description: str = Field(description="A short description of the product")
    url: str = Field(description="The URL of the product")
    
    topics: list[str] = Field(
        description="A list of topics the product belongs to. Can be found below the product description."
    )
    
    n_upvotes: int = Field(description="The number of upvotes the product has")
    n_comments: int = Field(description="The number of comments the product has")
    
    rank: int = Field(
        description="The rank of the product on Product Hunt's Yesterday's Top Products section."
    )
    logo_url: str = Field(description="The URL of the product's logo.")
```

The field descriptions in this class play a crucial role in guiding the LLM scraping engine. By providing natural language descriptions for each field, we tell the LLM exactly what information to look for and where to find it on the page. For example, when we say "A list of topics under the product description", the LLM understands both the content we want (topics) and its location (below the description).

This natural language approach allows Firecrawl to intelligently parse the page's HTML structure and extract the right information without requiring explicit CSS selectors or XPaths. The LLM analyzes the semantic meaning of our descriptions and matches them to the appropriate elements on the page.

This approach offers two practical advantages: it reduces initial development time since you don't need to manually inspect HTML structures, and it provides long-lasting resilience against HTML changes. Since the LLM understands the semantic meaning of the elements rather than relying on specific selectors, it can often continue working even when class names or IDs are updated. This makes it suitable for scenarios where long-term maintenance is a consideration.

Getting back to code, we write another Pydantic class for scraping a collection of Products from the 'Yesterday's Top Products' list:

```python
class YesterdayTopProducts(BaseModel):
    products: list[Product] = Field(
        description="A list of top products from yesterday on Product Hunt."
    )
```

The `YesterdayTopProducts` parent class is essential - without it, Firecrawl would only scrape a single product instead of the full list. This happens because Firecrawl strictly adheres to the provided schema structure, ensuring consistent and reliable output on every scraping run.

Now, we define a function that scrapes ProductHunt based on the schema we just defined:

```python
BASE_URL = "https://www.producthunt.com"


def get_yesterday_top_products():
    app = FirecrawlApp()

    data = app.scrape_url(
        BASE_URL,
        params={
            "formats": ["extract"],
            "extract": {
                "schema": YesterdayTopProducts.model_json_schema(),
                
                "prompt": "Extract the top products listed under the 'Yesterday's Top Products' section. There will be exactly 5 products.",
            },
        },
    )

    return data["extract"]["products"]
```

This function initializes a Firecrawl app, which reads your Firecrawl API key stored in an `.env` file and scrapes the URL. Notice the parameters being passed to the `scrape_url()` method:

- `formats` specifies how the data should be scraped and extracted. Firecrawl supports other formats like markdown, HTML, screenshots or links.
- `schema`: The JSON schema produced by the Pydantic class
- `prompt`: A general prompt guiding the underlying LLM on what to do. Providing a prompt usually improves the performance.

In the end, the function returns the extracted products, which will be a list of dictionaries.

The final step is writing a function to save this data to a JSON file:

```python
def save_yesterday_top_products():
    products = get_yesterday_top_products()
    
    date_str = datetime.now().strftime("%Y_%m_%d")
    filename = f"ph_top_products_{date_str}.json"
    
    with open(filename, "w") as f:
        json.dump(products, f)
        
if __name__ == "__main__":
    save_yesterday_top_products()
```

This function runs the previous one and saves the returned data to a JSON file identifiable with the following day's date.

-------------

We've just built a scraper that resiliently extracts data from ProductHunt. To keep things simple, we implemented everything in a single script with straightforward data persistence. In production environments with larger data flows and more complex websites, your project would likely span multiple directories and files.

Nevertheless, the deployment methods we'll explore work for nearly any type of project. You can always restructure your own projects to follow this pattern of having a single entry point that coordinates all the intermediate scraping stages.

## Deploying Web Scrapers With GitHub Actions

GitHub Actions is a powerful CI/CD platform built into GitHub that allows you to automate workflows, including running scrapers on a schedule. It provides a simple way to deploy and run automated tasks without managing infrastructure, making it an ideal choice for web scraping projects.

To get started, initialize Git and commit the work we've completed so far in your working directory:

```python
git init
touch .gitignore
echo ".env" >> .gitignore  # Remove the .env file from Git indexing
git add .
git commit -m "Initial commit"
```

Then, create an empty GitHub repository, copy its link and set it as the remote for your local repo:

```bash
git remote add origin your-repo-link
git push
```

### Creating workflow files

Workflow files are YAML configuration files that tell GitHub Actions how to automate tasks in your repository. For our web scraping project, these files will define when and how to run our scraper.

These files live in the `.github/workflows` directory of your repository and contain instructions for:

- When to trigger the workflow (like on a schedule or when code is pushed)
- What environment to use (Python version, dependencies to install)
- The actual commands to run your scraper
- What to do with the scraped data

Each workflow file acts like a recipe that GitHub Actions follows to execute your scraper automatically. This automation is perfect for web scraping since we often want to collect data on a regular schedule without manual intervention.

For our scraper, we need a single workflow file that executes the `scraper.py` file. Let's set it up:

```bash
mkdir -p .github/workflows
touch .github/workflows/ph-scraper.yml
```

Open the newly-created file and paste the following contents:

```yml
name: Product Hunt Scraper

on:
  schedule:
    - cron: '0 1 * * *'  # Runs at 1 AM UTC daily
  workflow_dispatch:  # Allows manual trigger

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      with:
          persist-credentials: true
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run scraper
      env:
        FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}
      run: python scraper.py
        
    - name: Commit and push if changes
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add *.json
        git diff --quiet && git diff --staged --quiet || git commit -m "Update ProductHunt data [skip ci]"
        git push
```

The YAML file defines a GitHub Actions workflow for automated web scraping:

name: Specifies the workflow name that appears in GitHub Actions UI

on: Defines how workflow triggers:

- schedule: Uses cron syntax to run daily at 1 AM UTC
- workflow_dispatch: Enables manual workflow triggering

jobs: Contains the workflow jobs:

- scrape: Main job that runs on ubuntu-latest
  - steps: Sequential actions to execute:
    1. Checkout repository using `actions/checkout`
    2. Setup Python 3.10 environment
    3. Install project dependencies from `requirements.txt`
    4. Run the scraper with environment variables
    5. Commit and push any changes to the repository

The workflow automatically handles repository interaction, dependency management, and data updates while providing both scheduled and manual execution options. You can read the [official GitHub guide on workflow file syntax](https://docs.github.com/en/actions/writing-workflows/workflow-syntax-for-github-actions#about-yaml-syntax-for-workflows) to learn more.

For this workflow file to sun successfully, we need to take a couple of additional steps.

### Generating a `requirements.txt` file

One of the steps in the workflow file is installing the dependencies for our project using a `requirements.txt` file, which is a standard format for listing packages used in your project.

For simple projects, you can create this file manually and adding each package on a new line like:

```text
pydantic
firecrawl-py
```

However, if you have a large project with multiple files and dozens of dependencies, you need an automated method. The simplest one I can suggest is using `pipreqs` package:

```bash
pip install pipreqs
pipreqs . 
```

`pipreqs` is a lightweight package that scans all Python scripts in your project and adds them to a new `requirements.txt` file with their used versions.

### Storing secrets

If you notice, the workflow file has a step that executes `scraper.py`:

```yml
- name: Run scraper
      env:
        FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}
      run: python scraper.py
```

The workflow retrieves environment variables using the `secrets.SECRET_NAME` syntax. Since the `.env` file containing your Firecrawl API key isn't uploaded to GitHub for security reasons, you'll need to store the key in your GitHub repository secrets.

To add your API key as a secret:

1. Navigate to your GitHub repository
2. Click on "Settings"
3. Select "Secrets and variables" then "Actions"
4. Click "New repository secret"
5. Enter "FIRECRAWL_API_KEY" as the name
6. Paste your API key as the value
7. Click "Add secret"

This allows the workflow to securely access your API key during execution without exposing it in the repository.

### Running the workflow

Before running the workflow, we need to commit all new changes and push them to GitHub:

```python
git add .
git commit -m "Descriptive commit message"
```

This makes our workflow visible to GitHub.

At the top of the workflow file, we set a schedule for the workflow file to run at 1 AM using `cron` syntax:

```yaml
on:
  schedule:
    - cron: '0 1 * * *'  # Runs at 1 AM UTC daily
  workflow_dispatch:  # Allows manual trigger
```

The `cron` syntax consists of 5 fields representing minute (0-59), hour (0-23), day of month (1-31), month (1-12), and day of week (0-6, where 0 is Sunday). Each field can contain specific values, ranges (1-5), lists (1,3,5), or asterisks (*) meaning "every". For example, `0 1 * * *` means "at minute 0 of hour 1 (1 AM UTC) on every day of every month". Here are some more patterns:

- `0 */2 * * *`: Every 2 hours
- `0 9-17 * * 1-5`: Every hour from 9 AM to 5 PM on weekdays
- `*/15 * * * *`: Every 15 minutes
- `0 0 * * 0`: Every Sunday at midnight
- `0 0 1 * *`: First day of every month at midnight
- `30 18 * * 1,3,5`: Monday, Wednesday, Friday at 6:30 PM

So, once the workflow file is pushed to GitHub, the scraper is scheduler to run. However, the `workflow_dispatch` parameter in the file allows us to run the scraper manually for debugging.

![Screenshot of GitHub Actions workflow interface showing run workflow button and workflow status for web scraper automation](deploying-web-scrapers-images/workflow.png)

Navigate to the Actions tab of your GitHub repository, click on the workflow name and press "Run workflow". In about a minute (if the workflow is successful), you will see the top five products from yesterday on ProductHunt saved as a JSON file to your repository.

Whenever you want to interrupt the scraping schedule, click on the three buttons in the top-right corner of the workflow page and disable it.

## Deploying Web Scrapers With Heroku

[Heroku](heroku.com) is a Platform-as-a-Service (PaaS) that makes deploying applications straightforward, even for beginners. While it removed its generous free tier in 2022, its basic $5 "dyno" plan still has some free features we can take advantage of for the purposes of this tutorial.

### Setting up Heroku

First, install the Heroku CLI and login to your account:

```bash
brew install heroku/brew/heroku  # macOS
curl https://cli-assets.heroku.com/install.sh | sh  # Linux
heroku login  # Opens your web browser
```

Then, create a new Heroku app and set it as a remote for your repository:

```bash
heroku create ph-scraper-your-name  # Make the app name unique
heroku git:remote -a ph-scraper-your-name
```

After this step, if you visit [dashboard.heroku.com](dashboard.heroku.com), your app must be visible.

### Configuring the Application

Heroku requires a few additional files to run your application. First, create a `Procfile` that tells Heroku what command to run:

```bash
touch Procfile
echo "worker: python scraper.py" > Procfile
```

Next, create a `runtime.txt` to specify the Python version:

```bash
touch runtime.txt
echo "python-3.10.12" > runtime.txt
```

### Environment Variables

Instead of using a `.env` file, Heroku requires you to set your environment variables directly using the Heroku CLI:

```bash
heroku config:set FIRECRAWL_API_KEY='your-api-key-here'
```

You can verify the variables are set correctly with:

```bash
heroku config
```

### Scheduling Scraper Runs

Heroku uses an add-on called ["Scheduler"](https://elements.heroku.com/addons/scheduler) for running periodic tasks. Install it with:

```bash
heroku addons:create scheduler:standard
```

Then open the scheduler dashboard:

```bash
heroku addons:open scheduler
```

In the web interface, add a new job with the command `python scraper.py` and set your desired frequency (daily, hourly, or every 10 minutes).

![Screenshot of Heroku Scheduler dashboard showing job configuration interface with frequency dropdown and command input field for scheduling automated web scraper tasks](deploying-web-scrapers-images/heroku-scheduler.png)

### Deployment and Monitoring

Now, to launch everything, you need to deploy your application by committing and pushing the local changes to Heroku:

```bash
git add .
git commit -m "Add Heroku-related files"
git push heroku main
```

You can periodically monitor the health of your application with the following command:

```bash
heroku logs --tail
```

### Platform Limitations

The basic $5 dyno has some important limitations to consider:

- Sleeps after 30 minutes of inactivity
- Limited to 512MB RAM
- Shares CPU with other applications
- Maximum of 23 hours active time per day

For most small to medium scraping projects, these limitations aren't problematic. However, if you need more resources, you can upgrade to Standard ($25/month) or Performance ($250/month) dynos.

### Data Persistence

Since Heroku's filesystem is temporary, you'll need to modify the scraper to store data externally. Here's a quick example using AWS S3:

```python
import boto3  # pip install boto3
from datetime import datetime

def save_yesterday_top_products():
    products = get_yesterday_top_products()
    
    # Initialize S3 client
    s3 = boto3.client('s3')
    
    # Create filename with date
    date_str = datetime.now().strftime("%Y_%m_%d")
    filename = f"ph_top_products_{date_str}.json"
    
    # Upload to S3
    s3.put_object(
        Bucket='your-bucket-name',
        Key=filename,
        Body=json.dumps(products)
    )
```

For this to work, you must already have an AWS account and an existing S3 bucket. Also, you must set your AWS credentials as Heroku secrets through the Heroku CLI:

```bash
heroku config:set AWS_ACCESS_KEY_ID='your-key'
heroku config:set AWS_SECRET_ACCESS_KEY='your-secret'
```

Once you do, add `boto3` to the list of dependencies in your `requirements.txt` file:

```bash
echo "boto3" >> requirements.txt
```

Finally, commit and push the changes:

```bash
git add .
git commit -m "Switch data persistence to S3"
git push heroku main
git push origin main
```

You can confirm that the app is functioning properly by setting the schedule frequency to 10 minutes and checking your S3 bucket for the JSON file containing the top five products from ProductHunt.

### Stopping Heroku Apps

To stop your app, you can use a few different methods:

- Pause the dyno:

```bash
heroku ps:scale worker=0
```

This stops the worker dyno without deleting the app. To resume later:

```bash
heroku ps:scale worker=1
```

- Disable the scheduler:

```bash
heroku addons:destroy scheduler
```

Or visit the Heroku dashboard and remove the scheduler add-on manually.

- Delete the entire app:

```bash
heroku apps:destroy --app your-app-name --confirm your-app-name
```

‚ö†Ô∏è Warning: This permanently deletes your app and all its data.

- Maintenance mode

```bash
heroku maintenance:on
```

This puts the app in maintenance mode. To disable:

```bash
heroku maintenance:off
```

-------------

To learn more about Heroku and how to run Python applications on its servers, please refer to [their Python support documentation](https://devcenter.heroku.com/categories/python-support).

## Deploying Web Scrapers With PythonAnywhere

[PythonAnywhere](https://www.pythonanywhere.com/) is a cloud-based Python development environment that offers an excellent platform for hosting web scrapers. It provides a free tier that includes:

- Daily scheduled tasks
- Web-based console access
- 512MB storage
- Basic CPU and memory allocation

### Setting Up PythonAnywhere

First, create a free account at [pythonanywhere.com](https://www.pythonanywhere.com). Once logged in, follow these steps:

Open a Bash console from your PythonAnywhere dashboard and execute these commands to clone the GitHub repository we've been building:

```bash

$ git clone https://github.com/your-username/your-repo.git
$ cd your-repo
$ python3 -m venv venv
$ source venv/bin/activate
$ pip install -r requirements.txt

# Recreate your .env file
$ touch .env
$ echo "FIRECRAWL_API_KEY='your-api-key-here'" >> .env
```

### Scheduling the Scraper

PA free tier includes a scheduler with a daily frequency. To enable it, follow these steps:

1. Go to the "Tasks" tab in your PythonAnywhere dashboard accessible via <https://www.pythonanywhere.com/user/your-username/>
2. Add a new scheduled task.
3. Set the timing using the provided interface.
4. Enter the command to run your scraper:

```bash
cd /home/your-username/your-repo && source venv/bin/activate && python scraper.py
```

The command changes the working directory to the project location, activates the virtual environment and executes the scraper.

![Screenshot of PythonAnywhere task scheduler interface showing scheduling options and command input field for automated web scraping tasks](deploying-web-scrapers-images/pa-scheduler.png)

### Data Storage Options

PythonAnywhere's filesystem is persistent, unlike Heroku, so you can store JSON files directly. However, for better scalability, consider using cloud storage:

```python
def save_yesterday_top_products():
    """
    Change back to JSON-based storage.
    """
    products = get_yesterday_top_products()
    
    # Local storage (works on PythonAnywhere)
    date_str = datetime.now().strftime("%Y_%m_%d")
    filename = f"data/ph_top_products_{date_str}.json"
    
    # Create data directory if it doesn't exist
    os.makedirs("data", exist_ok=True)
    
    with open(filename, "w") as f:
        json.dump(products, f)
```

### Platform benefits & limitations

PythonAnywhere offers several advantages for web scraping:

- **Always-on environment**: Unlike Heroku's free tier, PythonAnywhere doesn't sleep
- **Persistent storage**: Files remain stored between runs
- **Simple interface**: User-friendly web console and file editor
- **Built-in scheduler**: No need for additional add-ons
- **Free SSL**: HTTPS requests work out of the box
- **Multiple Python Versions**: Support for different Python versions

The free tier has some restrictions:

- Limited to 1 daily task
- CPU/RAM throttling
- 512MB storage limit

### Stopping or Modifying Tasks

To manage your scraper:

1. **Pause**: Disable the scheduled task in the Tasks tab
2. **Modify schedule**: Edit timing in the Tasks interface
3. **Delete**: Remove the task completely
4. **Update code**: Pull latest changes from git repository from any PythonAnywhere bash console:

```bash
cd your-repo
git pull origin main
```

## Best Practices and Optimization

Our scraper and deployment methods are far from perfect. In this section, we will cover some best practices and tips to optimize its performance.

### Error Handling & Monitoring

Proper error handling and monitoring are crucial for maintaining a reliable web scraper. Below, we will implement a few mechanisms.

#### Implement a robust retry mechanism

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    reraise=True
)
def get_yesterday_top_products():
    try:
        app = FirecrawlApp()
        data = app.scrape_url(
            BASE_URL,
            params={
                "formats": ["extract"],
                "extract": {
                    "schema": YesterdayTopProducts.model_json_schema(),
                    "prompt": "Extract the top products listed under the 'Yesterday's Top Products' section."
                },
            },
        )
        return data["extract"]["products"]
    except Exception as e:
        logger.error(f"Scraping failed: {str(e)}")
        raise
```

Above, we are implementing a retry mechanism using the tenacity library. It will retry the scraping operation up to 3 times with exponential backoff between attempts. The wait time starts at 4 seconds and increases exponentially up to 10 seconds between retries. If all retries fail, it will raise the last exception. Any errors are logged before being re-raised to trigger the retry mechanism.

#### Implement comprehensive logging

```python
import logging
from datetime import datetime

def setup_logging():
    """Configure logging with both file and console handlers."""
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)

    # Create formatters
    detailed_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    simple_formatter = logging.Formatter('%(levelname)s: %(message)s')

    # File handler
    file_handler = logging.FileHandler(
        f'logs/scraper_{datetime.now().strftime("%Y%m%d")}.log'
    )
    file_handler.setFormatter(detailed_formatter)
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(simple_formatter)

    # Add handlers
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger

logger = setup_logging()
```

The logging setup above configures comprehensive logging for our web scraper, which is essential for monitoring, debugging and maintaining the scraper in production. It creates two logging handlers - one that writes detailed logs to dated files (including timestamps and log levels), and another that outputs simplified logs to the console. This dual logging approach helps us track scraper execution both in real-time via console output and historically through log files. Having proper logging is crucial for diagnosing issues, monitoring performance, and ensuring the reliability of our web scraping system.

#### Set up monitoring alerts

```python
import requests

def send_alert(message, webhook_url):
    """Send alerts to Slack/Discord/etc."""
    payload = {"text": message}
    try:
        requests.post(webhook_url, json=payload)
    except Exception as e:
        logger.error(f"Failed to send alert: {str(e)}")

def monitor_scraping_health(products):
    """Monitor scraping health and send alerts if needed."""
    if not products:
        send_alert(
            "‚ö†Ô∏è Warning: No products scraped from ProductHunt",
            os.getenv("WEBHOOK_URL")
        )
        return False
        
    if len(products) < 5:
        send_alert(
            f"‚ö†Ô∏è Warning: Only {len(products)} products scraped (expected 5)",
            os.getenv("WEBHOOK_URL")
        )
        return False
        
    return True
```

The monitoring functions above help ensure our scraper is working properly. The `send_alert()` function sends notifications to messaging platforms like Slack or Discord when issues occur, requiring a webhook URL configured in environment variables. The `monitor_scraping_health()` function checks if we're getting the expected amount of scraped data and triggers alerts if not. Learn more about setting up webhooks in [Discord](https://discord.com/developers/docs/resources/webhook) and [Slack](https://api.slack.com/messaging/webhooks).

### Data Management

Proper data management is crucial for a production web scraper. This includes validating the scraped data to ensure quality and consistency, as well as implementing efficient storage mechanisms to handle large volumes of data. Let's look at the key components.

#### Implement data validation

```python
from typing import Optional
from datetime import datetime

class ProductValidator:
    @staticmethod
    def validate_product(product: dict) -> Optional[str]:
        """Validate product data and return error message if invalid."""
        required_fields = ['name', 'description', 'url', 'topics']
        
        for field in required_fields:
            if not product.get(field):
                return f"Missing required field: {field}"
                
        if not isinstance(product.get('n_upvotes'), int):
            return "Invalid upvote count"
            
        if not product.get('url').startswith('http'):
            return "Invalid URL format"
            
        return None

def validate_products(products: list) -> list:
    """Validate and filter products."""
    valid_products = []
    
    for product in products:
        error = ProductValidator.validate_product(product)
        if error:
            logger.warning(f"Invalid product data: {error}")
            continue
        valid_products.append(product)
    
    return valid_products
```

A class like `ProductValidator` is important for ensuring data quality and consistency in our web scraping pipeline. It validates product data against required fields and format specifications before storage. This validation step helps prevent corrupted or incomplete data from entering our system, making downstream processing more reliable. The class provides static methods to validate individual products and entire product lists, checking for required fields like name and description, proper URL formatting, and valid upvote counts.

#### Implement efficient storage

```python
import json
import gzip
from pathlib import Path

class DataManager:
    def __init__(self, base_dir: str = "data"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)

    def save_products(self, products: list, compress: bool = True):
        """Save products with optional compression."""
        date_str = datetime.now().strftime("%Y_%m_%d")
        
        if compress:
            filename = self.base_dir / f"ph_products_{date_str}.json.gz"
            with gzip.open(filename, 'wt', encoding='utf-8') as f:
                json.dump(products, f)
        else:
            filename = self.base_dir / f"ph_products_{date_str}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(products, f)

    def load_products(self, date_str: str) -> list:
        """Load products for a specific date."""
        gz_file = self.base_dir / f"ph_products_{date_str}.json.gz"
        json_file = self.base_dir / f"ph_products_{date_str}.json"
        
        if gz_file.exists():
            with gzip.open(gz_file, 'rt', encoding='utf-8') as f:
                return json.load(f)
        elif json_file.exists():
            with open(json_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []
```

The `DataManager` class extends our plain storage function from the previous sections. It provides a robust and organized way to handle data persistence for our web scraper. The class implements both compressed and uncompressed storage options using `gzip`, which helps optimize disk space usage while maintaining data accessibility. By organizing data by date and providing consistent file naming conventions, it enables easy tracking and retrieval of historical product data.

## Conclusion

And that's a wrap! We've covered several ways to deploy web scrapers in 2025, from simple GitHub Actions to more complex setups with Heroku and PythonAnywhere. Each method has its own sweet spot:

- GitHub Actions: Great for simple scrapers that run daily/weekly
- Heroku: Perfect for more frequent scraping with its flexible scheduler
- PythonAnywhere: Solid choice for beginners with its user-friendly interface

Remember, start small and scale up as needed. No need to jump straight into complex setups if GitHub Actions does the job. The best deployment method is the one that matches your specific needs and technical comfort level. üï∑Ô∏è

Here are some related resources that might interest you:

- [GitHub Actions documentation](https://docs.github.com/en/actions)
- [Firecrawl documentation](docs.firecrawl.dev)
- [Comprehensive guide on Firecrawl's `scrape_url` method](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint)
- [How to generate sitemaps in Python using Firecrawl](https://www.firecrawl.dev/blog/how-to-generate-sitemaps-using-firecrawl-map-endpoint)

Thank you for reading!

================
File: examples/blog-articles/github-actions-tutorial/notebook.ipynb
================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive GitHub Actions Tutorial For Beginners With Examples in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Actions is a powerful automation platform that helps developers automate tedious, time-wasting software development workflows. Instead of running tests, executing scripts at intervals, or doing any programmable task manually, you can let GitHub Actions take the wheel when certain events happen in your repository. In this tutorial, you will learn how to use this critical feature of GitHub and design your own workflows for several real-world use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are GitHub Actions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, [GitHub Actions](https://docs.github.com/en/actions) is a continuous integration and continuous delivery (CI/CD) platform that lets you automate various tasks directly from your GitHub repository. Think of it as your personal robot assistant that can:\n",
    "\n",
    "- Run your Python tests automatically when you push code\n",
    "- Deploy your application when you create a new release\n",
    "- Send notifications when issues are created\n",
    "- Schedule tasks to run at specific times\n",
    "- And much more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why automate with GitHub Actions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a common scenario: You are building a Python application that scrapes product prices from various e-commerce websites. Without GitHub actions, you would need to:\n",
    "\n",
    "1. Manually run your tests after each code change\n",
    "2. Remember to execute the scraper at regular intervals\n",
    "3. Deploy updates to your production environment\n",
    "4. Keep track of environment variables and secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With GitHub actions, all of these tasks can be automated through workflows, usually written in YAML files like below:\n",
    "\n",
    "```yaml\n",
    "name: Run Price Scraper\n",
    "\n",
    "on:\n",
    "  schedule:\n",
    "    - cron: '0 */12 * * *'  # Runs every 12 hours\n",
    "  workflow_dispatch:  # Allows manual triggers\n",
    "\n",
    "jobs:\n",
    "  scrape:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "    - uses: actions/checkout@v3\n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v4\n",
    "      with:\n",
    "        python-version: '3.9'\n",
    "        \n",
    "    - name: Run scraper\n",
    "      env:\n",
    "        API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\n",
    "      run: python scraper.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workflow automatically runs a scraper every 12 hours, handles Python version setup, and securely manages API keys - all without manual intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we'll build in this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this tutorial, we'll build several practical GitHub Actions workflows for Python applications. You will learn how to:\n",
    "\n",
    "1. Create basic and advanced workflow configurations.\n",
    "2. Work with environment variables and secrets.\n",
    "3. Set up automated testing pipelines.\n",
    "4. Build a real-world example: an automated scraping system app [Firecrawl](https://firecrawl.dev) in Python.\n",
    "5. Implement best practices for security and efficiency. \n",
    "\n",
    "By the end, you will have hands-on experience with GitHub Actions and be able to automate your own Python projects effectively. \n",
    "\n",
    "> Note: Even though code examples are Python, the concepts and hands-on experience you will gain from the tutorial will apply to any programming language. \n",
    "\n",
    "Let's start by understanding the core concepts that make GitHub Actions work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding GitHub Actions Core Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write your own GitHub Actions workflows, you need to understand how its different components work together. Let's break down these core concepts using a practical example: automating tests for a simple Python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub Actions workflows and their components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A workflow is an automated process that you define in a YAML file within your repository's `.github/workflows` directory. Think of it as a recipe that tells GitHub exactly what to do, how and when to do it. You can transform virtually any programmable task into a GitHub workflow as long as it can be executed in a Linux, Windows, or macOS environment and doesn't require direct user interaction.\n",
    "\n",
    "Here is a basic workflow structure:\n",
    "\n",
    "```yaml\n",
    "# test.yaml\n",
    "name: Python Tests\n",
    "on: [push, pull_request]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Check out repository\n",
    "        uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The YAML file starts by specifying the name of the workflow with the `name` field. Immediately after, we specify the events that triggers this workflow. In this example, the workflow automatically executes on each `git push` command and pull request. We will learn more about events and triggers in a later section. \n",
    "\n",
    "Next, we define jobs, which are the building blocks of workflows. Each job:\n",
    "\n",
    "- Runs on a fresh virtual machine (called a runner) that is specified using the `runs-on` field.\n",
    "- Can execute multiple steps in sequence\n",
    "- Can run in parallel with other jobs\n",
    "- Has access to shared workflow data\n",
    "\n",
    "For example, you might have separate jobs for testing and deployment:\n",
    "\n",
    "```yaml\n",
    "jobs:\n",
    "    test:\n",
    "        runs-on: ubuntu-latest\n",
    "        ...\n",
    "    deploy:\n",
    "        runs-on: macos-latest\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Each job can contain one or more `steps` that are executed sequentially. Steps are individual tasks that make up your job. They can:\n",
    "\n",
    "- Run commands or shell scripts\n",
    "- Execute actions (reusable units of code)\n",
    "- Run commands in Docker containers\n",
    "- Reference other GitHub repositories\n",
    "\n",
    "For example, a typical test job might have steps to:\n",
    "\n",
    "1. Check out (clone) code from your GitHub repository\n",
    "2. Set up dependencies\n",
    "3. Run tests\n",
    "4. Upload test results\n",
    "\n",
    "Each step can specify:\n",
    "\n",
    "- `name`: A display name for the step\n",
    "- `uses`: Reference to an action to run\n",
    "- `run`: Any operating-system specific terminal command like `pip install package` or `python script.py`\n",
    "- `with`: Input parameters for actions\n",
    "- `env`: Environment variables for the step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand jobs and steps, let's look at Actions - the reusable building blocks that make GitHub Actions so powerful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `test.yaml` file from earlier has a single `test` job that executes two steps:\n",
    "\n",
    "1. Checking out the repository code using a built-in `actions/checkout@v3` action.\n",
    "2. Setting up a Python environment with `actions/setup-python@v4` and `python-version` as an input parameter for said action.\n",
    "\n",
    "```bash\n",
    "# test.yaml\n",
    "name: Python Tests\n",
    "on: [push, pull_request]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Check out repository\n",
    "        uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "```\n",
    "\n",
    "Actions are reusable units of code that can be shared across workflows (this is where GitHub Actions take its name). They are like pre-packaged functions that handle common tasks. For instance, instead of writing code to set up Node.js or caching dependencies, you can use the GitHub official actions like:\n",
    "\n",
    "- `actions/setup-node@v3` - Sets up Node.js environment\n",
    "- `actions/cache@v3` - Caches dependencies and build outputs\n",
    "- `actions/upload-artifact@v3` - Uploads workflow artifacts\n",
    "- `actions/download-artifact@v3` - Downloads workflow artifacts\n",
    "- `actions/labeler@v4` - Automatically labels pull requests\n",
    "- `actions/stale@v8` - Marks and closes stale issues/PRs\n",
    "- `actions/dependency-review-action@v3` - Reviews dependency changes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events and triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Events are specific activities that trigger a workflow. Common triggers include:\n",
    "\n",
    "- `push`: When code is pushed to the repository\n",
    "- `pull_request`: When a PR is opened or updated\n",
    "- `schedule`: At specified times using cron syntax\n",
    "- `workflow_dispatch`: Manual trigger via GitHub UI\n",
    "\n",
    "Here is how you can configure multiple triggers:\n",
    "\n",
    "```yaml\n",
    "name: Comprehensive Workflow\n",
    "on:\n",
    "  push:\n",
    "    branches: [main]\n",
    "  pull_request:\n",
    "    branches: [main]\n",
    "  schedule:\n",
    "    - cron: '0 0 * * *'  # Daily at midnight\n",
    "  workflow_dispatch:  # Manual trigger\n",
    "\n",
    "jobs:\n",
    "  process:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Run daily tasks\n",
    "        run: python daily_tasks.py\n",
    "        env:\n",
    "          API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how a single workflow can:\n",
    "\n",
    "- Run automatically on code changes on `git push`\n",
    "- Execute daily scheduled tasks with cron\n",
    "- Be triggered automatically when needed through the GitHub UI\n",
    "- Handle sensitive data like API keys securely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cron jobs in GitHub Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `schedule` trigger effectively in GitHub Actions, you'll need to understand cron syntax. This powerful scheduling format lets you automate workflows to run at precise times. The syntax uses five fields to specify when a job should run:\n",
    "\n",
    "![](images/cron-syntax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some common cron schedule examples:\n",
    "\n",
    "```yaml\n",
    "# Daily at 3:30 AM UTC\n",
    "- cron: '30 3 * * *'\n",
    "\n",
    "# Every Monday at 1:00 PM UTC\n",
    "- cron: '0 13 * * 1'\n",
    "\n",
    "# Every 6 hours at the first minute\n",
    "- cron: '0 */6 * * *'\n",
    "\n",
    "# At minute 15 of every hour\n",
    "- cron: '15 * * * *'\n",
    "\n",
    "# Every weekday (Monday through Friday)\n",
    "- cron: '0 0 * * 1-5'\n",
    "\n",
    "# Each day at 12am, 6am, 12pm, 6pm on Tuesday, Thursday, Saturday\n",
    "- cron: '0 0,6,12,18 * * 1,3,5'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample workflow for a scraping job with four different schedules (multiple schedules are allowed):\n",
    "\n",
    "```yaml\n",
    "name: Price Scraper Schedules\n",
    "on:\n",
    "  schedule:\n",
    "    - cron: '0 */4 * * *'    # Every 4 hours\n",
    "    - cron: '30 1 * * *'     # Daily at 1:30 AM UTC\n",
    "    - cron: '0 9 * * 1-5'    # Weekdays at 9 AM UTC\n",
    "\n",
    "jobs:\n",
    "  scrape:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Run Firecrawl scraper\n",
    "        env:\n",
    "          API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\n",
    "        run: python scraper.py\n",
    "```\n",
    "\n",
    "Remember that GitHub Actions runs on UTC time, and schedules might experience slight delays during peak GitHub usage. That's why it's helpful to combine `schedule` with `workflow_dispatch` as we saw earlier - giving you both automated and manual trigger options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------------\n",
    "\n",
    "Understanding these core concepts allows you to create workflows that are efficient (running only when needed), secure (properly handling sensitive data), maintainable (using reusable actions) and scalable (running on different platforms). \n",
    "\n",
    "In the next section, we will put these concepts into practice by creating your first GitHub actions workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Your First GitHub Actions Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a practical GitHub Actions workflow from scratch. We'll build a workflow that automatically tests a Python script and runts it on a schedule - a universal task applicable to any programming language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a working directory for this mini-project:\n",
    "\n",
    "```bash\n",
    "mkdir first-workflows\n",
    "cd first-workflows\n",
    "```\n",
    "\n",
    "Let's create the standard `.github/workflows` folder structure GitHub uses for detecting workflow files:\n",
    "\n",
    "```bash\n",
    "mkdir -p .github/workflows\n",
    "```\n",
    "\n",
    "The workflow files can have any name but must have a `.yml` extension:\n",
    "\n",
    "```bash\n",
    "touch .github/workflows/system_monitor.yml\n",
    "```\n",
    "\n",
    "In addition to the workflows folder, create a `tests` folder as well as a test file:\n",
    "\n",
    "```bash\n",
    "mkdir tests\n",
    "touch tests/test_main.py\n",
    "```\n",
    "\n",
    "We should also create the `main.py` file along with a `requirements.txt`:\n",
    "\n",
    "```bash\n",
    "touch main.py requirements.txt\n",
    "```\n",
    "\n",
    "Then, add these two dependencies to `requirements.txt`:\n",
    "\n",
    "```text\n",
    "psutil>=5.9.0\n",
    "pytest>=7.0.0\n",
    "```\n",
    "\n",
    "Finally, let's initialize git and make our first commit:\n",
    "\n",
    "```bash\n",
    "git init \n",
    "git add .\n",
    "git commit -m \"Initial commit\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the [Git documentation](https://git-scm.com/doc) if you don't have it installed already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing your first workflow file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the workflow logic first. Open `system_monitor.yml` and paste each code snippet we are about to define one after the other. \n",
    "\n",
    "1. Workflow name and triggers:\n",
    "\n",
    "```yaml\n",
    "name: System Monitoring\n",
    "on:\n",
    "  schedule:\n",
    "    - cron: '*/30 * * * *'  # Run every 30 minutes\n",
    "  workflow_dispatch:        # Enables manual trigger\n",
    "```\n",
    "\n",
    "In this part, we give a descriptive name to the workflow that appears in GitHub's UI. Using the `on` field, we set the workflow to run every 30 minutes and through a manual trigger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Job definition:\n",
    "\n",
    "```yaml\n",
    "jobs:\n",
    "  run_script:\n",
    "    runs-on: ubuntu-latest\n",
    "```\n",
    "\n",
    "`jobs` contains all the jobs in this workflow and it has a `run_script` name, which is a unique identifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Steps:\n",
    "\n",
    "There are five steps that run sequentially in this workflow. They are given descriptive names that appear in the GitHub UI and uses official GitHub actions and custom terminal commands. \n",
    "\n",
    "```yaml\n",
    "jobs:\n",
    "  monitor:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "      - name: Check out repository\n",
    "        uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "      \n",
    "      - name: Run tests\n",
    "        run: pytest tests/\n",
    "      \n",
    "      - name: Collect system metrics\n",
    "        run: python main.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what each step does:\n",
    "\n",
    "1. Check out repository code with `actions/checkout@v3`.\n",
    "2. Configures Python 3.9 environment.\n",
    "3. Runs two terminal commands that:\n",
    "    - Install/upgrade `pip`\n",
    "    - Install `pytest` package\n",
    "4. Runs the tests located in the `tests` directory using `pytest`.\n",
    "5. Executes the main script with `python main.py`. \n",
    "\n",
    "Notice the use of `|` (pipe) operator for multi-line commands.\n",
    "\n",
    "After you complete writing the workflow, commit the changes to Git:\n",
    "\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Add a workflow file for monitoring system resources\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Python script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write the `main.py` file, which is a monitoring script that helps software developers track system resource usage over time, enabling them to identify performance bottlenecks and capacity issues in their development environment. \n",
    "\n",
    "```python\n",
    "import psutil\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "```\n",
    "\n",
    "This script collects and logs system metrics over time. It uses `psutil` to gather CPU usage, memory usage, disk usage, and active process counts. The metrics are timestamped and saved to JSON files organized by date.\n",
    "\n",
    "The script has three main functions:\n",
    "\n",
    "```python\n",
    "def get_system_metrics():\n",
    "    \"\"\"Collect key system metrics\"\"\"\n",
    "    metrics = {\n",
    "        \"cpu_percent\": psutil.cpu_percent(interval=1),\n",
    "        \"memory_percent\": psutil.virtual_memory().percent,\n",
    "        \"disk_usage\": psutil.disk_usage('/').percent,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Add running processes count\n",
    "    metrics[\"active_processes\"] = len(psutil.pids())\n",
    "    \n",
    "    return metrics\n",
    "```\n",
    "\n",
    "`get_system_metrics()` - Collects current system metrics including CPU percentage, memory usage percentage, disk usage percentage, timestamp, and count of active processes.\n",
    "\n",
    "```python\n",
    "def save_metrics(metrics):\n",
    "    \"\"\"Save metrics to a JSON file with today's date\"\"\"\n",
    "    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    reports_dir = Path(\"system_metrics\")\n",
    "    reports_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save to daily file\n",
    "    file_path = reports_dir / f\"metrics_{date_str}.json\"\n",
    "    \n",
    "    # Load existing metrics if file exists\n",
    "    if file_path.exists():\n",
    "        with open(file_path) as f:\n",
    "            daily_metrics = json.load(f)\n",
    "    else:\n",
    "        daily_metrics = []\n",
    "    \n",
    "    # Append new metrics\n",
    "    daily_metrics.append(metrics)\n",
    "    \n",
    "    # Save updated metrics\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(daily_metrics, f, indent=2)\n",
    "```\n",
    "\n",
    "`save_metrics()` - Handles saving the metrics to JSON files. It creates a `system_metrics` directory if needed, and saves metrics to date-specific files (e.g. `metrics_2024-12-12.json`). If a file for the current date exists, it loads and appends to it, otherwise creates a new file.\n",
    "\n",
    "```python\n",
    "def main():\n",
    "    try:\n",
    "        metrics = get_system_metrics()\n",
    "        save_metrics(metrics)\n",
    "        print(f\"System metrics collected at {metrics['timestamp']}\")\n",
    "        print(f\"CPU: {metrics['cpu_percent']}% | Memory: {metrics['memory_percent']}%\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting metrics: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "`main()` - Orchestrates the metric collection and saving process. It calls `get_system_metrics()`, saves the data via `save_metrics()`, prints current CPU and memory usage to console, and handles any errors that occur during execution.\n",
    "\n",
    "The script can be run directly or imported as a module. When run directly (which is what happens in a GitHub Actions workflow), it executes the `main()` function which collects and saves one set of metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the code snippets above into the `main.py` file and commit the changes:\n",
    "\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Add the main.py functionality\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing is a critical part of software engineering workflows for several reasons:\n",
    "\n",
    "1. Reliability: Tests help ensure code behaves correctly and consistently across changes.\n",
    "2. Regression prevention: Tests catch when new changes break existing functionality.\n",
    "3. Documentation: Tests serve as executable documentation of expected behavior\n",
    "4. Design feedback: Writing tests helps identify design issues early\n",
    "5. Confidence: A good test suite gives confidence when refactoring or adding features\n",
    "\n",
    "For our system metrics collection script, tests would be valuable to verify:\n",
    "\n",
    "- The `get_system_metrics()` function returns data in the expected format with valid ranges\n",
    "- The `save_metrics()` function properly handles file operations and JSON serialization\n",
    "- Error handling works correctly for various failure scenarios\n",
    "- The `main()` function orchestrates the workflow as intended\n",
    "\n",
    "With that said, let's work on the `tests/test_main.py` file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from main import get_system_metrics, save_metrics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test file we are about to write demonstrates key principles of testing with `pytest`, a popular Python testing framework. Pytest makes it easy to write tests by using simple `assert` statements and providing a rich set of features for test organization and execution. The test functions are automatically discovered by `pytest` when their names start with `test_`, and each function tests a specific aspect of the system's functionality.\n",
    "\n",
    "```python\n",
    "def test_get_system_metrics():\n",
    "    \"\"\"Test if system metrics are collected correctly\"\"\"\n",
    "    metrics = get_system_metrics()\n",
    "    \n",
    "    # Check if all required metrics exist and are valid\n",
    "    assert 0 <= metrics['cpu_percent'] <= 100\n",
    "    assert 0 <= metrics['memory_percent'] <= 100\n",
    "    assert metrics['active_processes'] > 0\n",
    "    assert 'timestamp' in metrics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have two test functions that verify different components of our metrics collection system. The first test, `test_get_system_metrics()`, checks if the metrics collection function returns data in the expected format and with valid ranges. It uses multiple assert statements to verify that CPU and memory percentages are between 0 and 100, that there are active processes, and that a timestamp is included. This demonstrates the practice of testing both the structure of returned data and the validity of its values.\n",
    "\n",
    "```python\n",
    "def test_save_and_read_metrics():\n",
    "    \"\"\"Test if metrics are saved and can be read back\"\"\"\n",
    "    # Get and save metrics\n",
    "    metrics = get_system_metrics()\n",
    "    save_metrics(metrics)\n",
    "    \n",
    "    # Check if file exists and contains data\n",
    "    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    file_path = Path(\"system_metrics\") / f\"metrics_{date_str}.json\"\n",
    "    \n",
    "    assert file_path.exists()\n",
    "    with open(file_path) as f:\n",
    "        saved_data = json.load(f)\n",
    "    \n",
    "    assert isinstance(saved_data, list)\n",
    "    assert len(saved_data) > 0\n",
    "```\n",
    "\n",
    "The second test, `test_save_and_read_metrics()`, showcases integration testing by verifying that metrics can be both saved to and read from a file. It follows a common testing pattern: arrange (setup the test conditions), act (perform the operations being tested), and assert (verify the results). The test ensures that the file is created in the expected location and that the saved data maintains the correct structure. This type of test is particularly valuable as it verifies that different components of the system work together correctly.\n",
    "\n",
    "Combine the above code snippets and commit the changes to GitHub:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "git add .\n",
    "git commit -m \"Write tests for main.py\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running your first GitHub Actions workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created our system monitoring workflow, let's set it up on GitHub and run it. First, push everything we have to a new GitHub repository:\n",
    "\n",
    "```bash\n",
    "git remote add origin https://github.com/your-username/your-repository.git\n",
    "git branch -M main\n",
    "git push -u origin main\n",
    "```\n",
    "\n",
    "Once the workflow file is pushed, GitHub automatically detects it and displays in the \"Actions\" tab of your repository. The workflow is scheduled so you don't need to do anything - the first workflow run will happen within 30 minutes (remember how we set the running interval to `*/30` with cron). Since the workflow also includes a `workflow_dispatch` field, you can trigger it manually by clicking on the \"Run workflow\" button. \n",
    "\n",
    "After clicking the button, a new run appears within a few seconds (refresh if you don't see it). To see the workflow run in real-time, click on it and expand the `monitor` job. You'll see each step executing:\n",
    "\n",
    "- Checking out repository\n",
    "- Setting up Python\n",
    "- Installing dependencies\n",
    "- Running tests\n",
    "- Collecting metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Committing changes made by GitHub Actions workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, our workflow file has a problem - while it successfully collects metrics, it doesn't commit and push the changes back to the repository. This means that although metrics are being gathered, they aren't being saved in version control. Let's modify the workflow to automatically commit and push the collected metrics:\n",
    "\n",
    "```yaml\n",
    "name: System Monitor\n",
    "\n",
    "on:\n",
    "  schedule:\n",
    "    - cron: '*/30 * * * *'\n",
    "  workflow_dispatch:\n",
    "\n",
    "permissions:\n",
    "  contents: write\n",
    "\n",
    "jobs:\n",
    "  monitor:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.10'\n",
    "          \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "          \n",
    "      - name: Run tests\n",
    "        run: python -m pytest\n",
    "        \n",
    "      - name: Collect metrics\n",
    "        run: python main.py\n",
    "        \n",
    "      - name: Commit and push changes\n",
    "        run: |\n",
    "          git config --global user.name 'github-actions[bot]'\n",
    "          git config --global user.email 'github-actions[bot]@users.noreply.github.com'\n",
    "          git add metrics.json\n",
    "          git commit -m \"Update metrics\" || exit 0\n",
    "          git push\n",
    "```\n",
    "\n",
    "The key changes in this updated workflow are:\n",
    "\n",
    "1. Added `permissions` block with `contents: write` to allow the workflow to push changes back to the repository.\n",
    "2. Added a new \"Commit and push changes\" step that:\n",
    "   - Configures git user identity as `github-actions` bot\n",
    "   - Stages the `metrics.json` file\n",
    "   - Creates a commit with message \"Update metrics\" \n",
    "   - Pushes the changes back to the repository\n",
    "   \n",
    "The \"|| exit 0\" after `git commit` ensures the workflow doesn't fail if there are no changes to commit.\n",
    "\n",
    "This allows the workflow to automatically save and version control the metrics it collects. Let's commit the changes to the workflow and push:\n",
    "\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Add a commit step to the workflow file\"\n",
    "git push origin main\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this change, try running the workflow manually and verify its success by navigating to the Actions tab in your GitHub repository. You should see the workflow run and the `metrics.json` file updated with new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Sensitive Data and Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building automated workflows with GitHub Actions, proper handling of sensitive data like API keys, passwords, and access tokens is crucial for security. Let's explore best practices for managing these credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding environment variables in GitHub Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment variables in GitHub Actions can be set at different levels:\n",
    "\n",
    "- Repository level (GitHub Secrets)\n",
    "- Workflow level\n",
    "- Job level\n",
    "- Step level\n",
    "\n",
    "Here is how to properly configure and use them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setting up repository secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, store sensitive values as repository secrets:\n",
    "\n",
    "1. Navigate to your GitHub repository\n",
    "2. Go to Settings ‚Üí Secrets and variables ‚Üí Actions\n",
    "3. Click \"New repository secret\"\n",
    "4. Add your secrets with descriptive names like:\n",
    "\n",
    "- `API_KEY`\n",
    "- `DATABASE_URL`\n",
    "- `AWS_ACCESS_KEY`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Using secrets in workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference secrets in your workflow file using the `secrets` context:\n",
    "\n",
    "```yaml\n",
    "name: Web Scraping Pipeline\n",
    "# ... the rest of the file\n",
    "\n",
    "jobs:\n",
    "  scrape:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "      # ... the rest of the steps\n",
    "          \n",
    "      - name: Run scraper\n",
    "        env:\n",
    "          API_KEY: ${{ secrets.API_KEY }}\n",
    "          DATABASE_URL: ${{ secrets.DATABASE_URL }}\n",
    "        run: python scraper.py\n",
    "```\n",
    "\n",
    "Above, the \"Run scraper\" step executes `scraper.py` which relies on two environment variables configured through the `secrets` context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Local development with .env files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For local development, use `.env` files to manage environment variables:\n",
    "\n",
    "```bash\n",
    "touch .env\n",
    "echo \"API_KEY='your-api-key-here'\" >> .env\n",
    "echo \"DATABASE_URL='postgresql://user:pass@localhost:5432/db'\" >> .env\n",
    "```\n",
    "\n",
    "Create a `.gitignore` file to prevent committing sensitive data:\n",
    "\n",
    "```bash\n",
    "echo \".env\" >> .gitignore\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Loading environment variables in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `python-dotenv` to load variables from `.env` files:\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access variables\n",
    "api_key = os.getenv('API_KEY')\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "if not api_key or not database_url:\n",
    "    raise ValueError(\"Missing required environment variables\")\n",
    "```\n",
    "\n",
    "This code demonstrates loading environment variables from a `.env` file using `python-dotenv`. The `load_dotenv()` function reads the variables, which can then be accessed via `os.getenv()`. Basic validation ensures required variables exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Environment variable validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a configuration class to validate environment variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pydantic import BaseSettings, SecretStr\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    api_key: SecretStr\n",
    "    database_url: str\n",
    "    debug_mode: bool = False\n",
    "\n",
    "    class Config:\n",
    "        env_file = \".env\"\n",
    "        env_file_encoding = \"utf-8\"\n",
    "\n",
    "\n",
    "# Initialize settings\n",
    "settings = Settings()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach using Pydantic provides several advantages for environment variable management:\n",
    "\n",
    "1. Type validation - Pydantic automatically validates types and converts values\n",
    "2. Default values - The `debug_mode` demonstrates setting defaults\n",
    "3. Secret handling - `SecretStr` provides secure handling of sensitive values\n",
    "4. Centralized config - All environment variables are defined in one place\n",
    "5. IDE support - Get autocomplete and type hints when using the settings object\n",
    "\n",
    "The `Settings` class inherits from `BaseSettings` which automatically loads from environment variables. The `Config` class specifies to also load from a `.env` file.\n",
    "\n",
    "Using `settings = Settings()` creates a validated configuration object that can be imported and used throughout the application. This is more robust than accessing `os.environ` directly.\n",
    "\n",
    "Example usage:\n",
    "\n",
    "```python\n",
    "settings.api_key.get_secret_value()  # Securely access API key\n",
    "settings.database_url  # Type-checked database URL\n",
    "settings.debug_mode  # Boolean with default value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Handle different environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle different environments (development, staging, production) using environment-specific files:\n",
    "\n",
    "```bash\n",
    ".env                # Default environment variables\n",
    ".env.development   # Development-specific variables\n",
    ".env.staging       # Staging-specific variables\n",
    ".env.production    # Production-specific variables\n",
    "```\n",
    "\n",
    "Load the appropriate file based on the environment:\n",
    "\n",
    "```bash\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "env = os.getenv('ENVIRONMENT', 'development')\n",
    "env_file = f'.env.{env}'\n",
    "\n",
    "load_dotenv(env_file)\n",
    "```\n",
    "\n",
    "This approach allows you to maintain separate configurations for different environments while keeping sensitive information secure. The environment-specific files can contain different values for the same variables, such as:\n",
    "\n",
    "- Development environment may use local services and dummy credentials\n",
    "- Staging environment may use test services with restricted access\n",
    "- Production environment contains real credentials and production service endpoints\n",
    "\n",
    "You can also combine this with the `Pydantic` settings approach shown earlier for robust configuration management across environments.\n",
    "\n",
    "For example, staging might use a test database while production uses the live database:\n",
    "\n",
    "```bash\n",
    "# .env.staging:\n",
    "DATABASE_URL=postgresql://test-db.example.com\n",
    "API_KEY=test-key\n",
    "```\n",
    "\n",
    "```bash\n",
    ".env.production:\n",
    "DATABASE_URL=postgresql://prod-db.example.com \n",
    "API_KEY=live-key\n",
    "```\n",
    "\n",
    "This separation helps prevent accidental use of production resources during development and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Real-World Python Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore three practical examples of GitHub Actions workflows for common Python tasks: web scraping, package publishing, and container builds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scheduled web scraping with Firecrawl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is a common use case for automated workflows. Let's build a workflow that scrapes [Hacker News](https://news.ycombinator.com/) on a schedule using [Firecrawl](https://docs.firecrawl.dev), which is a Python AI-based web scraping engine designed for large-scale data collection. Here are some key benefits that make Firecrawl an excellent choice for this task:\n",
    "\n",
    "1. **Enterprise-grade automation and scalability** - Firecrawl streamlines web scraping with powerful automation features.\n",
    "2. **AI-powered content extraction** - Maintains scraper reliability over time by identifying and extracting data based on semantic descriptions instead of relying HTML elements and CSS selectors.\n",
    "3. **Handles complex scraping challenges** - Automatically manages proxies, anti-bot mechanisms, and dynamic JavaScript content.\n",
    "4. **Multiple output formats** - Supports scraping and converting data in markdown, tabular, screenshots, and HTML, making it versatile for various applications.\n",
    "5. **Built-in rate limiting and request management** - Ensures efficient and compliant data extraction.\n",
    "6. **Geographic location customization** - Avoids IP bans by customizing the geographic location of requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our web scraping workflow using Firecrawl to demonstrate these capabilities.\n",
    "\n",
    "```bash\n",
    "# Create project directory and install dependencies\n",
    "mkdir hacker-news-scraper && cd hacker-news-scraper\n",
    "pip install firecrawl-py pydantic python-dotenv\n",
    "\n",
    "# Create necessary files\n",
    "touch requirements.txt scraper.py .env\n",
    "\n",
    "# Add dependencies to requirements.txt\n",
    "echo \"firecrawl-py\\npydantic\\npython-dotenv\" > requirements.txt\n",
    "\n",
    "# Add Firecrawl API key to .env (get your key at firecrawl.dev/signin/signup)\n",
    "echo \"FIRECRAWL_API_KEY='your_api_key_here'\" > .env\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the scraper script where we define our scraping logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper.py\n",
    "import json\n",
    "from firecrawl import FirecrawlApp\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "BASE_URL = \"https://news.ycombinator.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import necessary libraries and packages, also defining a base URL we are going to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsItem(BaseModel):\n",
    "    title: str = Field(description=\"The title of the news item\")\n",
    "    source_url: str = Field(description=\"The URL of the news item\")\n",
    "    author: str = Field(\n",
    "        description=\"The URL of the post author's profile concatenated with the base URL.\"\n",
    "    )\n",
    "    rank: str = Field(description=\"The rank of the news item\")\n",
    "    upvotes: str = Field(description=\"The number of upvotes of the news item\")\n",
    "    date: str = Field(description=\"The date of the news item.\")\n",
    "\n",
    "\n",
    "class NewsData(BaseModel):\n",
    "    news_items: List[NewsItem]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two Pydantic models to structure our scraped data:\n",
    "\n",
    "1. `NewsItem` - Represents a single news item with fields for title, URL, author, rank, upvotes and date\n",
    "2. `NewsData` - Contains a list of `NewsItem` objects\n",
    "\n",
    "These models help validate the scraped data and ensure it matches our expected schema. They also make it easier to serialize/deserialize the data when saving to JSON. Using `Field` with a detailed description is crucial because Firecrawl uses these definitions to automatically detect the HTMl elements and CSS selectors we are looking for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_data():\n",
    "    app = FirecrawlApp()\n",
    "\n",
    "    data = app.scrape_url(\n",
    "        BASE_URL,\n",
    "        params={\n",
    "            \"formats\": [\"extract\"],\n",
    "            \"extract\": {\"schema\": NewsData.model_json_schema()},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_news_data()` function uses Firecrawl to scrape Hacker News. It creates a `FirecrawlApp` instance and calls `scrape_url()` with the `BASE_URL` and parameters specifying we want to extract data according to our `NewsData` schema. The schema helps Firecrawl automatically identify and extract the relevant HTML elements. The function returns the scraped data containing news items with their titles, URLs, authors, ranks, upvotes and dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_firecrawl_news_data():\n",
    "    \"\"\"\n",
    "    Save the scraped news data to a JSON file with the current date in the filename.\n",
    "    \"\"\"\n",
    "    # Get the data\n",
    "    data = get_news_data()\n",
    "    # Format current date for filename\n",
    "    date_str = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "    filename = f\"firecrawl_hacker_news_data_{date_str}.json\"\n",
    "\n",
    "    # Save the news items to JSON file\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data[\"extract\"][\"news_items\"], f, indent=4)\n",
    "\n",
    "    print(f\"{datetime.now()}: Successfully saved the news data.\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    save_firecrawl_news_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `save_firecrawl_news_data()` function handles saving the scraped Hacker News data to a JSON file. It first calls `get_news_data()` to fetch the latest data from Hacker News. Then it generates a filename using the current timestamp to ensure uniqueness. The data is saved to a JSON file with that filename, with the news items formatted with proper indentation for readability. Finally, it prints a confirmation message with the current timestamp when the save is complete. This function provides a convenient way to store snapshots of Hacker News data that can be analyzed later.\n",
    "\n",
    "Combine these snippets into the `scraper.py` script. Then, we can write a workflow that executes it on schedule:\n",
    "\n",
    "```bash\n",
    "cd ..  # Change back to the project root directory\n",
    "touch .github/workflows/hacker-news-scraper.py  # Create the workflow file\n",
    "```\n",
    "\n",
    "Here is what the workflow file must look like:\n",
    "\n",
    "```yaml\n",
    "name: Run Hacker News Scraper\n",
    "\n",
    "permissions:\n",
    "  contents: write\n",
    "\n",
    "on:\n",
    "  schedule:\n",
    "    - cron: \"0 */6 * * *\"\n",
    "  workflow_dispatch:\n",
    "\n",
    "jobs:\n",
    "  scrape:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: \"3.10\"\n",
    "          \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r hacker-news-scraper/requirements.txt\n",
    "          \n",
    "      - name: Run scraper\n",
    "        run: python hacker-news-scraper/scraper.py\n",
    "        \n",
    "      - name: Commit and push if changes\n",
    "        run: |\n",
    "          git config --local user.email \"github-actions[bot]@users.noreply.github.com\"\n",
    "          git config --local user.name \"github-actions[bot]\"\n",
    "          git add .\n",
    "          git commit -m \"Update scraped data\" -a || exit 0\n",
    "          git push\n",
    "```\n",
    "\n",
    "This workflow runs our Hacker News scraper every 6 hours using GitHub Actions. It sets up Python, installs dependencies, executes the scraper, and automatically commits any new data to the repository. The workflow can also be triggered manually using the `workflow_dispatch` event. One important note about the paths specified in the workflow file is that they must match your repository's directory structure exactly, including the requirements.txt location and the path to your scraper script.\n",
    "\n",
    "To enable the workflow, simply push all the changes to GitHub and test it through the UI. The next runs will be automatic.\n",
    "\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Add a scraping workflow\"\n",
    "git push origin main\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Package publishing workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publishing Python packages to PyPI (Python Package Index) typically involves several steps. First, developers need to prepare their package by creating a proper directory structure, writing setup files, and ensuring all metadata is correct. Then, the package needs to be built into distribution formats - both source distributions (`sdist`) and wheel distributions (`bdist_wheel`). Finally, these distribution files are uploaded to PyPI using tools like `twine`. This process often requires careful version management and proper credentials for the package repository. While this can be done manually, automating it with CI/CD pipelines like GitHub Actions ensures consistency and reduces human error in the release process.\n",
    "\n",
    "For example, the following workflow publishes a new version of a package when you create a new release:\n",
    "\n",
    "```yaml\n",
    "name: Publish Python Package\n",
    "on:\n",
    "  release:\n",
    "    types: [created]\n",
    "\n",
    "jobs:\n",
    "  deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.10'\n",
    "          \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install build twine\n",
    "          \n",
    "      - name: Build package\n",
    "        run: python -m build\n",
    "        \n",
    "      - name: Publish to PyPI\n",
    "        env:\n",
    "          TWINE_USERNAME: __token__\n",
    "          TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}\n",
    "        run: |\n",
    "          python -m twine upload dist/*\n",
    "```\n",
    "\n",
    "The workflow automates publishing Python packages to PyPI when GitHub releases are created. \n",
    "\n",
    "Required setup steps:\n",
    "1. Package must have `setup.py` or `pyproject.toml` configured\n",
    "2. Create PyPI account at `pypi.org`\n",
    "3. Generate PyPI API token with upload permissions\n",
    "4. Store token as `PYPI_API_TOKEN` in repository secrets\n",
    "\n",
    "The workflow process:\n",
    "1. Triggers on new GitHub release\n",
    "2. Checks out code and sets up Python\n",
    "3. Installs build tools\n",
    "4. Creates distribution packages\n",
    "5. Uploads to PyPI using stored token\n",
    "\n",
    "The `__token__` username is used with PyPI's token authentication, while the actual token is accessed securely through GitHub secrets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Container build and push workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Actions can also automate building and pushing Docker containers to container registries like Docker Hub or GitHub Container Registry (GHCR). This workflow is useful for maintaining containerized applications and ensuring your latest code changes are packaged into updated container images.\n",
    "\n",
    "The process typically involves:\n",
    "\n",
    "1. Building a Docker image from your `Dockerfile`\n",
    "2. Tagging the image with version/metadata\n",
    "3. Authenticating with the container registry\n",
    "4. Pushing the tagged image to the registry\n",
    "\n",
    "This automation ensures your container images stay in sync with code changes and are readily available for deployment. Here is a sample workflow containing these steps:\n",
    "\n",
    "```yaml\n",
    "name: Build and Push Container\n",
    "on:\n",
    "  push:\n",
    "    branches: [main]\n",
    "    paths:\n",
    "      - 'Dockerfile'\n",
    "      - 'src/**'\n",
    "  workflow_dispatch:\n",
    "\n",
    "jobs:\n",
    "  build:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Docker Buildx\n",
    "        uses: docker/setup-buildx-action@v2\n",
    "        \n",
    "      - name: Login to Docker Hub\n",
    "        uses: docker/login-action@v2\n",
    "        with:\n",
    "          username: ${{ secrets.DOCKERHUB_USERNAME }}\n",
    "          password: ${{ secrets.DOCKERHUB_TOKEN }}\n",
    "          \n",
    "      - name: Build and push\n",
    "        uses: docker/build-push-action@v4\n",
    "        with:\n",
    "          context: .\n",
    "          push: true\n",
    "          tags: |\n",
    "            user/app:latest\n",
    "            user/app:${{ github.sha }}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workflow introduces a few new GitHub Actions concepts and syntax:\n",
    "\n",
    "The `paths` trigger filter ensures the workflow only runs when changes are made to the Dockerfile or files in the `src` directory, preventing unnecessary builds.\n",
    "\n",
    "`docker/setup-buildx-action` configures Docker Buildx, which provides enhanced build capabilities including multi-platform builds and build caching.\n",
    "\n",
    "`docker/login-action` handles registry authentication. Before using this, you must:\n",
    "\n",
    "1. [Create a Docker Hub account](https://app.docker.com/signup)\n",
    "2. Generate an access token in Docker Hub settings\n",
    "3. Add `DOCKERHUB_USERNAME` and `DOCKERHUB_TOKEN` as repository secrets in GitHub\n",
    "\n",
    "`docker/build-push-action` is a specialized action for building and pushing Docker images. The configuration shows:\n",
    "- `context: .` (builds from current directory)\n",
    "- `push: true` (automatically pushes after building)\n",
    "- `tags:` specifies multiple tags including:\n",
    "  - `latest:` rolling tag for most recent version\n",
    "  - `github.sha:` unique tag using commit hash for versioning\n",
    "\n",
    "The workflow assumes you have:\n",
    "- A valid Dockerfile in your repository\n",
    "- Required application code in `src` directory\n",
    "- Docker Hub repository permissions\n",
    "- Properly configured repository secrets\n",
    "\n",
    "When this workflow runs successfully, it produces a containerized version of your application that is automatically published to Docker Hub and can be pulled with either the latest tag or specific commit hash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this tutorial, we've explored the fundamentals and practical applications of GitHub Actions for Python development. From understanding core concepts like workflows, jobs, and actions, to implementing real-world examples including automated testing, web scraping, package publishing, and container builds, you've gained hands-on experience with this powerful automation platform. We've also covered critical aspects like managing sensitive data through environment variables and secrets, ensuring your automated workflows are both secure and maintainable.\n",
    "\n",
    "As you continue your journey with GitHub Actions, remember that automation is an iterative process. Start small with basic workflows, test thoroughly, and gradually add complexity as needed. The examples provided here serve as templates that you can adapt and expand for your specific use cases. For further learning, explore the [GitHub Actions documentation](https://docs.github.com/en/actions), join the [GitHub Community Forum](https://github.community/), and experiment with the vast ecosystem of pre-built actions available in the [GitHub Marketplace](https://github.com/marketplace?type=actions). Whether you're building a personal project or managing enterprise applications, GitHub Actions provides the tools you need to streamline your development workflow and focus on what matters most - writing great code.\n",
    "\n",
    "If you want to learn more about Firecrawl, the web scraping API we used today, you can read the following posts:\n",
    "\n",
    "- [Guide to Scheduling Web Scrapers in Python](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025)\n",
    "- [Mastering Firecrawl's Scrape Endpoint](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint)\n",
    "- [Getting Started With Predicted Outputs in OpenAI](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai)\n",
    "\n",
    "Thank you for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

================
File: examples/blog-articles/github-actions-tutorial/notebook.md
================
---
title: Comprehensive GitHub Actions Tutorial For Beginners With Examples in Python
description: Learn how to automate software development workflows with GitHub Actions. This beginner-friendly tutorial covers workflow creation, CI/CD pipelines, scheduled tasks, and practical Python examples to help you streamline your development process.
slug: github-actions-tutorial-for-beginners-with-python-examples
date: Dec 9, 2024
author: bex_tuychiev
image: /images/blog/github-actions-tutorial/github-actions-tutorial-for-beginners-with-python-examples.jpg
categories: [tutorials]
keywords: [github actions, github actions tutorial, github actions environment variables, github actions secrets, github actions workflow, github actions run, github actions jobs]
---

## Introduction

GitHub Actions is a powerful automation platform that helps developers automate repetitive, time-consuming software development workflows. Instead of manually running tests, executing scripts at intervals, or performing any programmable task, you can let GitHub Actions handle those operations when specific events occur in your repository. In this tutorial, you will learn how to use this critical feature of GitHub and design your own workflows for several real-world use cases.

### What are GitHub Actions?

At its core, [GitHub Actions](https://docs.github.com/en/actions) is a continuous integration and continuous delivery (CI/CD) platform that lets you automate various tasks directly from your GitHub repository. Think of it as your personal automation assistant, which can:

- Run your Python tests automatically when you push code
- Deploy your application when you create a new release
- Send notifications when issues are created
- Schedule tasks to run at specific times
- And much more...

### Why automate with GitHub Actions?

Consider this common scenario: You are building a Python application that scrapes product prices from various e-commerce websites. Without GitHub Actions, you would need to:

1. Manually run your tests after each code change
2. Remember to execute the scraper at regular intervals
3. Deploy updates to your production environment
4. Keep track of environment variables and secrets

With GitHub Actions, all of these tasks can be automated through workflows, typically written in YAML files like this:

```yaml
name: Run Price Scraper

on:
  schedule:
    - cron: '0 */12 * * *'  # Runs every 12 hours
  workflow_dispatch:  # Allows manual triggers

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Run scraper
      env:
        API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}
      run: python scraper.py
```

This workflow automatically runs a scraper every 12 hours, handles Python version setup, and securely manages API keys‚Äîall without manual intervention.

### What we'll build in this tutorial

Throughout this tutorial, we'll build several practical GitHub Actions workflows for Python applications. You will learn how to:

1. Create basic and advanced workflow configurations
2. Work with environment variables and secrets
3. Set up automated testing pipelines
4. Build a real-world example: an automated scraping system using [Firecrawl](https://firecrawl.dev) in Python
5. Implement best practices for security and efficiency

By the end, you will have hands-on experience with GitHub Actions and be able to automate your own Python projects effectively.

> Note: Even though code examples are Python, the concepts and hands-on experience you will gain from the tutorial will apply to any programming language.

Let's start by understanding the core concepts that make GitHub Actions work.

## How to Use This GitHub Actions Tutorial

Before diving into the technical details, here's how to get the most from this GitHub Actions tutorial:

1. Follow the examples sequentially - each builds on previous concepts
2. Try running the workflows yourself - hands-on practice is crucial
3. Refer back to earlier sections as needed
4. Use the provided code samples as templates for your own projects

## Understanding GitHub Actions Core Concepts

To write your own GitHub Actions workflows, you need to understand how its different components work together. Let's break down these core concepts using a practical example: automating tests for a simple Python script.

### GitHub Actions workflows and their components

A workflow is an automated process that you define in a YAML file within your repository's `.github/workflows` directory. Think of it as a recipe that tells GitHub exactly what to do, how and when to do it. You can transform virtually any programmable task into a GitHub workflow as long as it can be executed in a Linux, Windows, or macOS environment and doesn't require direct user interaction.

Here is a basic workflow structure:

```yaml
# test.yaml
name: Python Tests
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
```

The YAML file starts by specifying the name of the workflow with the `name` field. Immediately after, we specify the events that triggers this workflow. In this example, the workflow automatically executes on each `git push` command and pull request. We will learn more about events and triggers in a later section.

Next, we define jobs, which are the building blocks of workflows. Each job:

- Runs on a fresh virtual machine (called a runner) that is specified using the `runs-on` field.
- Can execute multiple steps in sequence
- Can run in parallel with other jobs
- Has access to shared workflow data

For example, you might have separate jobs for testing and deployment:

```yaml
jobs:
    test:
        runs-on: ubuntu-latest
        ...
    deploy:
        runs-on: macos-latest
        ...
```

Each job can contain one or more `steps` that are executed sequentially. Steps are individual tasks that make up your job. They can:

- Run commands or shell scripts
- Execute actions (reusable units of code)
- Run commands in Docker containers
- Reference other GitHub repositories

For example, a typical test job might have steps to:

1. Check out (clone) code from your GitHub repository
2. Set up dependencies
3. Run tests
4. Upload test results

Each step can specify:

- `name`: A display name for the step
- `uses`: Reference to an action to run
- `run`: Any operating-system specific terminal command like `pip install package` or `python script.py`
- `with`: Input parameters for actions
- `env`: Environment variables for the step

Now that we understand jobs and steps, let's look at Actions - the reusable building blocks that make GitHub Actions so powerful.

### Actions

The `test.yaml` file from earlier has a single `test` job that executes two steps:

1. Checking out the repository code using a built-in `actions/checkout@v3` action.
2. Setting up a Python environment with `actions/setup-python@v4` and `python-version` as an input parameter for said action.

```bash
# test.yaml
name: Python Tests
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
```

Actions are reusable units of code that can be shared across workflows (this is where GitHub Actions take its name). They are like pre-packaged functions that handle common tasks. For instance, instead of writing code to set up Node.js or caching dependencies, you can use the GitHub official actions like:

- `actions/setup-node@v3` - Sets up Node.js environment
- `actions/cache@v3` - Caches dependencies and build outputs
- `actions/upload-artifact@v3` - Uploads workflow artifacts
- `actions/download-artifact@v3` - Downloads workflow artifacts
- `actions/labeler@v4` - Automatically labels pull requests
- `actions/stale@v8` - Marks and closes stale issues/PRs
- `actions/dependency-review-action@v3` - Reviews dependency changes

### Events and triggers

Events are specific activities that trigger a workflow. Common triggers include:

- `push`: When code is pushed to the repository
- `pull_request`: When a PR is opened or updated
- `schedule`: At specified times using cron syntax
- `workflow_dispatch`: Manual trigger via GitHub UI

Here is how you can configure multiple triggers:

```yaml
name: Comprehensive Workflow
on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 0 * * *'  # Daily at midnight
  workflow_dispatch:  # Manual trigger

jobs:
  process:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run daily tasks
        run: python daily_tasks.py
        env:
          API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}
```

This example shows how a single workflow can:

- Run automatically on code changes on `git push`
- Execute daily scheduled tasks with cron
- Be triggered automatically when needed through the GitHub UI
- Handle sensitive data like API keys securely

### Cron jobs in GitHub Actions

To use the `schedule` trigger effectively in GitHub Actions, you'll need to understand cron syntax. This powerful scheduling format lets you automate workflows to run at precise times. The syntax uses five fields to specify when a job should run:

![Cron syntax diagram showing minute, hour, day of month, month, and day of week fields with examples and explanations for GitHub Actions scheduling](github-actions-tutorial-images/cron-syntax.png)

Here are some common cron schedule examples:

```yaml
# Daily at 3:30 AM UTC
- cron: '30 3 * * *'

# Every Monday at 1:00 PM UTC
- cron: '0 13 * * 1'

# Every 6 hours at the first minute
- cron: '0 */6 * * *'

# At minute 15 of every hour
- cron: '15 * * * *'

# Every weekday (Monday through Friday)
- cron: '0 0 * * 1-5'

# Each day at 12am, 6am, 12pm, 6pm on Tuesday, Thursday, Saturday
- cron: '0 0,6,12,18 * * 1,3,5'
```

Here is a sample workflow for a scraping job with four different schedules (multiple schedules are allowed):

```yaml
name: Price Scraper Schedules
on:
  schedule:
    - cron: '0 */4 * * *'    # Every 4 hours
    - cron: '30 1 * * *'     # Daily at 1:30 AM UTC
    - cron: '0 9 * * 1-5'    # Weekdays at 9 AM UTC

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run Firecrawl scraper
        env:
          API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}
        run: python scraper.py
```

Remember that GitHub Actions runs on UTC time, and schedules might experience slight delays during peak GitHub usage. That's why it's helpful to combine `schedule` with `workflow_dispatch` as we saw earlier - giving you both automated and manual trigger options.

---------------

Understanding these core concepts allows you to create workflows that are efficient (running only when needed), secure (properly handling sensitive data), maintainable (using reusable actions) and scalable (running on different platforms).

In the next section, we will put these concepts into practice by creating your first GitHub actions workflow.

## Creating Your First GitHub Actions Workflow

Let's create a practical GitHub Actions workflow from scratch. We'll build a workflow that automatically tests a Python script and runts it on a schedule - a universal task applicable to any programming language.

### Setting up the environment

Let's start by creating a working directory for this mini-project:

```bash
mkdir first-workflows
cd first-workflows
```

Let's create the standard `.github/workflows` folder structure GitHub uses for detecting workflow files:

```bash
mkdir -p .github/workflows
```

The workflow files can have any name but must have a `.yml` extension:

```bash
touch .github/workflows/system_monitor.yml
```

In addition to the workflows folder, create a `tests` folder as well as a test file:

```bash
mkdir tests
touch tests/test_main.py
```

We should also create the `main.py` file along with a `requirements.txt`:

```bash
touch main.py requirements.txt
```

Then, add these two dependencies to `requirements.txt`:

```text
psutil>=5.9.0
pytest>=7.0.0
```

Finally, let's initialize git and make our first commit:

```bash
git init 
git add .
git commit -m "Initial commit"
```

Check out the [Git documentation](https://git-scm.com/doc) if you don't have it installed already.

### Writing your first workflow file

Let's write the workflow logic first. Open `system_monitor.yml` and paste each code snippet we are about to define one after the other.

- Workflow name and triggers:

```yaml
name: System Monitoring
on:
  schedule:
    - cron: '*/30 * * * *'  # Run every 30 minutes
  workflow_dispatch:        # Enables manual trigger
```

In this part, we give a descriptive name to the workflow that appears in GitHub's UI. Using the `on` field, we set the workflow to run every 30 minutes and through a manual trigger.

- Job definition:

```yaml
jobs:
  run_script:
    runs-on: ubuntu-latest
```

`jobs` contains all the jobs in this workflow and it has a `run_script` name, which is a unique identifier.

- Steps:

There are five steps that run sequentially in this workflow. They are given descriptive names that appear in the GitHub UI and uses official GitHub actions and custom terminal commands.

```yaml
jobs:
  monitor:
    runs-on: ubuntu-latest
    
    steps:
      - name: Check out repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run tests
        run: pytest tests/
      
      - name: Collect system metrics
        run: python main.py
```

Here is what each step does:

1. Check out repository code with `actions/checkout@v3`.
2. Configures Python 3.9 environment.
3. Runs two terminal commands that:
    - Install/upgrade `pip`
    - Install `pytest` package
4. Runs the tests located in the `tests` directory using `pytest`.
5. Executes the main script with `python main.py`.

Notice the use of `|` (pipe) operator for multi-line commands.

After you complete writing the workflow, commit the changes to Git:

```bash
git add .
git commit -m "Add a workflow file for monitoring system resources"
```

### Creating the Python script

Now, let's write the `main.py` file, which is a monitoring script that helps software developers track system resource usage over time, enabling them to identify performance bottlenecks and capacity issues in their development environment.

```python
import psutil
import json
from datetime import datetime
from pathlib import Path
```

This script collects and logs system metrics over time. It uses `psutil` to gather CPU usage, memory usage, disk usage, and active process counts. The metrics are timestamped and saved to JSON files organized by date.

The script has three main functions:

```python
def get_system_metrics():
    """Collect key system metrics"""
    metrics = {
        "cpu_percent": psutil.cpu_percent(interval=1),
        "memory_percent": psutil.virtual_memory().percent,
        "disk_usage": psutil.disk_usage('/').percent,
        "timestamp": datetime.now().isoformat()
    }
    
    # Add running processes count
    metrics["active_processes"] = len(psutil.pids())
    
    return metrics
```

`get_system_metrics()` - Collects current system metrics including CPU percentage, memory usage percentage, disk usage percentage, timestamp, and count of active processes.

```python
def save_metrics(metrics):
    """Save metrics to a JSON file with today's date"""
    date_str = datetime.now().strftime("%Y-%m-%d")
    reports_dir = Path("system_metrics")
    reports_dir.mkdir(exist_ok=True)
    
    # Save to daily file
    file_path = reports_dir / f"metrics_{date_str}.json"
    
    # Load existing metrics if file exists
    if file_path.exists():
        with open(file_path) as f:
            daily_metrics = json.load(f)
    else:
        daily_metrics = []
    
    # Append new metrics
    daily_metrics.append(metrics)
    
    # Save updated metrics
    with open(file_path, 'w') as f:
        json.dump(daily_metrics, f, indent=2)
```

`save_metrics()` - Handles saving the metrics to JSON files. It creates a `system_metrics` directory if needed, and saves metrics to date-specific files (e.g. `metrics_2024-12-12.json`). If a file for the current date exists, it loads and appends to it, otherwise creates a new file.

```python
def main():
    try:
        metrics = get_system_metrics()
        save_metrics(metrics)
        print(f"System metrics collected at {metrics['timestamp']}")
        print(f"CPU: {metrics['cpu_percent']}% | Memory: {metrics['memory_percent']}%")
        return True
    except Exception as e:
        print(f"Error collecting metrics: {str(e)}")
        return False

if __name__ == "__main__":
    main()
```

`main()` - Orchestrates the metric collection and saving process. It calls `get_system_metrics()`, saves the data via `save_metrics()`, prints current CPU and memory usage to console, and handles any errors that occur during execution.

The script can be run directly or imported as a module. When run directly (which is what happens in a GitHub Actions workflow), it executes the `main()` function which collects and saves one set of metrics.

Combine the code snippets above into the `main.py` file and commit the changes:

```bash
git add .
git commit -m "Add the main.py functionality"
```

### Adding tests

Testing is a critical part of software engineering workflows for several reasons:

1. Reliability: Tests help ensure code behaves correctly and consistently across changes.
2. Regression prevention: Tests catch when new changes break existing functionality.
3. Documentation: Tests serve as executable documentation of expected behavior
4. Design feedback: Writing tests helps identify design issues early
5. Confidence: A good test suite gives confidence when refactoring or adding features

For our system metrics collection script, tests would be valuable to verify:

- The `get_system_metrics()` function returns data in the expected format with valid ranges
- The `save_metrics()` function properly handles file operations and JSON serialization
- Error handling works correctly for various failure scenarios
- The `main()` function orchestrates the workflow as intended

With that said, let's work on the `tests/test_main.py` file:

```python
import json
from datetime import datetime
from pathlib import Path
from main import get_system_metrics, save_metrics
```

The test file we are about to write demonstrates key principles of testing with `pytest`, a popular Python testing framework. Pytest makes it easy to write tests by using simple `assert` statements and providing a rich set of features for test organization and execution. The test functions are automatically discovered by `pytest` when their names start with `test_`, and each function tests a specific aspect of the system's functionality.

```python
def test_get_system_metrics():
    """Test if system metrics are collected correctly"""
    metrics = get_system_metrics()
    
    # Check if all required metrics exist and are valid
    assert 0 <= metrics['cpu_percent'] <= 100
    assert 0 <= metrics['memory_percent'] <= 100
    assert metrics['active_processes'] > 0
    assert 'timestamp' in metrics
```

In this example, we have two test functions that verify different components of our metrics collection system. The first test, `test_get_system_metrics()`, checks if the metrics collection function returns data in the expected format and with valid ranges. It uses multiple assert statements to verify that CPU and memory percentages are between 0 and 100, that there are active processes, and that a timestamp is included. This demonstrates the practice of testing both the structure of returned data and the validity of its values.

```python
def test_save_and_read_metrics():
    """Test if metrics are saved and can be read back"""
    # Get and save metrics
    metrics = get_system_metrics()
    save_metrics(metrics)
    
    # Check if file exists and contains data
    date_str = datetime.now().strftime("%Y-%m-%d")
    file_path = Path("system_metrics") / f"metrics_{date_str}.json"
    
    assert file_path.exists()
    with open(file_path) as f:
        saved_data = json.load(f)
    
    assert isinstance(saved_data, list)
    assert len(saved_data) > 0
```

The second test, `test_save_and_read_metrics()`, showcases integration testing by verifying that metrics can be both saved to and read from a file. It follows a common testing pattern: arrange (setup the test conditions), act (perform the operations being tested), and assert (verify the results). The test ensures that the file is created in the expected location and that the saved data maintains the correct structure. This type of test is particularly valuable as it verifies that different components of the system work together correctly.

Combine the above code snippets and commit the changes to GitHub:

```bash
git add .
git commit -m "Write tests for main.py"
```

### Running your first GitHub Actions workflow

Now that we've created our system monitoring workflow, let's set it up on GitHub and run it. First, push everything we have to a new GitHub repository:

```bash
git remote add origin https://github.com/your-username/your-repository.git
git branch -M main
git push -u origin main
```

Once the workflow file is pushed, GitHub automatically detects it and displays in the "Actions" tab of your repository. The workflow is scheduled so you don't need to do anything - the first workflow run will happen within 30 minutes (remember how we set the running interval to `*/30` with cron). Since the workflow also includes a `workflow_dispatch` field, you can trigger it manually by clicking on the "Run workflow" button.

After clicking the button, a new run appears within a few seconds (refresh if you don't see it). To see the workflow run in real-time, click on it and expand the `monitor` job. You'll see each step executing:

- Checking out repository
- Setting up Python
- Installing dependencies
- Running tests
- Collecting metrics

### Committing changes made by GitHub Actions workflows

Right now, our workflow file has a problem - while it successfully collects metrics, it doesn't commit and push the changes back to the repository. This means that although metrics are being gathered, they aren't being saved in version control. Let's modify the workflow to automatically commit and push the collected metrics:

```yaml
name: System Monitor

on:
  schedule:
    - cron: '*/30 * * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  monitor:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Run tests
        run: python -m pytest
        
      - name: Collect metrics
        run: python main.py
        
      - name: Commit and push changes
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add metrics.json
          git commit -m "Update metrics" || exit 0
          git push
```

The key changes in this updated workflow are:

1. Added `permissions` block with `contents: write` to allow the workflow to push changes back to the repository.
2. Added a new "Commit and push changes" step that:
   - Configures git user identity as `github-actions` bot
   - Stages the `metrics.json` file
   - Creates a commit with message "Update metrics"
   - Pushes the changes back to the repository

The "|| exit 0" after `git commit` ensures the workflow doesn't fail if there are no changes to commit.

This allows the workflow to automatically save and version control the metrics it collects. Let's commit the changes to the workflow and push:

```bash
git add .
git commit -m "Add a commit step to the workflow file"
git push origin main
```

After this change, try running the workflow manually and verify its success by navigating to the Actions tab in your GitHub repository. You should see the workflow run and the `metrics.json` file updated with new data.

## Managing Sensitive Data and Environment Variables

When building automated workflows with GitHub Actions, proper handling of sensitive data like API keys, passwords, and access tokens is crucial for security. Let's explore best practices for managing these credentials.

### Understanding environment variables in GitHub Actions

Environment variables in GitHub Actions can be set at different levels:

- Repository level (GitHub Secrets)
- Workflow level
- Job level
- Step level

Here is how to properly configure and use them:

#### 1. Setting up the repository secrets

First, store sensitive values as repository secrets:

1. Navigate to your GitHub repository
2. Go to Settings ‚Üí Secrets and variables ‚Üí Actions
3. Click "New repository secret"
4. Add your secrets with descriptive names like:

- `API_KEY`
- `DATABASE_URL`
- `AWS_ACCESS_KEY`

#### 2. Using secrets in workflows

Reference secrets in your workflow file using the `secrets` context:

```yaml
name: Web Scraping Pipeline
# ... the rest of the file

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      # ... the rest of the steps
          
      - name: Run scraper
        env:
          API_KEY: ${{ secrets.API_KEY }}
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: python scraper.py
```

Above, the "Run scraper" step executes `scraper.py` which relies on two environment variables configured through the `secrets` context.

#### 3. Local development with .env files

For local development, use `.env` files to manage environment variables:

```bash
touch .env
echo "API_KEY='your-api-key-here'" >> .env
echo "DATABASE_URL='postgresql://user:pass@localhost:5432/db'" >> .env
```

Create a `.gitignore` file to prevent committing sensitive data:

```bash
echo ".env" >> .gitignore
```

#### 4. Loading environment variables in Python

Use `python-dotenv` to load variables from `.env` files:

```python
from dotenv import load_dotenv
import os

# Load environment variables from .env file
load_dotenv()

# Access variables
api_key = os.getenv('API_KEY')
database_url = os.getenv('DATABASE_URL')

if not api_key or not database_url:
    raise ValueError("Missing required environment variables")
```

This code demonstrates loading environment variables from a `.env` file using `python-dotenv`. The `load_dotenv()` function reads the variables, which can then be accessed via `os.getenv()`. Basic validation ensures required variables exist.

#### 5. Environment variable validation

Create a configuration class to validate environment variables:

```python
from pydantic import BaseSettings, SecretStr


class Settings(BaseSettings):
    api_key: SecretStr
    database_url: str
    debug_mode: bool = False

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"


# Initialize settings
settings = Settings()
```

This approach using Pydantic provides several advantages for environment variable management:

1. Type validation - Pydantic automatically validates types and converts values
2. Default values - The `debug_mode` demonstrates setting defaults
3. Secret handling - `SecretStr` provides secure handling of sensitive values
4. Centralized config - All environment variables are defined in one place
5. IDE support - Get autocomplete and type hints when using the settings object

The `Settings` class inherits from `BaseSettings` which automatically loads from environment variables. The `Config` class specifies to also load from a `.env` file.

Using `settings = Settings()` creates a validated configuration object that can be imported and used throughout the application. This is more robust than accessing `os.environ` directly.

Example usage:

```python
settings.api_key.get_secret_value()  # Securely access API key
settings.database_url  # Type-checked database URL
settings.debug_mode  # Boolean with default value
```

#### 6. Handle different environments

Handle different environments (development, staging, production) using environment-specific files:

```bash
.env                # Default environment variables
.env.development   # Development-specific variables
.env.staging       # Staging-specific variables
.env.production    # Production-specific variables
```

Load the appropriate file based on the environment:

```bash
from dotenv import load_dotenv
import os

env = os.getenv('ENVIRONMENT', 'development')
env_file = f'.env.{env}'

load_dotenv(env_file)
```

This approach allows you to maintain separate configurations for different environments while keeping sensitive information secure. The environment-specific files can contain different values for the same variables, such as:

- Development environment may use local services and dummy credentials
- Staging environment may use test services with restricted access
- Production environment contains real credentials and production service endpoints

You can also combine this with the `Pydantic` settings approach shown earlier for robust configuration management across environments.

For example, staging might use a test database while production uses the live database:

```bash
# .env.staging:
DATABASE_URL=postgresql://test-db.example.com
API_KEY=test-key
```

```bash
.env.production:
DATABASE_URL=postgresql://prod-db.example.com 
API_KEY=live-key
```

This separation helps prevent accidental use of production resources during development and testing.

## Building Real-World Python Workflows

Let's explore three practical examples of GitHub Actions workflows for common Python tasks: web scraping, package publishing, and container builds.

### 1. Scheduled web scraping with Firecrawl

Web scraping is a common use case for automated workflows. Let's build a workflow that scrapes [Hacker News](https://news.ycombinator.com/) on a schedule using [Firecrawl](https://docs.firecrawl.dev), which is a Python AI-based web scraping engine designed for large-scale data collection. Here are some key benefits that make Firecrawl an excellent choice for this task:

1. **Enterprise-grade automation and scalability** - Firecrawl streamlines web scraping with powerful automation features.
2. **AI-powered content extraction** - Maintains scraper reliability over time by identifying and extracting data based on semantic descriptions instead of relying HTML elements and CSS selectors.
3. **Handles complex scraping challenges** - Automatically manages proxies, anti-bot mechanisms, and dynamic JavaScript content.
4. **Multiple output formats** - Supports scraping and converting data in markdown, tabular, screenshots, and HTML, making it versatile for various applications.
5. **Built-in rate limiting and request management** - Ensures efficient and compliant data extraction.
6. **Geographic location customization** - Avoids IP bans by customizing the geographic location of requests.

Let's build our web scraping workflow using Firecrawl to demonstrate these capabilities.

```bash
# Create project directory and install dependencies
mkdir hacker-news-scraper && cd hacker-news-scraper
pip install firecrawl-py pydantic python-dotenv

# Create necessary files
touch requirements.txt scraper.py .env

# Add dependencies to requirements.txt
echo "firecrawl-py\npydantic\npython-dotenv" > requirements.txt

# Add Firecrawl API key to .env (get your key at firecrawl.dev/signin/signup)
echo "FIRECRAWL_API_KEY='your_api_key_here'" > .env
```

Open the scraper script where we define our scraping logic:

```python
# scraper.py
import json
from firecrawl import FirecrawlApp
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from typing import List
from datetime import datetime

load_dotenv()

BASE_URL = "https://news.ycombinator.com/"
```

First, we import necessary libraries and packages, also defining a base URL we are going to scrape.

```python
class NewsItem(BaseModel):
    title: str = Field(description="The title of the news item")
    source_url: str = Field(description="The URL of the news item")
    author: str = Field(
        description="The URL of the post author's profile concatenated with the base URL."
    )
    rank: str = Field(description="The rank of the news item")
    upvotes: str = Field(description="The number of upvotes of the news item")
    date: str = Field(description="The date of the news item.")


class NewsData(BaseModel):
    news_items: List[NewsItem]
```

We define two Pydantic models to structure our scraped data:

1. `NewsItem` - Represents a single news item with fields for title, URL, author, rank, upvotes and date
2. `NewsData` - Contains a list of `NewsItem` objects

These models help validate the scraped data and ensure it matches our expected schema. They also make it easier to serialize/deserialize the data when saving to JSON. Using `Field` with a detailed description is crucial because Firecrawl uses these definitions to automatically detect the HTMl elements and CSS selectors we are looking for.

```python
def get_news_data():
    app = FirecrawlApp()

    data = app.scrape_url(
        BASE_URL,
        params={
            "formats": ["extract"],
            "extract": {"schema": NewsData.model_json_schema()},
        },
    )

    return data
```

The `get_news_data()` function uses Firecrawl to scrape Hacker News. It creates a `FirecrawlApp` instance and calls `scrape_url()` with the `BASE_URL` and parameters specifying we want to extract data according to our `NewsData` schema. The schema helps Firecrawl automatically identify and extract the relevant HTML elements. The function returns the scraped data containing news items with their titles, URLs, authors, ranks, upvotes and dates.

```python
def save_firecrawl_news_data():
    """
    Save the scraped news data to a JSON file with the current date in the filename.
    """
    # Get the data
    data = get_news_data()
    # Format current date for filename
    date_str = datetime.now().strftime("%Y_%m_%d_%H_%M")
    filename = f"firecrawl_hacker_news_data_{date_str}.json"

    # Save the news items to JSON file
    with open(filename, "w") as f:
        json.dump(data["extract"]["news_items"], f, indent=4)

    print(f"{datetime.now()}: Successfully saved the news data.")
    
if __name__ == "__main__":
    save_firecrawl_news_data()

```

The `save_firecrawl_news_data()` function handles saving the scraped Hacker News data to a JSON file. It first calls `get_news_data()` to fetch the latest data from Hacker News. Then it generates a filename using the current timestamp to ensure uniqueness. The data is saved to a JSON file with that filename, with the news items formatted with proper indentation for readability. Finally, it prints a confirmation message with the current timestamp when the save is complete. This function provides a convenient way to store snapshots of Hacker News data that can be analyzed later.

Combine these snippets into the `scraper.py` script. Then, we can write a workflow that executes it on schedule:

```bash
cd ..  # Change back to the project root directory
touch .github/workflows/hacker-news-scraper.py  # Create the workflow file
```

Here is what the workflow file must look like:

```yaml
name: Run Hacker News Scraper

permissions:
  contents: write

on:
  schedule:
    - cron: "0 */6 * * *"
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r hacker-news-scraper/requirements.txt
          
      - name: Run scraper
        run: python hacker-news-scraper/scraper.py
        
      - name: Commit and push if changes
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add .
          git commit -m "Update scraped data" -a || exit 0
          git push
```

This workflow runs our Hacker News scraper every 6 hours using GitHub Actions. It sets up Python, installs dependencies, executes the scraper, and automatically commits any new data to the repository. The workflow can also be triggered manually using the `workflow_dispatch` event. One important note about the paths specified in the workflow file is that they must match your repository's directory structure exactly, including the requirements.txt location and the path to your scraper script.

To enable the workflow, simply push all the changes to GitHub and test it through the UI. The next runs will be automatic.

```bash
git add .
git commit -m "Add a scraping workflow"
git push origin main
```

### 2. Package publishing workflow

Publishing Python packages to PyPI (Python Package Index) typically involves several steps. First, developers need to prepare their package by creating a proper directory structure, writing setup files, and ensuring all metadata is correct. Then, the package needs to be built into distribution formats - both source distributions (`sdist`) and wheel distributions (`bdist_wheel`). Finally, these distribution files are uploaded to PyPI using tools like `twine`. This process often requires careful version management and proper credentials for the package repository. While this can be done manually, automating it with CI/CD pipelines like GitHub Actions ensures consistency and reduces human error in the release process.

For example, the following workflow publishes a new version of a package when you create a new release:

```yaml
name: Publish Python Package
on:
  release:
    types: [created]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install build twine
          
      - name: Build package
        run: python -m build
        
      - name: Publish to PyPI
        env:
          TWINE_USERNAME: __token__
          TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}
        run: |
          python -m twine upload dist/*
```

The workflow automates publishing Python packages to PyPI when GitHub releases are created.

Required setup steps:

1. Package must have `setup.py` or `pyproject.toml` configured
2. Create PyPI account at `pypi.org`
3. Generate PyPI API token with upload permissions
4. Store token as `PYPI_API_TOKEN` in repository secrets

The workflow process:

1. Triggers on new GitHub release
2. Checks out code and sets up Python
3. Installs build tools
4. Creates distribution packages
5. Uploads to PyPI using stored token

The `__token__` username is used with PyPI's token authentication, while the actual token is accessed securely through GitHub secrets.

### 3. Container build and push workflow

GitHub Actions can also automate building and pushing Docker containers to container registries like Docker Hub or GitHub Container Registry (GHCR). This workflow is useful for maintaining containerized applications and ensuring your latest code changes are packaged into updated container images.

The process typically involves:

1. Building a Docker image from your `Dockerfile`
2. Tagging the image with version/metadata
3. Authenticating with the container registry
4. Pushing the tagged image to the registry

This automation ensures your container images stay in sync with code changes and are readily available for deployment. Here is a sample workflow containing these steps:

```yaml
name: Build and Push Container
on:
  push:
    branches: [main]
    paths:
      - 'Dockerfile'
      - 'src/**'
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
        
      - name: Login to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
          
      - name: Build and push
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: |
            user/app:latest
            user/app:${{ github.sha }}
```

This workflow introduces a few new GitHub Actions concepts and syntax:

The `paths` trigger filter ensures the workflow only runs when changes are made to the Dockerfile or files in the `src` directory, preventing unnecessary builds.

`docker/setup-buildx-action` configures Docker Buildx, which provides enhanced build capabilities including multi-platform builds and build caching.

`docker/login-action` handles registry authentication. Before using this, you must:

1. [Create a Docker Hub account](https://app.docker.com/signup)
2. Generate an access token in Docker Hub settings
3. Add `DOCKERHUB_USERNAME` and `DOCKERHUB_TOKEN` as repository secrets in GitHub

`docker/build-push-action` is a specialized action for building and pushing Docker images. The configuration shows:

- `context: .` (builds from current directory)
- `push: true` (automatically pushes after building)
- `tags:` specifies multiple tags including:
  - `latest:` rolling tag for most recent version
  - `github.sha:` unique tag using commit hash for versioning

The workflow assumes you have:

- A valid Dockerfile in your repository
- Required application code in `src` directory
- Docker Hub repository permissions
- Properly configured repository secrets

When this workflow runs successfully, it produces a containerized version of your application that is automatically published to Docker Hub and can be pulled with either the latest tag or specific commit hash.

## Understanding How GitHub Actions Run

When you trigger a GitHub Actions run, whether manually or through automated events, the platform:

1. Provisions a fresh virtual machine (runner)
2. Executes your workflow steps sequentially
3. Reports results back to GitHub
4. Tears down the runner environment

This isolated execution model ensures consistency and security for each GitHub Actions run.

## Glossary of GitHub Actions Terms

- **GitHub Actions**: GitHub's built-in automation platform for software development workflows
- **GitHub Actions Workflow**: A configurable automated process made up of one or more jobs
- **GitHub Actions Jobs**: Individual units of work that can run sequentially or in parallel
- **GitHub Actions Run**: A single execution instance of a workflow
- **GitHub Actions Environment Variables**: Configuration values available during workflow execution
- **GitHub Actions Secrets**: Encrypted environment variables for sensitive data
- **GitHub Actions Tutorial**: A guide teaching the fundamentals of GitHub Actions (like this one!)

## Conclusion

Throughout this tutorial, we've explored the fundamentals and practical applications of GitHub Actions for Python development. From understanding core concepts like workflows, jobs, and actions, to implementing real-world examples including automated testing, web scraping, package publishing, and container builds, you've gained hands-on experience with this powerful automation platform. We've also covered critical aspects like managing sensitive data through environment variables and secrets, ensuring your automated workflows are both secure and maintainable.

As you continue your journey with GitHub Actions, remember that automation is an iterative process. Start small with basic workflows, test thoroughly, and gradually add complexity as needed. The examples provided here serve as templates that you can adapt and expand for your specific use cases. For further learning, explore the [GitHub Actions documentation](https://docs.github.com/en/actions), join the [GitHub Community Forum](https://github.community/), and experiment with the vast ecosystem of pre-built actions available in the [GitHub Marketplace](https://github.com/marketplace?type=actions). Whether you're building a personal project or managing enterprise applications, GitHub Actions provides the tools you need to streamline your development workflow and focus on what matters most - writing great code.

If you want to learn more about Firecrawl, the web scraping API we used today, you can read the following posts:

- [Guide to Scheduling Web Scrapers in Python](https://www.firecrawl.dev/blog/automated-web-scraping-free-2025)
- [Mastering Firecrawl's Scrape Endpoint](https://www.firecrawl.dev/blog/mastering-firecrawl-scrape-endpoint)
- [Getting Started With Predicted Outputs in OpenAI](https://www.firecrawl.dev/blog/getting-started-with-predicted-outputs-openai)

Thank you for reading!

================
File: examples/blog-articles/mastering-map-endpoint/mastering-map-endpoint.md
================
---
title: "How to Generate a Sitemap Using Firecrawl's /map Endpoint: A Complete Guide"
meta_description: "Learn how to generate XML and visual sitemaps using Firecrawl's /map endpoint. Step-by-step guide with Python code examples, performance comparisons, and interactive visualization techniques for effective website mapping."
slug: how-to-generate-sitemap-using-firecrawl-map-endpoint
date: 23 Nov, 2024
author: Bex Tuychiev
image: to_fill_in_later
categories: [tutorials]
keywords: [sitemap generator, website mapping, firecrawl, url discovery, site structure analysis, seo optimization, web crawling, website indexing, automated sitemap, site mapping tools, website architecture, sitemap creation, web development tools, website maintenance, seo tools, map endpoint, firecrawl map endpoint, python sitemap generator, visual sitemap creator, website structure visualization, interactive site mapping, xml sitemap generation, website crawling api, sitemap automation tools, website structure analysis tools]
---

## Introduction

In this guide, we'll explore Firecrawl's `/map` endpoint - a powerful tool for automated website mapping and URL discovery. We'll cover what it does, why it matters, and how to use it effectively in your web development workflow.

Website mapping has become increasingly critical in modern web development. As sites grow more complex with dynamic content and single-page applications, having a clear understanding of your site's structure and URL hierarchy is essential for SEO, maintenance, and user experience.

The /map endpoint helps solve common challenges like keeping track of site structure, identifying broken links, and ensuring search engines can properly crawl and index your content. Let's dive into how it works.

## Table of Contents

- [Introduction](#introduction)
- [Understanding Firecrawl's `/map` Endpoint: Features and Benefits](#understanding-firecrawls-map-endpoint-features-and-benefits)
  - [What is Site Mapping and Why is it Essential for Modern Websites?](#what-is-site-mapping-and-why-is-it-essential-for-modern-websites)
  - [Guide to Sitemap Types: Visual vs XML Sitemaps](#guide-to-sitemap-types-visual-vs-xml-sitemaps)
  - [How Firecrawl's `/map` Endpoint Solves These Challenges](#how-firecrawls-map-endpoint-solves-these-challenges)
  - [Limitations of `/map` in the alpha stage](#limitations-of-map-in-the-alpha-stage)
- [Step-by-Step Guide to Using the /map Endpoint](#step-by-step-guide-to-using-the-map-endpoint)
- [Further Configuration Options for Website Mapping](#further-configuration-options-for-website-mapping)
  - [Optimizing URL discovery with `search` parameter](#optimizing-url-discovery-with-search-parameter)
  - [Essential `/map` parameters for customized site mappning](#essential-map-parameters-for-customized-site-mappning)
- [Comparing `/crawl` and `/map`: When to Use Each Endpoint](#comparing-crawl-and-map-when-to-use-each-endpoint)
- [Step-by-Step Guide: Creating XML Sitemaps with `/map`](#step-by-step-guide-creating-xml-sitemaps-with-map)
- [Advanced Visualization: Building Interactive Visual Sitemaps with `/map`](#advanced-visualization-building-interactive-visual-sitemaps-with-map)
- [Conclusion](#conclusion)

## Understanding Firecrawl's `/map` Endpoint: Features and Benefits

To understand what the `/map` endpoint does, let's briefly cover what's __site mapping__ and why it is important.

### What is Site Mapping and Why is it Essential for Modern Websites?

Put simply, a sitemap is a list or a diagram that communicates the structure of web pages in a website. It is useful for a number of reasons.

First, it helps developers and site owners understand and maintain their website's structure. Having a clear overview of how pages are connected makes it easier to manage content, identify navigation issues, and ensure a logical flow for users.

Second, sitemaps are crucial for SEO. Search engines use sitemaps to discover and index pages more efficiently. A well-structured sitemap helps ensure all your important content gets crawled and indexed properly.

Third, sitemaps can help identify potential issues like broken links, orphaned pages (pages with no incoming links), or circular references. This makes troubleshooting and maintenance much more manageable.

Finally, sitemaps are valuable for planning site improvements and expansions. They provide a bird's-eye view that helps in making strategic decisions about content organization and information architecture.

### Guide to Sitemap Types: Visual vs XML Sitemaps

There are two main types of sitemaps: visual and XML.

Visual sitemaps are diagrams or flowcharts that show how websites are structured at-a-glance. They typically use boxes, lines, and other visual elements to represent pages and their relationships. These visual representations make it easy for stakeholders, designers, and developers to quickly understand site hierarchy, navigation paths, and content organization. They're particularly useful during the planning and design phases of web development, as well as for communicating site structure to non-technical team members.

![Visual sitemap example showing hierarchical website structure with connected pages and sections for better site organization and planning](https://cdn.prod.website-files.com/649ae86ac1a4be707b656519/65a84b0bfe7e308f557dce75_Generate%20a%20sitemap%20online.webp)

Source: [Flowapp](https://www.flowmapp.com/features/generate-sitemap-online)

XML sitemaps are shown to the public much less frequently because they contain structured XML code that can look intimidating to non-technical users. But an XML sitemap is just an organized file containing all the URLs of a website that is readable to search engines. It includes important metadata about each URL like when it was last modified, how often it changes, and its relative importance. Search engines like Google use this information to crawl websites more intelligently and ensure all important pages are indexed. While XML sitemaps aren't meant for human consumption, they play a vital role in SEO and are often required for large websites to achieve optimal search engine visibility.

![Example of an XML sitemap showing structured URL data with lastmod, changefreq and priority tags for search engine optimization](notebook_files/image.png)

Source: [DataCamp](https://www.datacamp.com/sitemap/es/tutorial/category.xml)

### How Firecrawl's `/map` Endpoint Solves These Challenges

When you are building a website from scratch, you usually need a visual sitemap and can develop the XML one over the time as you add more pages. However, if you neglected these steps early on and suddenly find yourself with a massive website, possibly with thousands of URLs, creating either type of sitemap manually becomes an overwhelming task. This is where automated solutions like the `/map` endpoint can become invaluable.

The real challenge of mapping existing sites is finding all the URLs that exist on your website. Without automated tools, you'd need to manually click through every link, record every URL, and track which pages link to which others. Traditional web scraping solutions using Python libraries like `beautifulsoup`, `scrapy` or `lxml` can automate this process but they can quickly become useless when dealing with modern web applications that heavily rely on JavaScript for rendering content, use complex authentication systems, or implement rate limiting and bot detection.

These traditional approaches are not only time-consuming but also error-prone, as it's easy to miss URLs in JavaScript-rendered content, dynamically generated pages, or deeply nested navigation menus.

The `/map` endpoint solves these challenges and provides the fastest and easiest solution to go from a single URL to a map of the entire website. The /map endpoint is particularly useful in scenarios where:

- You want to give end-users control over which links to scrape by presenting them with options
- Rapid discovery of all available links on a website is crucial
- You need to focus on topic-specific content, you can use the search parameter to find relevant pages
- You only want to extract data from particular sections of a website rather than crawling everything

### Limitations of `/map` in the alpha stage

While the /map endpoint is still in alpha stage, it has some limitations. The endpoint prioritizes speed so it may not capture all website links. The vision for this endpoint is to maintain its blazing-fast speed and still capture every single link in a given website. Feedback and suggestions are welcome.

## Step-by-Step Guide to Using the /map Endpoint

Firecrawl is a scraping engine exposed as a REST API, which means you can use it from the command-line using cURL or by using one of its SDKs in Python, Node, Go or Rust. In this tutorial, we will use its Python SDK, so please install it in your environment:

```bash
pip install firecrawl-py
```

The next step is obtaining a Firecrawl API key by signing up at [firecrawl.dev](firecrawl.dev) and choosing a plan (the free plan is fine for this tutorial).

Once you have your API key, you should save it in a .env file, which provides a secure way to store sensitive credentials without exposing them in your code:

```bash
touch .env
echo "FIRECRAWL_API_KEY='YOUR-API-KEY'" >> .env
```

Then, you should install python-dotenv to automatically load the variables in `.env` files in Python scripts and notebooks:

```bash
pip install python-dotenv
```

Then, using the /map endpoint is as easy as the following code:

```python
from firecrawl import FirecrawlApp
from dotenv import load_dotenv; load_dotenv()

app = FirecrawlApp()

response = app.map_url(url="https://firecrawl.dev")
```

In this code snippet, we're using the Firecrawl Python SDK to map a URL. Let's break down what's happening:

First, we import two key components:

- FirecrawlApp from the firecrawl package, which provides the main interface to interact with Firecrawl's API
- `load_dotenv` from `dotenv` to load our environment variables containing the API key

After importing, we initialize a FirecrawlApp instance, which automatically picks up our API key from the environment variables.

Finally, we make a request to map the URL `https://firecrawl.dev` using the`map_url()` method. This crawls the website and returns information about its structure and pages, taking about two seconds on my machine (the speed may vary based on internet speeds).

Let's look at the `response` dictionary:

```python
response.keys()
```

```python
dict_keys(['success', 'links'])
```

It only has two keys: 'success' and 'links'. The 'success' key indicates whether the request was successful, and the 'links' key contains the URLs found on the website:

```python
len(response['links'])
```

```python
98
```

## Further Configuration Options for Website Mapping

### Optimizing URL discovery with `search` parameter

The most notable feature of the endpoint is its `search` parameter. This parameter allows you to filter the URLs returned by the crawler based on specific patterns or criteria. For example, you can use it to only retrieve URLs containing certain keywords or matching specific paths. This makes it incredibly useful for focused crawling tasks where you're only interested in a subset of pages for massive websites.

Let's use this feature on the Stripe documentation and only search for pages related to taxes:

```python
url = "https://docs.stripe.com"

response = app.map_url(url=url, params={"search": "tax"})
```

The response structure will be the same:

```python
response["links"][:10]
```

```python
    ['https://docs.stripe.com/tax',
     'https://docs.stripe.com/tax/how-tax-works',
     'https://docs.stripe.com/tax/reports',
     'https://docs.stripe.com/tax/calculating',
     'https://docs.stripe.com/api/tax_rates',
     'https://docs.stripe.com/tax/tax-codes',
     'https://docs.stripe.com/tax/zero-tax',
     'https://docs.stripe.com/tax/products-prices-tax-codes-tax-behavior',
     'https://docs.stripe.com/payments/checkout/taxes',
     'https://docs.stripe.com/billing/taxes/tax-rates']
```

Let's count up the found links:

```python
len(response["links"])
```

```out
2677
```

More than 2600 in only three seconds!

### Essential `/map` parameters for customized site mappning

There are some additional parameters /map accepts to control its behavior:

__`ignoreSitemap`__

- Type: boolean
- Default: true
- Description: When set to true, the crawler will not attempt to parse or use the website's `sitemap.xml` file during crawling. This can be useful when you want to discover pages through navigation links only.

__`sitemapOnly`__

- Type: boolean
- Default: false
- Description: When enabled, the crawler will exclusively return URLs found in the website's sitemap files, ignoring any links discovered through page crawling. This is useful for quickly indexing officially published pages.

__`includeSubdomains`__

- Type: boolean
- Default: false
- Description: Controls whether the crawler should follow and return links to subdomains (e.g., blog.example.com when crawling example.com). Enabling this provides a more comprehensive view of the entire web property.

__`limit`__

- Type: integer
- Default: 5000
- Description: Specifies the maximum number of URLs the crawler will return in a single request. This helps manage response sizes and processing time. Must be less than 5000 to prevent excessive server load for the time being.
- Required range: x < 5000

Let's try running the Stripe example by including some of these parameters, like the `sitemapOnly` and `includeSubdomains` options set to True:

```python
url = "https://docs.stripe.com"

response = app.map_url(url=url, params={"search": "tax", "sitemapOnly": True, "includeSubdomains": True})
len(response['links'])
```

```python
2712
```

This time, the link count increased.

## Comparing `/crawl` and `/map`: When to Use Each Endpoint

If you read our [separate guide on the `/crawl` endpoint](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl) of Firecrawl, you may notice one similarity between it and the `/map` endpoint:

If you set the response format of crawl to "links", you will also get a list of URLs found on the website. While the purpose is the same, there are huge differences performance-wise.

First, the `/crawl` endpoint is painfully slow for URL discovery, as evidenced by the execution times in the examples below:

```python
%%time

url = "books.toscrape.com"

crawl_response = app.crawl_url(url=url, params={"scrapeOptions": {"formats": ["links"]}})
```

```text
CPU times: user 843 ms, sys: 470 ms, total: 1.31 s
Wall time: 2min 9s
```

```python
%%time

url = "books.toscrape.com"

map_response = app.map_url(url=url)
```

```out
CPU times: user 4.91 ms, sys: 3.58 ms, total: 8.49 ms
Wall time: 2.04 s
```

![Comparison chart showing significant performance difference between map and crawl endpoints of Firecrawl with map being much faster](notebook_files/notebook_42_0.png)

This is because `/crawl` needs to fully load and parse each page's HTML content, even when we only want the links. In contrast, `/map` is optimized specifically for URL discovery, making it much faster for generating sitemaps and link analysis.

But, since `/map` is alpha, it doesn't capture as many links as `/crawl`:

```python
crawl_links = set()

for page in crawl_response['data']:
    crawl_links.update(page["links"])

len(crawl_links)
```

```python
1216
```

```python
len(map_response["links"])
```

```python
298
```

In fact, `/map` found three times less links than `/crawl`.

However, due to its speed, `/map` can still provide a good foundation for sitemap generation and its accuracy will increase as it progresses towards being stable.

## Step-by-Step Guide: Creating XML Sitemaps with `/map`

Now, let's see how to convert the links found with `/map` to an XML sitemap in Python. We will need to import the following packages:

```python
from datetime import datetime
import xml.etree.ElementTree as ET
from urllib.parse import urlparse
```

We'll use:

- `datetime`: To add timestamps to our sitemap entries
- `xml.etree.ElementTree`: To create and structure the XML sitemap file
- `urllib.parse`: To parse and validate URLs before adding them to the sitemap

Let's start by defining a new function - `create_xml_sitemap`:

```python
def create_xml_sitemap(urls, base_url):
    # Create the root element
    urlset = ET.Element("urlset")
    urlset.set("xmlns", "http://www.sitemaps.org/schemas/sitemap/0.9")
```

In the body of the function, we first create the root XML element named "urlset" using `ET.Element()`. Then we set its `xmlns` attribute to the sitemap schema URL `http://www.sitemaps.org/schemas/sitemap/0.9` to identify this as a valid sitemap XML document.

Then, we get the current date for providing a last modified date (since `/map` doesn't return the modified dates of pages):

```python
def create_xml_sitemap(urls, base_url):
    # Create the root element
    ...

    # Get current date for lastmod
    today = datetime.now().strftime("%Y-%m-%d")
```

Then, we add each URL to the sitemap:

```python
def create_xml_sitemap(urls, base_url):
    ...

    # Add each URL to the sitemap
    for url in urls:
        # Only include URLs from the same domain
        if urlparse(url).netloc == urlparse(base_url).netloc:
            url_element = ET.SubElement(urlset, "url")
            loc = ET.SubElement(url_element, "loc")
            loc.text = url

            # Add optional elements
            lastmod = ET.SubElement(url_element, "lastmod")
            lastmod.text = today

            changefreq = ET.SubElement(url_element, "changefreq")
            changefreq.text = "monthly"

            priority = ET.SubElement(url_element, "priority")
            priority.text = "0.5"
```

The loop iterates through each URL in the provided list and adds it to the sitemap XML structure. For each URL, it first checks if the domain matches the base URL's domain to ensure we only include URLs from the same website. If it matches, it creates a new `<url>` element and adds several child elements:

- `<loc>`: Contains the actual URL
- `<lastmod>`: Set to today's date to indicate when the page was last modified
- `<changefreq>`: Set to "monthly" to suggest how often the page content changes
- `<priority>`: Set to "0.5" to indicate the relative importance of the page

This creates a properly formatted sitemap entry for each URL following the Sitemap XML protocol specifications.

After the loop finishes, we create and return the XML string:

```python
def create_xml_sitemap(urls, base_url):
    ...

    # Add each URL to the sitemap
    for url in urls:
        ...

    # Create the XML string
    return ET.tostring(urlset, encoding="unicode", method="xml")
```

Here is the full function:

```python
def create_xml_sitemap(urls, base_url):
    # Create the root element
    urlset = ET.Element("urlset")
    urlset.set("xmlns", "http://www.sitemaps.org/schemas/sitemap/0.9")

    # Get current date for lastmod
    today = datetime.now().strftime("%Y-%m-%d")

    # Add each URL to the sitemap
    for url in urls:
        # Only include URLs from the same domain
        if urlparse(url).netloc == urlparse(base_url).netloc:
            url_element = ET.SubElement(urlset, "url")
            loc = ET.SubElement(url_element, "loc")
            loc.text = url

            # Add optional elements
            lastmod = ET.SubElement(url_element, "lastmod")
            lastmod.text = today

            changefreq = ET.SubElement(url_element, "changefreq")
            changefreq.text = "monthly"

            priority = ET.SubElement(url_element, "priority")
            priority.text = "0.5"

    # Create the XML string
    return ET.tostring(urlset, encoding="unicode", method="xml")
```

Let's use the function on the links returned by the last `/map` endpoint use:

```python
base_url = "https://books.toscrape.com"
links = map_response["links"]

xml_sitemap = create_xml_sitemap(links, base_url)

# Save to file
with open("sitemap.xml", "w", encoding="utf-8") as f:
    f.write('<?xml version="1.0" encoding="UTF-8"?>\n')
    f.write(xml_sitemap)
```

Here is what the file looks like after saving:

![XML sitemap example showing structured website data with URLs, last modified dates, change frequencies and priorities, generated using Firecrawl](notebook_files/image.png)

Such `sitemap.xml` file provides a standardized way for search engines to discover and crawl all pages on your website.

## Advanced Visualization: Building Interactive Visual Sitemaps with `/map`

If you want a visual sitemap of a website, you don't have to sign up for expensive third-party services and platforms. You can automatically generate one using the `/map` endpoint, Plotly and a few other libraries.

The resulting graph would like the following:

![Interactive Sankey diagram visualization showing hierarchical structure of Stripe documentation with color-coded sections and flow widths representing page counts, generated using Firecrawl's /map endpoint](images/stripe_docs.png)

The Sankey diagram above visualizes the hierarchical structure of [the Stripe documentation](docs.stripe.org) (which is quite large) by showing how pages are organized and connected across different sections. The width of each flow represents the number of pages in that section, making it easy to identify which parts of the website contain the most content. The colors help distinguish between different sections and their subsections.

The diagram starts from a central root node and branches out into main sections of the website. Each section can then split further into subsections, creating a tree-like visualization of the site's architecture. This makes it simple to understand the overall organization and identify potential navigation or structural issues.

For example, you can quickly spot which sections are the largest (the API section), how content is distributed across different areas, and whether there's a logical grouping of related pages. This visualization is particularly useful for content strategists, SEO specialists, and web architects who need to analyze and optimize website structure.

[The script that generated this plot](https://github.com/FirstClassML/firecrawl_articles/blob/main/3_generating_sitemap/sitemap_generator.py) contains more than 400 lines of code and I made it fully customizable. The code in `sitemap_generator.py` follows a modular, object-oriented approach with several key components:

1. A `HierarchyBuilder` class that analyzes URLs returned my `/map` or `/crawl` and builds a tree-like data structure up to 4 levels deep.
2. A `SankeyDataPreparator` class that transforms this hierarchy into a format suitable for visualization, using thresholds to control complexity
3. A `SitemapVisualizer` class that creates the final Sankey diagram with proper styling and interactivity

The script automatically handles things like grouping smaller sections together, truncating long labels, generating color schemes, and adding hover information (the generated plots are all interactive through Plotly). All aspects like minimum branch size, relative thresholds, label length, and color schemes can be customized through parameters.

Here is another plot generated for the [PyData.org](https://pydata.org) website:

![Interactive Sankey diagram showing hierarchical website structure of PyData.org with color-coded sections, flow visualization and navigation paths for improved site architecture understanding, generated using Firecrawl's map endpoint and Plotly](images/pydata.png)

## Conclusion

The `/map` endpoint represents a powerful tool in the modern web developer's toolkit, offering a fast and efficient way to discover and analyze website structures. While still in alpha, it provides significant advantages:

- __Speed__: As demonstrated, it's significantly faster than traditional crawling methods, making it ideal for quick site analysis
- __Flexibility__: With parameters like `search`, `sitemapOnly`, and `includeSubdomains`, it can be tailored to specific needs
- __Practical Applications__: From generating XML sitemaps for SEO to creating visual site hierarchies, the endpoint serves multiple use cases

While it may not capture every single URL compared to full crawling solutions, its speed and ease of use make it an excellent choice for rapid site mapping and initial structure analysis. As the endpoint continues to evolve, its combination of performance and accuracy will make it an increasingly valuable tool for website maintenance, SEO optimization, and content strategy.

To discover what more Firecrawl has to offer, be sure to read the following related resources:

- [Firecrawl Documentation](docs.firecrawl.dev)
- [Firecrawl Blog](https://www.firecrawl.dev/blog/category/tutorials)
- [Firecrawl API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## Frequently Asked Questions

### How fast is Firecrawl's `/map` endpoint?

The /map endpoint typically processes websites in 2-3 seconds, compared to several minutes with traditional crawling methods.

### Can I use `/map` endpoint for large websites?

Yes, the `/map` endpoint can handle large websites with a current limit of 5000 URLs per request, making it suitable for most medium to large websites.

### What's the difference between XML and visual sitemaps?

XML sitemaps are machine-readable files used by search engines for indexing, while visual sitemaps provide a graphical representation of website structure for human understanding and planning.

================
File: examples/blog-articles/mastering-scrape-endpoint/mastering-scrape-endpoint.md
================
---
title: "How to Use Firecrawl's Scrape API: Complete Web Scraping Tutorial"
meta_description: Learn how to scrape websites using Firecrawl's /scrape endpoint. Master JavaScript rendering, structured data extraction, and batch operations with Python code examples.
slug: mastering-firecrawl-scrape-endpoint
date: 22 Nov, 2024
author: Bex Tuychiev
image: to_fill_in_later
categories: [tutorials]
keywords: ["firecrawl", "web scraping", "scrape endpoint", "data extraction", "javascript rendering", "structured data", "web automation", "python sdk", "api", "web crawling", "data collection", "web data", "web scraping tutorial", "python"]
---

## Getting Started with Modern Web Scraping: An Introduction

Traditional web scraping offers unique challenges. Relevant information is often scattered across multiple pages containing complex elements like code blocks, iframes, and media. JavaScript-heavy websites and authentication requirements add additional complexity to the scraping process.

Even after successfully scraping, the content requires specific formatting to be useful for downstream processes like data engineering or training AI and machine learning models.

Firecrawl addresses these challenges by providing a specialized scraping solution. Its [`/scrape` endpoint](https://docs.firecrawl.dev/features/scrape) offers features like JavaScript rendering, automatic content extraction, bypassing blockers and flexible output formats that make it easier to collect high-quality information and training data at scale.

In this guide, we'll explore how to effectively use Firecrawl's `/scrape` endpoint to extract structured data from static and dynamic websites. We'll start with basic scraping setup and then dive into a real-world example of scraping weather data from weather.com, demonstrating how to handle JavaScript-based interactions, extract structured data using schemas, and capture screenshots during the scraping process.

## Table of Contents

- [Getting Started with Modern Web Scraping: An Introduction](#getting-started-with-modern-web-scraping-an-introduction)
- [What Is Firecrawl's `/scrape` Endpoint? The Short Answer](#what-is-firecrawls-scrape-endpoint-the-short-answer)
- [Prerequisites: Setting Up Firecrawl](#prerequisites-setting-up-firecrawl)
- [Basic Scraping Setup](#basic-scraping-setup)
- [Large-scale Scraping With Batch Operations](#large-scale-scraping-with-batch-operations)
  - [Batch Scraping with `batch_scrape_urls`](#batch-scraping-with-batch_scrape_urls)
  - [Asynchronous batch scraping with `async_batch_scrape_urls`](#asynchronous-batch-scraping-with-async_batch_scrape_urls)
- [How to Scrape Dynamic JavaScript Websites](#how-to-scrape-dynamic-javascript-websites)
- [Conclusion](#conclusion)

## What Is Firecrawl's `/scrape` Endpoint? The Short Answer

The `/scrape` endpoint is Firecrawl's core web scraping API that enables automated extraction of content from any webpage. It handles common web scraping challenges like:

- JavaScript rendering - Executes JavaScript to capture dynamically loaded content
- Content extraction - Automatically identifies and extracts main content while filtering out noise
- Format conversion - Converts HTML to clean formats like Markdown or structured JSON
- Screenshot capture - Takes full or partial page screenshots during scraping
- Browser automation - Supports clicking, typing and other browser interactions
- Anti-bot bypass - Uses rotating proxies and browser fingerprinting to avoid blocks

The endpoint accepts a URL and configuration parameters, then returns the scraped content in your desired format. It's designed to be flexible enough for both simple static page scraping and complex dynamic site automation.

Now that we understand what the endpoint does at a high level, let's look at how to set it up and start using it in practice.

## Prerequisites: Setting Up Firecrawl

Firecrawl's scraping engine is exposed as a REST API, so you can use command-line tools like cURL to use it. However, for a more comfortable experience, better flexibility and control, I recommend using one of its SDKs for Python, Node, Rust or Go. This tutorial will focus on the Python version.

To get started, please make sure to:

1. Sign up at [firecrawl.dev](firecrawl.dev).
2. Choose a plan (the free one will work fine for this tutorial).

Once you sign up, you will be given an API token which you can copy from your [dashboard](https://www.firecrawl.dev/app). The best way to save your key is by using a `.env` file, ideal for the purposes of this article:

```bash
touch .env
echo "FIRECRAWL_API_KEY='YOUR_API_KEY'" >> .env
```

Now, let's install Firecrawl Python SDK, `python-dotenv` to read `.env` files, and Pandas for data analysis later:

```bash
pip install firecrawl-py python-dotenv pandas
```

## Basic Scraping Setup

Scraping with Firecrawl starts by creating an instance of the `FirecrawlApp` class:

```python
from firecrawl import FirecrawlApp
from dotenv import load_dotenv

load_dotenv()

app = FirecrawlApp()
```

When you use the `load_dotenv()` function, the app can automatically use your loaded API key to establish a connection with the scraping engine. Then, scraping any URL takes a single line of code:

```python
url = "https://arxiv.org"
data = app.scrape_url(url)
```

Let's take a look at the response format returned by `scrape_url` method:

```python
data['metadata']
```

{
    "title": "arXiv.org e-Print archiveopen searchopen navigation menucontact arXivsubscribe to arXiv mailings",
    "language": "en",
    "ogLocaleAlternate": [],
    "viewport": "width=device-width, initial-scale=1",
    "msapplication-TileColor": "#da532c",
    "theme-color": "#ffffff",
    "sourceURL": "<https://arxiv.org>",
    "url": "<https://arxiv.org/>",
    "statusCode": 200
}

The response `metadata` includes basic information like the page title, viewport settings and a status code.

Now, let's look at the scraped contents, which is converted into `markdown` by default:

```python
from IPython.display import Markdown

Markdown(data['markdown'][:500])

```

```text
arXiv is a free distribution service and an open-access archive for nearly 2.4 million
scholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.
Materials on this site are not peer-reviewed by arXiv.

Subject search and browse:

Physics

Mathematics

Quantitative Biology

Computer Science

Quantitative Finance

Statistics

Electrical Engineering and Systems Scienc
```

The response can include several other formats that we can request when scraping a URL. Let's try requesting multiple formats at once to see what additional data we can get back:

```python
data = app.scrape_url(
    url, 
    params={
        'formats': [
            'html', 
            'rawHtml', 
            'links', 
            'screenshot',
        ]
    }
)
```

Here is what these formats scrape:

- **HTML**: The raw HTML content of the page.
- **rawHtml**: The unprocessed HTML content, exactly as it appears on the page.
- **links**: A list of all the hyperlinks found on the page.
- **screenshot**: An image capture of the page as it appears in a browser.

The HTML format is useful for developers who need to analyze or manipulate the raw structure of a webpage. The `rawHtml` format is ideal for cases where the exact original HTML content is required, such as for archival purposes or detailed comparison. The links format is beneficial for SEO specialists and web crawlers who need to extract and analyze all hyperlinks on a page. The screenshot format is perfect for visual documentation, quality assurance, and capturing the appearance of a webpage at a specific point in time.

Passing more than one scraping format to `params` adds additional keys to the response:

```python
data.keys()
```

```text
dict_keys(['rawHtml', 'screenshot', 'metadata', 'html', 'links'])
```

Let's display the screenshot Firecrawl took of arXiv.org:

```python
from IPython.display import Image

Image(data['screenshot'])
```

![Screenshot of arXiv.org homepage that was taken with Firecrawl's screenshot feature showing research paper categories like Computer Science, Mathematics, Physics and other scientific disciplines](notebook_files/notebook_20_0.png)

Notice how the screenshot is cropped to fit a certain viewport. For most pages, it is better to capture the entire screen by using the `screenshot@fullPage` format:

```python
data = app.scrape_url(
    url,
    params={
        "formats": [
            "screenshot@fullPage",
        ]
    }
)

Image(data['screenshot'])
```

![Full page screenshot of arXiv.org homepage taken with Firecrawl's full-page screenshot capture feature showing research paper categories, search functionality, and recent submissions in an academic layout](notebook_files/notebook_22_0.png)

As a bonus, the `/scrape` endpoint can handle PDF links as well:

```python
pdf_link = "https://arxiv.org/pdf/2411.09833.pdf"
data = app.scrape_url(pdf_link)

Markdown(data['markdown'][:500])
```

```text
arXiv:2411.09833v1 \[math.DG\] 14 Nov 2024
EINSTEIN METRICS ON THE FULL FLAG F(N).
MIKHAIL R. GUZMAN
Abstract.LetM=G/Kbe a full flag manifold. In this work, we investigate theG-
stability of Einstein metrics onMand analyze their stability types, including coindices,
for several cases. We specifically focus onF(n) = SU(n)/T, emphasizingn= 5, where
we identify four new Einstein metrics in addition to known ones. Stability data, including
coindex and Hessian spectrum, confirms that these metrics on
```

### Further Scrape Configuration Options

By default, `scrape_url` converts everything it sees on a webpage to one of the specified formats. To control this behavior, Firecrawl offers the following parameters:

- `onlyMainContent`
- `includeTags`
- `excludeTags`

`onlyMainContent` excludes the navigation, footers, headers, etc. and is set to True by default.

`includeTags` and `excludeTags` can be used to whitelist/blacklist certain HTML elements:

```python
url = "https://arxiv.org"

data = app.scrape_url(url, params={"includeTags": ["p"], "excludeTags": ["span"]})

Markdown(data['markdown'][:1000])
```

```markdown
[Help](https://info.arxiv.org/help) \| [Advanced Search](https://arxiv.org/search/advanced)

arXiv is a free distribution service and an open-access archive for nearly 2.4 million
scholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.
Materials on this site are not peer-reviewed by arXiv.


[arXiv Operational Status](https://status.arxiv.org)

Get status notifications via
[email](https://subscribe.sorryapp.com/24846f03/email/new)
or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)
```

`includeTags` and `excludeTags` also support referring to HTML elements by their `#id` or `.class-name`.

These configuration options help ensure efficient and precise scraping. While `onlyMainContent` filters out peripheral elements, `includeTags` and `excludeTags` enable surgical targeting of specific HTML elements - particularly valuable when dealing with complex webpage structures or when only certain content types are needed.

## Advanced Data Extraction: Structured Techniques

Scraping clean, LLM-ready data is the core philosophy of Firecrawl. However, certain web pages with their complex structures can interfere with this philosophy when scraped in their entirety. For this reason, Firecrawl offers two scraping methods for better structured outputs:

1. Natural language extraction - Use prompts to extract specific information and have an LLM structure the response
2. Manual structured data extraction - Define JSON schemas to have an LLM scrape data in a predefined format

In this section, we will cover both methods.

### Natural Language Extraction - Use AI to Extract Data

To illustrate natural language scraping, let's try extracting all news article links that may be related to the 2024 US presidential election from the New York Times:

```python
url = "https://nytimes.com"

data = app.scrape_url(
    url,
    params={
        'formats': ['markdown', 'extract', 'screenshot'],
        'extract': {
            'prompt': "Return a list of links of news articles that may be about the 2024 US presidential election"
        }
    }
)
```

To enable this feature, you are required to pass the `extract` option to the list of `formats` and provide a prompt in a dictionary to a separate `extract` field.

Once scraping finishes, the response will include a new `extract` key:

```python
data['extract']
```

```python
    {'news_articles': [{'title': 'Harris Loss Has Democrats Fighting Over How to Talk About Transgender Rights',
       'link': 'https://www.nytimes.com/2024/11/20/us/politics/presidential-campaign-transgender-rights.html'},
      {'title': 'As Democrats Question How to Win Back Latinos, Ruben Gallego Offers Answers',
       'link': 'https://www.nytimes.com/2024/11/20/us/politics/ruben-gallego-arizona-latino-voters-democrats.html'},
      ...
      {'title': 'The Final Push for Ukraine?',
       'link': 'https://www.nytimes.com/2024/11/20/briefing/ukraine-russia-trump.html'}]}
```

Due to the nature of this scraping method, the returned output can have arbitrary structure as we can see above. It seems the above output has the following format:

```python
{
    "news_articles": [
        {"title": "article_title", "link": "article_url"},
        ...
    ]
}
```

This LLM-based extraction can have endless applications, from extracting specific data points from complex websites to analyzing sentiment across multiple news sources to gathering structured information from unstructured web content.

To improve the accuracy of the extraction and give additional instructions, you have the option to include a system prompt to the underlying LLM:

```python
data = app.scrape_url(
    url,
    params={
        'formats': ['markdown', 'extract'],
        'extract': {
            'prompt': "Find any mentions of specific dollar amounts or financial figures and return them with their context and article link.",
            'systemPrompt': "You are a helpful assistant that extracts numerical financial data."
        }
    }
)
```

Above, we are dictating that the LLM must act as an assistant that extracts numerical financial data. Let's look at its response:

```python
data['extract']
```

```python
    {'financial_data': [
        {
            'amount': 121200000,
            'context': 'Ren√© Magritte became the 16th artist whose work broke the nine-figure '
                      'threshold at auction when his painting sold for $121.2 million.',
            'article_link': 'https://www.nytimes.com/2024/11/19/arts/design/magritte-surrealism-christies-auction.html'
        },
        {
            'amount': 5000000,
            'context': 'Benjamin Netanyahu offers $5 million for each hostage freed in Gaza.',
            'article_link': 'https://www.nytimes.com/2024/11/19/world/middleeast/israel-5-million-dollars-hostage.html'
        }
    ]}
```

The output shows the LLM successfully extracted two financial data points from the articles.

The LLM not only identified the specific amounts but also provided relevant context and source article links for each figure.

### Schema-Based Data Extraction - Building Structured Models

While natural language scraping is powerful for exploration and prototyping, production systems typically require more structured and deterministic approaches. LLM responses can vary between runs of the same prompt, making the output format inconsistent and difficult to reliably parse in automated workflows.

For this reason, Firecrawl allows you to pass a predefined schema to guide the LLM's output when transforming the scraped content. To facilitate this feature, Firecrawl uses Pydantic models.

In the example below, we will extract only news article links, their titles with some additional details from the New York Times:

```python
from pydantic import BaseModel, Field

class IndividualArticle(BaseModel):
    title: str = Field(description="The title of the news article")
    subtitle: str = Field(description="The subtitle of the news article")
    url: str = Field(description="The URL of the news article")
    author: str = Field(description="The author of the news article")
    date: str = Field(description="The date the news article was published")
    read_duration: int = Field(description="The estimated time it takes to read the news article")
    topics: list[str] = Field(description="A list of topics the news article is about")

class NewsArticlesSchema(BaseModel):
    news_articles: list[IndividualArticle] = Field(
        description="A list of news articles extracted from the page"
    )
```

Above, we define a Pydantic schema that specifies the structure of the data we want to extract. The schema consists of two models:

`IndividualArticle` defines the structure for individual news articles with fields for:

- `title`
- `subtitle`
- `url`
- `author`
- `date`
- `read_duration`
- `topics`

`NewsArticlesSchema` acts as a container model that holds a list of `IndividualArticle` objects, representing multiple articles extracted from the page. If we don't use this container model, Firecrawl will only return the first news article it finds.

Each model field uses Pydantic's `Field` class to provide descriptions that help guide the LLM in correctly identifying and extracting the requested data. This structured approach ensures consistent output formatting.

The next step is passing this schema to the `extract` parameter of `scrape_url`:

```python
url = "https://nytimes.com"

structured_data = app.scrape_url(
    url,
    params={
        "formats": ["extract", "screenshot"],
        "extract": {
            "schema": NewsArticlesSchema.model_json_schema(),
            "prompt": "Extract the following data from the NY Times homepage: news article title, url, author, date, read_duration for all news articles",
            "systemPrompt": "You are a helpful assistant that extracts news article data from NY Times.",
        },
    },
)
```

While passing the schema, we call its `model_json_schema()` method to automatically convert it to valid JSON. Let's look at the output:

```python
structured_data['extract']
```

```python
{
    'news_articles': [
        {
            'title': 'How Google Spent 15 Years Creating a Culture of Concealment',
            'subtitle': '',
            'url': 'https://www.nytimes.com/2024/11/20/technology/google-antitrust-employee-messages.html',
            'author': 'David Streitfeld',
            'date': '2024-11-20',
            'read_duration': 9,
            'topics': []
        },
        # ... additional articles ...
        {
            'title': 'The Reintroduction of Daniel Craig',
            'subtitle': '',
            'url': 'https://www.nytimes.com/2024/11/20/movies/daniel-craig-queer.html',
            'author': '',
            'date': '2024-11-20',
            'read_duration': 9,
            'topics': []
        }
    ]
}
```

This time, the response fields exactly match the fields we set during schema definition:

```python
{
    "news_articles": [
        {...}, # Article 1
        {...}, # Article 2,
        ... # Article n
    ]
}
```

When creating the scraping schema, the following best practices can go a long way in ensuring reliable and accurate data extraction:

1. Keep field names simple and descriptive
2. Use clear field descriptions that guide the LLM
3. Break complex data into smaller, focused fields
4. Include validation rules where possible
5. Consider making optional fields that may not always be present
6. Test the schema with a variety of content examples
7. Iterate and refine based on extraction results

To follow these best practices, the following Pydantic tips can help:

1. Use `Field(default=None)` to make fields optional
2. Add validation with `Field(min_length=1, max_length=100)`
3. Create custom validators with @validator decorator
4. Use `conlist()` for list fields with constraints
5. Add example values with `Field(example="Sample text")`
6. Create nested models for complex data structures
7. Use computed fields with `@property` decorator

If you follow all these tips, your schema can become quite sophisticated like below:

```python
from pydantic import BaseModel, Field
from typing import Optional, List
from datetime import datetime


class Author(BaseModel):
    # Required field - must be provided when creating an Author
    name: str = Field(
        ...,
        min_length=1,
        max_length=100,
        description="The full name of the article author",
    )

    # Optional field - can be None or omitted
    title: Optional[str] = Field(
        None, description="Author's title or role, if available"
    )


class NewsArticle(BaseModel):
    # Required field - must be provided when creating a NewsArticle
    title: str = Field(
        ...,
        min_length=5,
        max_length=300,
        description="The main headline or title of the news article",
        example="Breaking News: Major Scientific Discovery",
    )

    # Required field - must be provided when creating a NewsArticle
    url: str = Field(
        ...,
        description="The full URL of the article",
        example="https://www.nytimes.com/2024/01/01/science/discovery.html",
    )

    # Optional field - can be None or omitted
    authors: Optional[List[Author]] = Field(
        default=None, description="List of article authors and their details"
    )

    # Optional field - can be None or omitted
    publish_date: Optional[datetime] = Field(
        default=None, description="When the article was published"
    )

    # Optional field with default empty list
    financial_amounts: List[float] = Field(
        default_factory=list,
        max_length=10,
        description="Any monetary amounts mentioned in the article in USD",
    )

    @property
    def is_recent(self) -> bool:
        if not self.publish_date:
            return False
        return (datetime.now() - self.publish_date).days < 7
```

The schema above defines two key data models for news article data:

Author - Represents article author information with:

- `name` (required): The author's full name
- `title` (optional): The author's role or title

NewsArticle - Represents a news article with:

- `title` (required): The article headline (5-300 chars)
- `url` (required): Full article URL
- `authors` (optional): List of Author objects
- `publish_date` (optional): Article publication datetime
- `financial_amounts` (optional): List of monetary amounts in USD

The `NewsArticle` model includes an `is_recent` property that checks if the article was published within the last 7 days.

As you can see, web scraping process becomes much easier and more powerful if you combine it with structured data models that validate and organize the scraped information. This allows for consistent data formats, type checking, and easy access to properties like checking if an article is recent.

## Large-scale Scraping With Batch Operations

Up to this point, we have been focusing on scraping pages one URL at a time. In reality, you will work with multiple, perhaps, thousands of URLs that need to be scraped in parallel. This is where batch operations become essential for efficient web scraping at scale. Batch operations allow you to process multiple URLs simultaneously, significantly reducing the overall time needed to collect data from multiple web pages.

### Batch Scraping with `batch_scrape_urls`

The `batch_scrape_urls` method lets you scrape multiple URLs at once.

Let's scrape all the news article links we obtained from our previous schema extraction example.

```python
articles = structured_data['extract']['news_articles']
article_links = [article['url'] for article in articles]

class ArticleSummary(BaseModel):
    title: str = Field(description="The title of the news article")
    summary: str = Field(description="A short summary of the news article")

batch_data = app.batch_scrape_urls(article_links, params={
    "formats": ["extract"],
    "extract": {
        "schema": ArticleSummary.model_json_schema(),
        "prompt": "Extract the title of the news article and generate its brief summary",
    }
})
```

Here is what is happening in the codeblock above:

- We extract the list of news articles from our previous structured data result
- We create a list of article URLs by mapping over the articles and getting their 'url' field
- We define an `ArticleSummary` model with title and summary fields to structure our output
- We use `batch_scrape_urls()` to process all article URLs in parallel, configuring it to:
  - Extract data in structured format
  - Use our `ArticleSummary` schema
  - Generate titles and summaries based on the article content

The response from `batch_scrape_urls()` is a bit different:

```python
batch_data.keys()
```

```python
    dict_keys(['success', 'status', 'completed', 'total', 'creditsUsed', 'expiresAt', 'data'])
```

It contains the following fields:

- `success`: Boolean indicating if the batch request succeeded
- `status`: Current status of the batch job
- `completed`: Number of URLs processed so far
- `total`: Total number of URLs in the batch
- `creditsUsed`: Number of API credits consumed
- `expiresAt`: When the results will expire
- `data`: The extracted data for each URL

Let's focus on the `data` key where the actual content is stored:

```python
len(batch_data['data'])

```

```out
19
```

The batch processing completed successfully with 19 articles. Let's examine the structure of the first article:

```python
batch_data['data'][0].keys()
```

```out
dict_keys(['extract', 'metadata'])


The response format here matches what we get from individual `scrape_url` calls.


```python
print(batch_data['data'][0]['extract'])

```

```out
    {'title': 'Ukrainian Forces Face Increasing Challenges Amidst Harsh Winter Conditions', 'summary': 'As the war in Ukraine enters its fourth winter, conditions are worsening for Ukrainian soldiers who find themselves trapped on the battlefield, surrounded by Russian forces. Military commanders express concerns over dwindling supplies and increasingly tough situations. The U.S. has recently allowed Ukraine to use American weapons for deeper strikes into Russia, marking a significant development in the ongoing conflict.'}
```

The scraping was performed according to our specifications, extracting the metadata, the title and generating a brief summary.

### Asynchronous batch scraping with `async_batch_scrape_urls`

Scraping the 19 NY Times articles in a batch took about 10 seconds on my machine. While that's not much, in practice, we cannot wait around as Firecrawl batch-scrapes thousands of URLs. For these larger workloads, Firecrawl provides an asynchronous batch scraping API that lets you submit jobs and check their status later, rather than blocking until completion. This is especially useful when integrating web scraping into automated workflows or processing large URL lists.

This feature is available through the `async_batch_scrape_urls` method and it works a bit differently:

```python
batch_scrape_job = app.async_batch_scrape_urls(
    article_links,
    params={
        "formats": ["extract"],
        "extract": {
            "schema": ArticleSummary.model_json_schema(),
            "prompt": "Extract the title of the news article and generate its brief summary",
        },
    },
)
```

When using `async_batch_scrape_urls` instead of the synchronous version, the response comes back immediately rather than waiting for all URLs to be scraped. This allows the program to continue executing while the scraping happens in the background.

```python
batch_scrape_job
```

```python
    {'success': True,
     'id': '77a94b62-c676-4db2-b61b-4681e99f4704',
     'url': 'https://api.firecrawl.dev/v1/batch/scrape/77a94b62-c676-4db2-b61b-4681e99f4704'}
```

The response contains an ID belonging the background task that was initiated to process the URLs under the hood.

You can use this ID later to check the job's status with `check_batch_scrape_status` method:

```python
batch_scrape_job_status = app.check_batch_scrape_status(batch_scrape_job['id'])

batch_scrape_job_status.keys()
```

```python
dict_keys(['success', 'status', 'total', 'completed', 'creditsUsed', 'expiresAt', 'data', 'error', 'next'])
```

If the job finished scraping all URLs, its `status` will be set to `completed`:

```python
batch_scrape_job_status['status']
```

```out
'completed'
```

Let's look at how many pages were scraped:

```python
batch_scrape_job_status['total']
```

```python
19
```

The response always includes the `data` field, whether the job is complete or not, with the content scraped up to that point. It has `error` and `next` fields to indicate if any errors occurred during scraping and whether there are more results to fetch.

## How to Scrape Dynamic JavaScript Websites

Out in the wild, many websites you encounter will be dynamic, meaning their content is generated on-the-fly using JavaScript rather than being pre-rendered on the server. These sites often require user interaction like clicking buttons or typing into forms before displaying their full content. Traditional web scrapers that only look at the initial HTML fail to capture this dynamic content, which is why browser automation capabilities are essential for comprehensive web scraping.

Firecrawl supports dynamic scraping by default. In the parameters of `scrape_url` or `batch_scrape_url`, you can define necessary actions to reach the target state of the page you are scraping. As an example, we will build a scraper that will extract the following information from `https://weather.com`:

- Current Temperature
- Temperature High
- Temperature Low
- Humidity
- Pressure
- Visibility
- Wind Speed
- Dew Point
- UV Index
- Moon Phase

These details are displayed for every city you search through the website:

![Weather.com interface showing detailed weather forecast for London including temperature, humidity, wind speed and other meteorological data in an interactive dashboard layout](notebook_files/image.png)

Unlike websites such as Amazon where you can simply modify the URL's search parameter (e.g. `?search=your-query`), weather.com presents a unique challenge. The site generates dynamic and unique IDs for each city, making traditional URL manipulation techniques ineffective. To scrape weather data for any given city, you must simulate the actual user journey: visiting the homepage, interacting with the search bar, entering the city name, and selecting the appropriate result from the dropdown list. This multi-step interaction process is necessary because of how weather.com structures its dynamic content delivery (at this point, I urge to visit the website and visit a few city pages).

Fortunately, Firecrawl natively supports such interactions through the `actions` parameter. It accepts a list of dictionaries, where each dictionary represents one of the following interactions:

- Waiting for the page to load
- Clicking on an element
- Writing text in input fields
- Scrolling up/down
- Take a screenshot at the current state
- Scrape the current state of the webpage

Let's define the actions we need for weather.com:

```python
actions = [
    {"type": "wait", "milliseconds": 3000},
    {"type": "click", "selector": 'input[id="LocationSearch_input"]'},
    {"type": "write", "text": "London"},
    {"type": "screenshot"},
    {"type": "wait", "milliseconds": 1000},
    {"type": "click", "selector": "button[data-testid='ctaButton']"},
    {"type": "wait", "milliseconds": 3000},
]
```

Let's examine how we choose the selectors, as this is the most technical aspect of the actions. Using browser developer tools, we inspect the webpage elements to find the appropriate selectors. For the search input field, we locate an element with the ID "LocationSearch_input". After entering a city name, we include a 3-second wait to allow the dropdown search results to appear. At this stage, we capture a screenshot for debugging to verify the text input was successful.

The final step involves clicking the first matching result, which is identified by a button element with the `data-testid` attribute `ctaButton`. Note that if you're implementing this in the future, these specific attribute names may have changed - you'll need to use browser developer tools to find the current correct selectors.

Now, let's define a Pydantic schema to guide the LLM:

```python
class WeatherData(BaseModel):
    location: str = Field(description="The name of the city")
    temperature: str = Field(description="The current temperature in degrees Fahrenheit")
    temperature_high: str = Field(description="The high temperature for the day in degrees Fahrenheit")
    temperature_low: str = Field(description="The low temperature for the day in degrees Fahrenheit")
    humidity: str = Field(description="The current humidity as a percentage")
    pressure: str = Field(description="The current air pressure in inches of mercury")
    visibility: str = Field(description="The current visibility in miles")
    wind_speed: str = Field(description="The current wind speed in miles per hour")
    dew_point: str = Field(description="The current dew point in degrees Fahrenheit")
    uv_index: str = Field(description="The current UV index")
    moon_phase: str = Field(description="The current moon phase")
```

Finally, let's pass these objects to `scrape_url`:

```python
url = "https://weather.com"

data = app.scrape_url(
    url,
    params={
        "formats": ["screenshot", "markdown", "extract"],
        "actions": actions,
        "extract": {
            "schema": WeatherData.model_json_schema(),
            "prompt": "Extract the following weather data from the weather.com page: temperature, temperature high, temperature low, humidity, pressure, visibility, wind speed, dew point, UV index, and moon phase",
        },
    },
)
```

The scraping only happens once all actions are performed. Let's see if it was successful by looking at the `extract` key:

```python
data['extract']
```

```python
    {'location': 'London, England, United Kingdom',
     'temperature': '33¬∞',
     'temperature_high': '39¬∞',
     'temperature_low': '33¬∞',
     'humidity': '79%',
     'pressure': '29.52in',
     'visibility': '10 mi',
     'wind_speed': '5 mph',
     'dew_point': '28¬∞',
     'uv_index': '0 of 11',
     'moon_phase': 'Waning Gibbous'}
```

All details are accounted for! But, for illustration, we need to take a closer look at the response structure when using JS-based actions:

```python
data.keys()
```

```python
dict_keys(['markdown', 'screenshot', 'actions', 'metadata', 'extract'])
```

The response has a new actions key:

```python
data['actions']
```

```python
    {'screenshots': ['https://service.firecrawl.dev/storage/v1/object/public/media/screenshot-16bf71d8-dcb5-47eb-9af4-5fa84195b91d.png'],
     'scrapes': []}
```

The actions array contained a single screenshot-generating action, which is reflected in the output above.

Let's look at the screenshot:

```python
from IPython.display import Image

Image(data['actions']['screenshots'][0])
```

![Screenshot of weather.com search interface showing search bar with typed city name, demonstrating automated web scraping process with Firecrawl](notebook_files/notebook_96_0.png)

The image shows the stage where the scraper just typed the search query.

Now, we have to convert this whole process into a function that works for any given city:

```python
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any


class WeatherData(BaseModel):
    location: str = Field(description="The name of the city")
    temperature: str = Field(
        description="The current temperature in degrees Fahrenheit"
    )
    temperature_high: str = Field(
        description="The high temperature for the day in degrees Fahrenheit"
    )
    temperature_low: str = Field(
        description="The low temperature for the day in degrees Fahrenheit"
    )
    humidity: str = Field(description="The current humidity as a percentage")
    pressure: str = Field(description="The current air pressure in inches of mercury")
    visibility: str = Field(description="The current visibility in miles")
    wind_speed: str = Field(description="The current wind speed in miles per hour")
    dew_point: str = Field(description="The current dew point in degrees Fahrenheit")
    uv_index: str = Field(description="The current UV index")
    moon_phase: str = Field(description="The current moon phase")


def scrape_weather_data(app: FirecrawlApp, city: str) -> Optional[WeatherData]:
    try:
        # Define the actions to search for the city
        actions = [
            {"type": "wait", "milliseconds": 3000},
            {"type": "click", "selector": 'input[id="LocationSearch_input"]'},
            {"type": "write", "text": city},
            {"type": "wait", "milliseconds": 1000},
            {"type": "click", "selector": "button[data-testid='ctaButton']"},
            {"type": "wait", "milliseconds": 3000},
        ]

        # Perform the scraping
        data = app.scrape_url(
            "https://weather.com",
            params={
                "formats": ["extract"],
                "actions": actions,
                "extract": {
                    "schema": WeatherData.model_json_schema(),
                    "prompt": "Extract the following weather data from the weather.com page: temperature, temperature high, temperature low, humidity, pressure, visibility, wind speed, dew point, UV index, and moon phase",
                },
            },
        )

        # Return the extracted weather data
        return WeatherData(**data["extract"])

    except Exception as e:
        print(f"Error scraping weather data for {city}: {str(e)}")
        return None
```

The code is the same but it is wrapped inside a function. Let's test it on various cities:

```python
cities = ["Tashkent", "New York", "Tokyo", "Paris", "Istanbul"]
data_full = []

for city in cities:
    weather_data = scrape_weather_data(app, city)
    data_full.append(weather_data)
```

We can convert the data for all cities into a DataFrame now:

```python
import pandas as pd

# Convert list of WeatherData objects into dictionaries
data_dicts = [city.model_dump() for city in data_full]

# Convert list of dictionaries into DataFrame
df = pd.DataFrame(data_dicts)

print(df.head())
```

| location | temperature | temperature_high | temperature_low | humidity | pressure | visibility | wind_speed | dew_point | uv_index | moon_phase |
|----------|-------------|------------------|-----------------|----------|-----------|------------|------------|-----------|-----------|------------|
| Tashkent, Uzbekistan | 48 | 54 | 41 | 81 | 30.30 | 2.5 | 2 | 43 | 0 | Waning Gibbous |
| New York City, NY | 48¬∞ | 49¬∞ | 39¬∞ | 93% | 29.45 in | 4 mi | 10 mph | 46¬∞ | 0 of 11 | Waning Gibbous |
| Tokyo, Tokyo Prefecture, Japan | 47¬∞ | 61¬∞ | 48¬∞ | 95% | 29.94 in | 10 mi | 1 mph | 45¬∞ | 0 of 11 | Waning Gibbous |
| Paris, France | 34¬∞ | 36¬∞ | 30¬∞ | 93% | 29.42 in | 2.4 mi | 11 mph | 33¬∞ | 0 of 11 | Waning Gibbous |
| Istanbul, T√ºrkiye | 47¬∞ | 67¬∞ | 44¬∞ | 79% | 29.98 in | 8 mi | 4 mph | 41¬∞ | 0 of 11 | Waning Gibbous |

We have successfully scraped weather data from multiple cities using Firecrawl and organized it into a structured DataFrame. This demonstrates how we can efficiently collect and analyze data generated by dynamic websites for further analysis and monitoring.

## Conclusion

In this comprehensive guide, we've explored Firecrawl's `/scrape` endpoint and its powerful capabilities for modern web scraping. We covered:

- Basic scraping setup and configuration options
- Multiple output formats including HTML, markdown, and screenshots
- Structured data extraction using both natural language prompts and Pydantic schemas
- Batch operations for processing multiple URLs efficiently
- Advanced techniques for scraping JavaScript-heavy dynamic websites

Through practical examples like extracting news articles from the NY Times and weather data from weather.com, we've demonstrated how Firecrawl simplifies complex scraping tasks while providing flexible output formats suitable for data engineering and AI/ML pipelines.

The combination of LLM-powered extraction, structured schemas, and browser automation capabilities makes Firecrawl a versatile tool for gathering high-quality web data at scale, whether you're building training datasets, monitoring websites, or conducting research.

To discover more what Firecrawl has to offer, refer to [our guide on the `/crawl` endpoint](https://www.firecrawl.dev/blog/mastering-the-crawl-endpoint-in-firecrawl), which scrapes websites in their entirety with a single command while using the `/scrape` endpoint under the hood.

For more hands-on uses-cases of Firecrawl, these posts may interest you as well:

- [Using Prompt Caching With Anthropic](https://www.firecrawl.dev/blog/using-prompt-caching-with-anthropic)
- [Scraping Job Boards With Firecrawl and OpenAI](https://www.firecrawl.dev/blog/scrape-job-boards-firecrawl-openai)
- [Scraping and Analyzing Airbnb Listings in Python Tutorial](https://www.firecrawl.dev/blog/scrape-analyze-airbnb-data-with-e2b)

================
File: examples/claude_stock_analyzer/claude_stock_analyzer.py
================
import os
from firecrawl import FirecrawlApp
import json
from dotenv import load_dotenv
import anthropic
from e2b_code_interpreter import Sandbox
import base64

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
e2b_api_key = os.getenv("E2B_API_KEY")

# Initialize the FirecrawlApp and Anthropic client
app = FirecrawlApp(api_key=firecrawl_api_key)
client = anthropic.Anthropic(api_key=anthropic_api_key)
sandbox = Sandbox(api_key=e2b_api_key)

# Find the relevant stock pages via map
def find_relevant_page_via_map(stock_search_term, url, app):
    try:
        print(f"{Colors.CYAN}Searching for stock: {stock_search_term}{Colors.RESET}")
        print(f"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}")

        map_search_parameter = stock_search_term

        print(f"{Colors.GREEN}Search parameter: {map_search_parameter}{Colors.RESET}")

        print(f"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}")
        map_website = app.map_url(url, params={"search": map_search_parameter})
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")
        print(f"{Colors.GREEN}Located {len(map_website['links'])} relevant links.{Colors.RESET}")
        return map_website['links']
    except Exception as e:
        print(f"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}")
        return None

# Function to plot the scores using e2b
def plot_scores(stock_names, stock_scores):
    print(f"{Colors.YELLOW}Plotting scores...{Colors.RESET}")
    code_to_run = f"""
import matplotlib.pyplot as plt

stock_names = {stock_names}
stock_scores = {stock_scores}

plt.figure(figsize=(10, 5))
plt.bar(stock_names, stock_scores, color='blue')
plt.xlabel('Stock Names')
plt.ylabel('Scores')
plt.title('Stock Investment Scores')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('chart.png')
plt.show()
"""
    # Run the code inside the sandbox
    execution = sandbox.run_code(code_to_run)

    # Check if there are any results
    if execution.results and execution.results[0].png:
        first_result = execution.results[0]

        # Get the directory where the current python file is located
        current_dir = os.path.dirname(os.path.abspath(__file__))
        # Save the png to a file in the examples directory. The png is in base64 format.
        with open(os.path.join(current_dir, 'chart.png'), 'wb') as f:
            f.write(base64.b64decode(first_result.png))
        print('Chart saved as examples/chart.png')
    else:
        print(f"{Colors.RED}No results returned from the sandbox execution.{Colors.RESET}")

# Analyze the top stocks and provide investment recommendation
def analyze_top_stocks(map_website, app, client):
    try:
        # Get top 5 links from the map result
        top_links = map_website[:10]
        print(f"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}")

        # Scrape the pages in batch
        batch_scrape_result = app.batch_scrape_urls(top_links, {'formats': ['markdown']})
        print(f"{Colors.GREEN}Batch page scraping completed successfully.{Colors.RESET}")

        # Prepare content for LLM
        stock_contents = []
        for scrape_result in batch_scrape_result['data']:
            stock_contents.append({
                'content': scrape_result['markdown']
            })

        # Pass all the content to the LLM to analyze and decide which stock to invest in
        analyze_prompt = f"""
Based on the following information about different stocks from their Robinhood pages, analyze and determine which stock is the best investment opportunity. DO NOT include any other text, just the JSON.

Return the result in the following JSON format. Only return the JSON, nothing else. Do not include backticks or any other formatting, just the JSON.
{{
    "scores": [
        {{
            "stock_name": "<stock_name>",
            "score": <score-out-of-100>
        }},
        ...
    ]
}}

Stock Information:
"""

        for stock in stock_contents:
            analyze_prompt += f"Content:\n{stock['content']}\n"

        print(f"{Colors.YELLOW}Analyzing stock information with LLM...{Colors.RESET}")
        analyze_prompt += f"\n\nStart JSON:\n"
        completion = client.messages.create(
            model="claude-3-5-sonnet-20240620",
            max_tokens=1000,
            temperature=0,
            system="You are a financial analyst. Only return the JSON, nothing else.",
            messages=[
                {
                    "role": "user",
                    "content": analyze_prompt
                }
            ]
        )

        result = completion.content[0].text
        print(f"{Colors.GREEN}Analysis completed. Here is the recommendation:{Colors.RESET}")
        print(f"{Colors.MAGENTA}{result}{Colors.RESET}")

        # Plot the scores using e2b
        try:
            result_json = json.loads(result)
            scores = result_json['scores']
            stock_names = [score['stock_name'] for score in scores]
            stock_scores = [score['score'] for score in scores]

            plot_scores(stock_names, stock_scores)
        except json.JSONDecodeError as json_err:
            print(f"{Colors.RED}Error decoding JSON response: {str(json_err)}{Colors.RESET}")

    except Exception as e:
        print(f"{Colors.RED}Error encountered during stock analysis: {str(e)}{Colors.RESET}")

# Main function to execute the process
def main():
    # Get user input
    stock_search_term = input(f"{Colors.BLUE}Enter the stock you're interested in: {Colors.RESET}")
    if not stock_search_term.strip():
        print(f"{Colors.RED}No stock entered. Exiting.{Colors.RESET}")
        return

    url = "https://robinhood.com/stocks"

    print(f"{Colors.YELLOW}Initiating stock analysis process...{Colors.RESET}")
    # Find the relevant pages
    map_website = find_relevant_page_via_map(stock_search_term, url, app)

    if map_website:
        print(f"{Colors.GREEN}Relevant stock pages identified. Proceeding with detailed analysis...{Colors.RESET}")
        # Analyze top stocks
        analyze_top_stocks(map_website, app, client)
    else:
        print(f"{Colors.RED}No relevant stock pages identified. Consider refining the search term or trying a different stock.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/claude-3.7-stock-analyzer/claude-3.7-stock-analyzer.py
================
import os
from firecrawl import FirecrawlApp
import json
from dotenv import load_dotenv
import anthropic
from e2b_code_interpreter import Sandbox
import base64

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
e2b_api_key = os.getenv("E2B_API_KEY")

# Initialize the FirecrawlApp and Anthropic client
app = FirecrawlApp(api_key=firecrawl_api_key)
client = anthropic.Anthropic(api_key=anthropic_api_key)
sandbox = Sandbox(api_key=e2b_api_key)

# Find the relevant stock pages via map
def find_relevant_page_via_map(stock_search_term, url, app):
    try:
        print(f"{Colors.CYAN}Searching for stock: {stock_search_term}{Colors.RESET}")
        print(f"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}")

        map_search_parameter = stock_search_term

        print(f"{Colors.GREEN}Search parameter: {map_search_parameter}{Colors.RESET}")

        print(f"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}")
        map_website = app.map_url(url, params={"search": map_search_parameter})
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")
        print(f"{Colors.GREEN}Located {len(map_website['links'])} relevant links.{Colors.RESET}")
        return map_website['links']
    except Exception as e:
        print(f"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}")
        return None

# Function to plot the scores using e2b
def plot_scores(stock_names, stock_scores):
    print(f"{Colors.YELLOW}Plotting scores...{Colors.RESET}")
    code_to_run = f"""
import matplotlib.pyplot as plt

stock_names = {stock_names}
stock_scores = {stock_scores}

plt.figure(figsize=(10, 5))
plt.bar(stock_names, stock_scores, color='blue')
plt.xlabel('Stock Names')
plt.ylabel('Scores')
plt.title('Stock Investment Scores')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('chart.png')
plt.show()
"""
    # Run the code inside the sandbox
    execution = sandbox.run_code(code_to_run)

    # Check if there are any results
    if execution.results and execution.results[0].png:
        first_result = execution.results[0]

        # Get the directory where the current python file is located
        current_dir = os.path.dirname(os.path.abspath(__file__))
        # Save the png to a file in the examples directory. The png is in base64 format.
        with open(os.path.join(current_dir, 'chart.png'), 'wb') as f:
            f.write(base64.b64decode(first_result.png))
        print('Chart saved as examples/chart.png')
    else:
        print(f"{Colors.RED}No results returned from the sandbox execution.{Colors.RESET}")

# Analyze the top stocks and provide investment recommendation
def analyze_top_stocks(map_website, app, client):
    try:
        # Get top 5 links from the map result
        top_links = map_website[:10]
        print(f"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}")

        # Scrape the pages in batch
        batch_scrape_result = app.batch_scrape_urls(top_links, {'formats': ['markdown']})
        print(f"{Colors.GREEN}Batch page scraping completed successfully.{Colors.RESET}")

        # Prepare content for LLM
        stock_contents = []
        for scrape_result in batch_scrape_result['data']:
            stock_contents.append({
                'content': scrape_result['markdown']
            })

        # Pass all the content to the LLM to analyze and decide which stock to invest in
        analyze_prompt = f"""
Based on the following information about different stocks from their Robinhood pages, analyze and determine which stock is the best investment opportunity. DO NOT include any other text, just the JSON.

Return the result in the following JSON format. Only return the JSON, nothing else. Do not include backticks or any other formatting, just the JSON.
{{
    "scores": [
        {{
            "stock_name": "<stock_name>",
            "score": <score-out-of-100>
        }},
        ...
    ]
}}

Stock Information:
"""

        for stock in stock_contents:
            analyze_prompt += f"Content:\n{stock['content']}\n"

        print(f"{Colors.YELLOW}Analyzing stock information with LLM...{Colors.RESET}")
        analyze_prompt += f"\n\nStart JSON:\n"
        completion = client.messages.create(
            model="claude-3-7-sonnet-20250219",
            max_tokens=1000,
            temperature=0,
            system="You are a financial analyst. Only return the JSON, nothing else.",
            messages=[
                {
                    "role": "user",
                    "content": analyze_prompt
                }
            ]
        )

        result = completion.content[0].text
        print(f"{Colors.GREEN}Analysis completed. Here is the recommendation:{Colors.RESET}")
        print(f"{Colors.MAGENTA}{result}{Colors.RESET}")

        # Plot the scores using e2b
        try:
            result_json = json.loads(result)
            scores = result_json['scores']
            stock_names = [score['stock_name'] for score in scores]
            stock_scores = [score['score'] for score in scores]

            plot_scores(stock_names, stock_scores)
        except json.JSONDecodeError as json_err:
            print(f"{Colors.RED}Error decoding JSON response: {str(json_err)}{Colors.RESET}")

    except Exception as e:
        print(f"{Colors.RED}Error encountered during stock analysis: {str(e)}{Colors.RESET}")

# Main function to execute the process
def main():
    # Get user input
    stock_search_term = input(f"{Colors.BLUE}Enter the stock you're interested in: {Colors.RESET}")
    if not stock_search_term.strip():
        print(f"{Colors.RED}No stock entered. Exiting.{Colors.RESET}")
        return

    url = "https://robinhood.com/stocks"

    print(f"{Colors.YELLOW}Initiating stock analysis process...{Colors.RESET}")
    # Find the relevant pages
    map_website = find_relevant_page_via_map(stock_search_term, url, app)

    if map_website:
        print(f"{Colors.GREEN}Relevant stock pages identified. Proceeding with detailed analysis...{Colors.RESET}")
        # Analyze top stocks
        analyze_top_stocks(map_website, app, client)
    else:
        print(f"{Colors.RED}No relevant stock pages identified. Consider refining the search term or trying a different stock.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/claude3.7-web-crawler/claude3.7-web-crawler.py
================
import os
from firecrawl import FirecrawlApp
import json
from dotenv import load_dotenv
import anthropic

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
claude_api_key = os.getenv("ANTHROPIC_API_KEY")

# Initialize the FirecrawlApp and Claude client
app = FirecrawlApp(api_key=firecrawl_api_key)
client = anthropic.Anthropic(api_key=claude_api_key)

# Find the page that most likely contains the objective
def find_relevant_page_via_map(objective, url, app, client):
    try:
        print(f"{Colors.CYAN}Understood. The objective is: {objective}{Colors.RESET}")
        print(f"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}")
        
        map_prompt = f"""
        The map function generates a list of URLs from a website and it accepts a search parameter. Based on the objective of: {objective}, come up with a 1-2 word search parameter that will help us find the information we need. Only respond with 1-2 words nothing else.
        """

        print(f"{Colors.YELLOW}Analyzing objective to determine optimal search parameter...{Colors.RESET}")
        completion = client.messages.create(
            model="claude-3-7-sonnet-20250219",
            max_tokens=100,
            messages=[
                {
                    "role": "user",
                    "content": map_prompt
                }
            ]
        )

        map_search_parameter = completion.content[0].text
        print(f"{Colors.GREEN}Optimal search parameter identified: {map_search_parameter}{Colors.RESET}")

        print(f"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}")
        map_website = app.map_url(url, params={"search": map_search_parameter})
        
        # Debug print to see the response structure
        print(f"{Colors.MAGENTA}Debug - Map response structure: {json.dumps(map_website, indent=2)}{Colors.RESET}")
        
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")
        
        # Handle the response based on its structure
        if isinstance(map_website, dict):
            # Assuming the links are in a 'urls' or similar key
            links = map_website.get('urls', []) or map_website.get('links', [])
        elif isinstance(map_website, str):
            try:
                parsed = json.loads(map_website)
                links = parsed.get('urls', []) or parsed.get('links', [])
            except json.JSONDecodeError:
                links = []
        else:
            links = map_website if isinstance(map_website, list) else []

        if not links:
            print(f"{Colors.RED}No links found in map response.{Colors.RESET}")
            return None

        rank_prompt = f"""
        Given this list of URLs and the objective: {objective}
        Analyze each URL and rank the top 3 most relevant ones that are most likely to contain the information we need.
        Return your response as a JSON array with exactly 3 objects, each containing:
        - "url": the full URL
        - "relevance_score": number between 0-100 indicating relevance to objective
        - "reason": brief explanation of why this URL is relevant

        Example output:
        [
            {{
                "url": "https://example.com/about",
                "relevance_score": 95,
                "reason": "Main about page containing company information"
            }},
            {{
                "url": "https://example.com/team",
                "relevance_score": 80,
                "reason": "Team page with leadership details"
            }},
            {{
                "url": "https://example.com/contact",
                "relevance_score": 70,
                "reason": "Contact page with location information"
            }}
        ]

        URLs to analyze:
        {json.dumps(links, indent=2)}
        """

        print(f"{Colors.YELLOW}Ranking URLs by relevance to objective...{Colors.RESET}")
        completion = client.messages.create(
            model="claude-3-7-sonnet-20250219",
            max_tokens=1000,
            messages=[
                {
                    "role": "user", 
                    "content": rank_prompt
                }
            ]
        )

        # Debug print to see Claude's raw response
        print(f"{Colors.MAGENTA}Debug - Claude's raw response:{Colors.RESET}")
        print(f"{Colors.MAGENTA}{completion.content[0].text}{Colors.RESET}")

        try:
            # Try to clean the response by stripping any potential markdown or extra whitespace
            cleaned_response = completion.content[0].text.strip()
            if cleaned_response.startswith("```json"):
                cleaned_response = cleaned_response.split("```json")[1]
            if cleaned_response.endswith("```"):
                cleaned_response = cleaned_response.rsplit("```", 1)[0]
            cleaned_response = cleaned_response.strip()
            
            ranked_results = json.loads(cleaned_response)
            
            # Validate the structure of the results
            if not isinstance(ranked_results, list):
                raise ValueError("Response is not a list")
            
            for result in ranked_results:
                if not all(key in result for key in ["url", "relevance_score", "reason"]):
                    raise ValueError("Response items missing required fields")
            
            links = [result["url"] for result in ranked_results]
            
            # Print detailed ranking info
            print(f"{Colors.CYAN}Top 3 ranked URLs:{Colors.RESET}")
            for result in ranked_results:
                print(f"{Colors.GREEN}URL: {result['url']}{Colors.RESET}")
                print(f"{Colors.YELLOW}Relevance Score: {result['relevance_score']}{Colors.RESET}")
                print(f"{Colors.BLUE}Reason: {result['reason']}{Colors.RESET}")
                print("---")

            if not links:
                print(f"{Colors.RED}No relevant links identified.{Colors.RESET}")
                return None

        except (json.JSONDecodeError, KeyError) as e:
            print(f"{Colors.RED}Error parsing ranked results: {str(e)}{Colors.RESET}")
            return None
            
        print(f"{Colors.GREEN}Located {len(links)} relevant links.{Colors.RESET}")
        return links
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}")
        return None
    
# Scrape the top 3 pages and see if the objective is met, if so return in json format else return None
def find_objective_in_top_pages(map_website, objective, app, client):
    try:
        # Get top 3 links from the map result
        if not map_website:
            print(f"{Colors.RED}No links found to analyze.{Colors.RESET}")
            return None
            
        top_links = map_website[:3]
        print(f"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}")
        
        for link in top_links:
            print(f"{Colors.YELLOW}Initiating scrape of page: {link}{Colors.RESET}")
            scrape_result = app.scrape_url(link, params={'formats': ['markdown']})
            print(f"{Colors.GREEN}Page scraping completed successfully.{Colors.RESET}")
     
            check_prompt = f"""
            Given the following scraped content and objective, determine if the objective is met.
            If it is, extract the relevant information in a simple and concise JSON format. Use only the necessary fields and avoid nested structures if possible.
            If the objective is not met with confidence, respond with exactly 'Objective not met'.

            Objective: {objective}
            Scraped content: {scrape_result['markdown']}

            Remember:
            1. Only return JSON if you are confident the objective is fully met.
            2. Keep the JSON structure as simple and flat as possible.
            3. If returning JSON, ensure it's valid JSON format without any markdown formatting.
            4. If objective is not met, respond only with 'Objective not met'.
            """
        
            completion = client.messages.create(
                model="claude-3-7-sonnet-20250219",
                max_tokens=1500,
                messages=[{"role": "user", "content": check_prompt}]
            )
            
            result = completion.content[0].text.strip()
            
            # Clean up the response if it contains markdown formatting
            if result.startswith("```json"):
                result = result.split("```json")[1]
            if result.endswith("```"):
                result = result.rsplit("```", 1)[0]
            result = result.strip()
            
            if result == "Objective not met":
                print(f"{Colors.YELLOW}Objective not met on this page. Proceeding to next link...{Colors.RESET}")
                continue
                
            try:
                json_result = json.loads(result)
                print(f"{Colors.GREEN}Objective fulfilled. Relevant information found.{Colors.RESET}")
                return json_result
            except json.JSONDecodeError as e:
                print(f"{Colors.RED}Error parsing JSON response: {str(e)}{Colors.RESET}")
                print(f"{Colors.MAGENTA}Raw response: {result}{Colors.RESET}")
                continue

        print(f"{Colors.RED}All available pages analyzed. Objective not fulfilled in examined content.{Colors.RESET}")
        return None
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during page analysis: {str(e)}{Colors.RESET}")
        return None

# Main function to execute the process
def main():
    # Get user input
    url = input(f"{Colors.BLUE}Enter the website to crawl : {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter your objective: {Colors.RESET}")
    
    print(f"{Colors.YELLOW}Initiating web crawling process...{Colors.RESET}")
    # Find the relevant page
    map_website = find_relevant_page_via_map(objective, url, app, client)
    
    if map_website:
        print(f"{Colors.GREEN}Relevant pages identified. Proceeding with detailed analysis using Claude 3.7...{Colors.RESET}")
        # Find objective in top pages
        result = find_objective_in_top_pages(map_website, objective, app, client)
        
        if result:
            print(f"{Colors.GREEN}Objective successfully fulfilled. Extracted information :{Colors.RESET}")
            print(f"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}")
        else:
            print(f"{Colors.RED}Unable to fulfill the objective with the available content.{Colors.RESET}")
    else:
        print(f"{Colors.RED}No relevant pages identified. Consider refining the search parameters or trying a different website.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/claude3.7-web-extractor/claude-3.7-web-extractor.py
================
import os
import json
import time
import requests
from dotenv import load_dotenv
from serpapi.google_search import GoogleSearch

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Initialize clients
anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
serp_api_key = os.getenv("SERP_API_KEY")

if not anthropic_api_key:
    print(f"{Colors.RED}Warning: ANTHROPIC_API_KEY not found in environment variables{Colors.RESET}")
if not firecrawl_api_key:
    print(f"{Colors.RED}Warning: FIRECRAWL_API_KEY not found in environment variables{Colors.RESET}")

def search_google(query):
    """Search Google using SerpAPI and return top results."""
    print(f"{Colors.YELLOW}Searching Google for '{query}'...{Colors.RESET}")
    search = GoogleSearch({"q": query, "api_key": serp_api_key})
    return search.get_dict().get("organic_results", [])

def select_urls_with_claude(company, objective, serp_results):
    """
    Use Claude 3.7 Sonnet to select URLs from SERP results.
    Returns a list of URLs.
    """
    try:
        serp_data = [{"title": r.get("title"), "link": r.get("link"), "snippet": r.get("snippet")} 
                     for r in serp_results if r.get("link")]

        prompt = (
            "Task: Select relevant URLs from search results.\n\n"
            "Instructions:\n"
            "1. Analyze the search results for information about the specified company\n"
            "2. Select URLs that are most likely to contain the requested information\n"
            "3. Return ONLY a JSON object with the following structure: {\"selected_urls\": [\"url1\", \"url2\"]}\n"
            "4. Do not include social media links\n\n"
            f"Company: {company}\n"
            f"Information Needed: {objective}\n"
            f"Search Results: {json.dumps(serp_data, indent=2)}\n\n"
            "Response Format: {\"selected_urls\": [\"https://example.com\", \"https://example2.com\"]}"
        )

        headers = {
            "x-api-key": anthropic_api_key,
            "content-type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        payload = {
            "model": "claude-3-7-sonnet-20250219",
            "max_tokens": 1000,
            "messages": [
                {"role": "user", "content": prompt}
            ]
        }
        
        response = requests.post(
            "https://api.anthropic.com/v1/messages",
            headers=headers,
            json=payload
        )
        
        response_data = response.json()
        cleaned_response = response_data.get("content", [{}])[0].get("text", "")

        # Clean the response text
        if cleaned_response.startswith('```'):
            cleaned_response = cleaned_response.split('```')[1]
            if cleaned_response.startswith('json'):
                cleaned_response = cleaned_response[4:]
        cleaned_response = cleaned_response.strip()

        try:
            # Parse JSON response
            result = json.loads(cleaned_response)
            if isinstance(result, dict) and "selected_urls" in result:
                urls = result["selected_urls"]
            else:
                # Fallback to text parsing
                urls = [line.strip() for line in cleaned_response.split('\n') 
                       if line.strip().startswith(('http://', 'https://'))]
        except json.JSONDecodeError:
            # Fallback to text parsing
            urls = [line.strip() for line in cleaned_response.split('\n') 
                   if line.strip().startswith(('http://', 'https://'))]

        # Clean up URLs
        cleaned_urls = [url.replace('/*', '').rstrip('/') for url in urls]
        cleaned_urls = [url for url in cleaned_urls if url]

        if not cleaned_urls:
            print(f"{Colors.YELLOW}No valid URLs found in response.{Colors.RESET}")
            return []

        print(f"{Colors.CYAN}Selected URLs for extraction:{Colors.RESET}")
        for url in cleaned_urls:
            print(f"- {url}")

        return cleaned_urls

    except Exception as e:
        print(f"{Colors.RED}Error selecting URLs: {str(e)}{Colors.RESET}")
        return []

def extract_company_info(urls, prompt, company, api_key):
    """Use requests to call Firecrawl's extract endpoint with selected URLs."""
    print(f"{Colors.YELLOW}Extracting structured data from the provided URLs using Firecrawl...{Colors.RESET}")
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    payload = {
        "urls": urls,
        "prompt": prompt + " for " + company,
        "enableWebSearch": True
    }
    
    try:
        response = requests.post(
            "https://api.firecrawl.dev/v1/extract",
            headers=headers,
            json=payload,
            timeout=30
        )
        
        data = response.json()
        
        if not data.get('success'):
            print(f"{Colors.RED}API returned error: {data.get('error', 'No error message')}{Colors.RESET}")
            return None
        
        extraction_id = data.get('id')
        if not extraction_id:
            print(f"{Colors.RED}No extraction ID found in response.{Colors.RESET}")
            return None

        return poll_firecrawl_result(extraction_id, api_key)

    except requests.exceptions.RequestException as e:
        print(f"{Colors.RED}Request failed: {e}{Colors.RESET}")
        return None
    except json.JSONDecodeError as e:
        print(f"{Colors.RED}Failed to parse response: {e}{Colors.RESET}")
        return None
    except Exception as e:
        print(f"{Colors.RED}Failed to extract data: {e}{Colors.RESET}")
        return None

def poll_firecrawl_result(extraction_id, api_key, interval=10, max_attempts=60):
    """Poll Firecrawl API to get the extraction result."""
    url = f"https://api.firecrawl.dev/v1/extract/{extraction_id}"
    headers = {
        'Authorization': f'Bearer {api_key}'
    }

    print(f"{Colors.YELLOW}Waiting for extraction to complete...{Colors.RESET}")
    
    for attempt in range(1, max_attempts + 1):
        try:
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()

            if data.get('success') and data.get('data'):
                print(f"{Colors.GREEN}Data successfully extracted:{Colors.RESET}")
                print(json.dumps(data['data'], indent=2))
                return data['data']
            elif data.get('success') and not data.get('data'):
                if attempt % 6 == 0:  
                    print(f"{Colors.YELLOW}Still processing... (attempt {attempt}/{max_attempts}){Colors.RESET}")
                time.sleep(interval)
            else:
                print(f"{Colors.RED}API Error: {data.get('error', 'No error message provided')}{Colors.RESET}")
                return None

        except requests.exceptions.RequestException as e:
            print(f"{Colors.RED}Request error: {str(e)}{Colors.RESET}")
            return None
        except json.JSONDecodeError as e:
            print(f"{Colors.RED}JSON parsing error: {str(e)}{Colors.RESET}")
            return None
        except Exception as e:
            print(f"{Colors.RED}Unexpected error: {str(e)}{Colors.RESET}")
            return None

    print(f"{Colors.RED}Max polling attempts reached. Extraction did not complete in time.{Colors.RESET}")
    return None

def main():
    company = input(f"{Colors.BLUE}Enter the company name: {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter what information you want about the company: {Colors.RESET}")
    
    serp_results = search_google(f"{company}")
    if not serp_results:
        print(f"{Colors.RED}No search results found.{Colors.RESET}")
        return
    
    selected_urls = select_urls_with_claude(company, objective, serp_results)
    
    if not selected_urls:
        print(f"{Colors.RED}No URLs were selected.{Colors.RESET}")
        return
    
    data = extract_company_info(selected_urls, objective, company, firecrawl_api_key)
    
    if data:
        print(f"{Colors.GREEN}Extraction completed successfully.{Colors.RESET}")
    else:
        print(f"{Colors.RED}Failed to extract the requested information. Try refining your prompt or choosing a different company.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/contradiction_testing/web-data-contradiction-testing-using-llms.mdx
================
# Build an agent that checks your website for contradictions

Learn how to use Firecrawl and Claude to scrape your website's data and look for contradictions and inconsistencies in a few lines of code. When you are shipping fast, data is bound to get stale, with FireCrawl and LLMs you can make sure your public web data is always consistent! We will be using Opus's huge 200k context window and Firecrawl's parellization, making this process accurate and fast.

## Setup

Install our python dependencies, including anthropic and firecrawl-py.

```bash
pip install firecrawl-py anthropic
```

## Getting your Claude and Firecrawl API Keys

To use Claude Opus and Firecrawl, you will need to get your API keys. You can get your Anthropic API key from [here](https://www.anthropic.com/) and your Firecrawl API key from [here](https://firecrawl.dev).

## Load website with Firecrawl

To be able to get all the data from our website page put it into an easy to read format for the LLM, we will use [FireCrawl](https://firecrawl.dev). It handles by-passing JS-blocked websites, extracting the main content, and outputting in a LLM-readable format for increased accuracy.

Here is how we will scrape a website url using Firecrawl-py

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="YOUR-KEY")

crawl_result = app.crawl_url('mendable.ai', {'crawlerOptions': {'excludes': ['blog/*','usecases/*']}})

print(crawl_result)
```

With all of the web data we want scraped and in a clean format, we can move onto the next step.

## Combination and Generation

Now that we have the website data, let's pair up every page and run every combination through Opus for analysis.

```python
from itertools import combinations

page_combinations = []

for first_page, second_page in combinations(crawl_result, 2):
    combined_string = "First Page:\n" + first_page['markdown'] + "\n\nSecond Page:\n" + second_page['markdown']
    page_combinations.append(combined_string)

import anthropic

client = anthropic.Anthropic(
    # defaults to os.environ.get("ANTHROPIC_API_KEY")
    api_key="YOUR-KEY",
)

final_output = []

for page_combination in page_combinations:

    prompt = "Here are two pages from a companies website, your job is to find any contradictions or differences in opinion between the two pages, this could be caused by outdated information or other. If you find any contradictions, list them out and provide a brief explanation of why they are contradictory or differing. Make sure the explanation is specific and concise. It is okay if you don't find any contradictions, just say 'No contradictions found' and nothing else. Here are the pages: " + "\n\n".join(page_combination)

    message = client.messages.create(
        model="claude-3-opus-20240229",
        max_tokens=1000,
        temperature=0.0,
        system="You are an assistant that helps find contradictions or differences in opinion between pages in a company website and knowledge base. This could be caused by outdated information in the knowledge base.",
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    final_output.append(message.content)

```

## That's about it!

You have now built an agent that looks at your website and spots any inconsistencies it might have.

If you have any questions or need help, feel free to reach out to us at [Firecrawl](https://firecrawl.dev).

================
File: examples/crm_lead_enrichment/crm_lead_enrichment.py
================
import json
import os
from dotenv import load_dotenv
from openai import OpenAI
from hubspot import HubSpot
from firecrawl import FirecrawlApp

# Load environment variables
load_dotenv()

# Initialize clients
def initialize_clients():
    firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
    openai_api_key = os.getenv("OPENAI_API_KEY")
    hubspot_api_key = os.getenv("HUBSPOT_API_KEY")
    
    openai_client = OpenAI(api_key=openai_api_key)
    hubspot_client = HubSpot(access_token=hubspot_api_key)
    firecrawl_client = FirecrawlApp(api_key=firecrawl_api_key)
    
    return openai_client, hubspot_client, firecrawl_client

# Get list of companies from HubSpot
def get_companies_from_hubspot(hubspot_client):
    companies = []
    after = None
    while True:
        try:
            response = hubspot_client.crm.companies.basic_api.get_page(
                limit=100, 
                properties=["name", "website"],
                after=after
            )
            companies.extend(response.results)
            if not response.paging:
                break
            after = response.paging.next.after
        except Exception as e:
            print(f"Error fetching companies from HubSpot: {str(e)}")
            break
    return [company for company in companies if company.properties.get("website")]

# Scrape URL using Firecrawl
def scrape_url(firecrawl_client, url):
    try:
        return firecrawl_client.scrape_url(url, params={'formats': ['markdown']})
    except Exception as e:
        print(f"Error scraping URL {url}: {str(e)}")
        return None

# Extract information using OpenAI
def extract_info(openai_client, content):
    prompt = f"""
    Based on the markdown content, extract the following information in JSON format: 
    {{
        "is_open_source": boolean,
        "value_proposition": "string",
        "main_product": "string",
        "potential_scraping_use": "string"
    }}

    Are they open source?
    What is their value proposition?
    What are their main products?
    How could they use a web scraping service in one one their products?

    Markdown content:
    {content}

    Respond only with the JSON object, ensuring all fields are present even if the information is not found (use null in that case). Do not include the markdown code snippet like ```json or ``` at all in the response.
    """
    
    try:
        completion = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
        return json.loads(completion.choices[0].message.content)
    except Exception as e:
        print(f"Error extracting information: {str(e)}")
        print(completion.choices[0].message.content)
        return None

# Update company properties in HubSpot
def update_hubspot(hubspot_client, company, extracted_info):
    try:
        hubspot_client.crm.companies.basic_api.update(
            company_id=company.id,
            simple_public_object_input={
                "properties": {
                    "is_open_source": str(extracted_info["is_open_source"]).lower(),
                    "value_prop": extracted_info["value_proposition"],
                    "main_products_offered": extracted_info["main_product"],
                    "how_they_can_use_scraping": extracted_info["potential_scraping_use"]
                }
            }
        )
        print(f"Successfully updated HubSpot for company {company.properties['name']}")
    except Exception as e:
        print(f"Error updating HubSpot for company {company.properties['name']}: {str(e)}")

# Main process
def main():
    openai_client, hubspot_client, firecrawl_client = initialize_clients()
    companies = get_companies_from_hubspot(hubspot_client)
    
    scraped_data = []
    for company in companies:
        company_name = company.properties.get("name", "Unknown")
        url = company.properties["website"]
        print(f"Processing {company_name} at {url}...")
        
        scrape_status = scrape_url(firecrawl_client, url)
        if not scrape_status:
            continue
        
        extracted_info = extract_info(openai_client, scrape_status["content"])
        if not extracted_info:
            continue
        
        update_hubspot(hubspot_client, company, extracted_info)
        
        scraped_data.append({
            "company": company_name,
            "url": url,
            "markdown": scrape_status["content"],
            "extracted_info": extracted_info
        })
        
        print(f"Successfully processed {company_name}")
        print(json.dumps(extracted_info, indent=2))

    print(f"Scraped, analyzed, and updated {len(scraped_data)} companies")

if __name__ == "__main__":
    main()

================
File: examples/deep-research-apartment-finder/.env.example
================
# Firecrawl API key (get from https://firecrawl.dev)
FIRECRAWL_API_KEY=your_firecrawl_api_key_here

# Anthropic API key (get from https://console.anthropic.com)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

================
File: examples/deep-research-apartment-finder/apartment_finder.py
================
#!/usr/bin/env python3

import os
import sys
import json
from typing import Dict, List, Any
import anthropic
from firecrawl import FirecrawlApp
from dotenv import load_dotenv

# Define colors for terminal output
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    BOLD = '\033[1m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

FIRECRAWL_API_KEY = os.getenv("FIRECRAWL_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")

if not FIRECRAWL_API_KEY or not ANTHROPIC_API_KEY:
    print(f"{Colors.RED}Error: API keys not found. Please set FIRECRAWL_API_KEY and ANTHROPIC_API_KEY environment variables.{Colors.RESET}")
    print(f"{Colors.YELLOW}You can create a .env file with these variables or set them in your shell.{Colors.RESET}")
    sys.exit(1)

# Initialize clients
firecrawl = FirecrawlApp(api_key=FIRECRAWL_API_KEY)
claude = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

def get_user_preferences():
    """Get apartment search preferences from user input"""
    print(f"\n{Colors.BOLD}{Colors.CYAN}=== Apartment Finder ==={Colors.RESET}")
    print(f"{Colors.CYAN}Please enter your apartment search preferences:{Colors.RESET}")
    
    # Get required inputs
    location = input(f"\n{Colors.YELLOW}Enter location (city or neighborhood): {Colors.RESET}")
    while not location.strip():
        location = input(f"{Colors.RED}Location cannot be empty. Please enter a location: {Colors.RESET}")
    
    budget = input(f"{Colors.YELLOW}Enter your maximum budget (e.g., $2000): {Colors.RESET}")
    while not budget.strip():
        budget = input(f"{Colors.RED}Budget cannot be empty. Please enter your maximum budget: {Colors.RESET}")
    if not budget.startswith('$'):
        budget = f"${budget}"
    
    # Get optional inputs with defaults
    bedrooms = input(f"{Colors.YELLOW}Enter number of bedrooms (default: 1): {Colors.RESET}") or "1"
    
    amenities = input(f"{Colors.YELLOW}Enter desired amenities, separated by commas (e.g., gym,pool,parking): {Colors.RESET}") or ""
    
    return {
        "location": location.strip(),
        "budget": budget.strip(),
        "bedrooms": bedrooms.strip(),
        "amenities": amenities.strip()
    }

def build_search_query(user_prefs):
    amenities_str = f" with {user_prefs['amenities'].replace(',', ', ')}" if user_prefs['amenities'] else ""
    return f"{user_prefs['bedrooms']} bedroom apartments for rent in {user_prefs['location']} under {user_prefs['budget']}{amenities_str}"

def research_apartments(query: str) -> Dict[str, Any]:
    """Use Firecrawl's deep research to find apartment listings"""
    print(f"\n{Colors.BOLD}{Colors.CYAN}üîç INITIATING DEEP RESEARCH üîç{Colors.RESET}")
    print(f"{Colors.BLUE}Researching apartments with query: '{query}'{Colors.RESET}")
    print(f"{Colors.BLUE}This may take a few minutes...{Colors.RESET}\n")
    
    # Define research parameters
    params = {
        "maxDepth": 3,  # Number of research iterations
        "timeLimit": 180,  # Time limit in seconds
        "maxUrls": 20  # Maximum URLs to analyze
    }
    
    # Start research with real-time updates
    def on_activity(activity):
        activity_type = activity['type']
        message = activity['message']
        
        if activity_type == 'info':
            color = Colors.CYAN
        elif activity_type == 'search':
            color = Colors.BLUE
        elif activity_type == 'scrape':
            color = Colors.MAGENTA
        elif activity_type == 'analyze':
            color = Colors.GREEN
        else:
            color = Colors.RESET
            
        print(f"[{color}{activity_type}{Colors.RESET}] {message}")
    
    # Run deep research
    results = firecrawl.deep_research(
        query=query,
        params=params,
        on_activity=on_activity
    )
    
    return results

def analyze_with_claude(research_results: Dict[str, Any], user_prefs: Dict[str, str]) -> List[Dict[str, Any]]:
    """Use Claude to analyze apartment data and extract top options"""
    print(f"\n{Colors.BOLD}{Colors.MAGENTA}üß† ANALYZING RESULTS WITH CLAUDE 3.7 üß†{Colors.RESET}")
    
    # Extract relevant information from sources
    sources_text = "\n\n".join([
        f"Source {i+1}:\n{source.get('content', '')}"
        for i, source in enumerate(research_results['data']['sources'][:15])  # Limit to first 15 sources
    ])
    
    # Add the final analysis as an additional source
    final_analysis = research_results['data'].get('finalAnalysis', '')
    if final_analysis:
        sources_text += f"\n\nFinal Analysis:\n{final_analysis}"
    
    # Prepare system prompt with better handling for limited data
    system_prompt = """
    You are an expert apartment finder assistant. Your task is to analyze text about apartments and find the top apartment options that best match the user's preferences.
    
    If you find specific apartment listings with details, extract and organize them into exactly 3 options.
    
    For each listing you can identify, extract:
    1. Price (monthly rent)
    2. Location (specific neighborhood, address if available)
    3. Key features (bedrooms, bathrooms, square footage, type of building)
    4. Amenities (both in-unit and building amenities)
    5. Pros and cons (at least 3 of each)
    
    If you cannot find 3 complete listings with all details, do your best with the information available. You can:
    - Create fewer than 3 listings if that's all you can find
    - Extrapolate missing information based on similar listings or market trends
    - For missing specific details, use general information about the area
    
    You MUST format your response as a JSON array of objects. Each object should have these exact fields: 
    - title (string)
    - price (string)
    - location (string)
    - features (array of strings)
    - amenities (array of strings)
    - pros (array of strings)
    - cons (array of strings)
    
    If you absolutely cannot find any apartment listings with enough details, return an array with a single object containing general information about apartments in the area, with "No specific listings found" as the title.
    
    Example JSON structure:
    [
      {
        "title": "Luxury 2BR in Downtown",
        "price": "$2,500/month",
        "location": "123 Main St, Downtown",
        "features": ["2 bedrooms", "2 bathrooms", "950 sq ft"],
        "amenities": ["In-unit laundry", "Parking garage", "Fitness center"],
        "pros": ["Great location", "Modern appliances", "Pet friendly"],
        "cons": ["Street noise", "Small kitchen", "Limited storage"]
      }
    ]
    
    Return ONLY the JSON array, nothing else.
    """
    
    # Create the user message
    user_message = f"""
    I'm looking for {user_prefs['bedrooms']} bedroom apartments in {user_prefs['location']} with a budget of {user_prefs['budget']}.
    
    Additional preferences: {user_prefs.get('amenities', 'None specified')}
    
    Please analyze the following information and find apartment options that match my criteria:
    
    {sources_text}
    """
    
    # Call Claude API
    response = claude.messages.create(
        model="claude-3-7-sonnet-20250219",
        max_tokens=4000,
        temperature=0,
        system=system_prompt,
        messages=[{"role": "user", "content": user_message}]
    )
    
    # Extract and parse JSON from response with better error handling
    try:
        content = response.content[0].text
        
        # Clean the content - strip markdown formatting or text before/after JSON
        content = content.strip()
        if content.startswith('```json'):
            content = content[7:]
        if content.endswith('```'):
            content = content[:-3]
        content = content.strip()
        
        # Look for JSON array in the response
        if content.startswith('[') and content.endswith(']'):
            return json.loads(content)
        
        # Try to find JSON brackets if not properly formatted
        json_start = content.find('[')
        json_end = content.rfind(']') + 1
        if json_start >= 0 and json_end > json_start:
            json_str = content[json_start:json_end]
            return json.loads(json_str)
        
        # If we can't find JSON, create a fallback response
        print(f"{Colors.YELLOW}Could not find valid JSON in Claude's response, creating fallback response{Colors.RESET}")
        return [{
            "title": "No specific listings found",
            "price": f"Target: {user_prefs['budget']}",
            "location": user_prefs['location'],
            "features": [f"{user_prefs['bedrooms']} bedroom(s)"],
            "amenities": user_prefs['amenities'].split(',') if user_prefs['amenities'] else ["Not specified"],
            "pros": ["Information is based on general market research", "Consider visiting apartment listing websites directly", "Contact local real estate agents for current availability"],
            "cons": ["No specific listings were found in the research", "Prices and availability may vary", "Additional research recommended"]
        }]
    except Exception as e:
        print(f"{Colors.RED}Error parsing Claude's response: {e}{Colors.RESET}")
        print(f"{Colors.YELLOW}Creating fallback response{Colors.RESET}")
        return [{
            "title": "Error analyzing apartment listings",
            "price": f"Target: {user_prefs['budget']}",
            "location": user_prefs['location'],
            "features": [f"{user_prefs['bedrooms']} bedroom(s)"],
            "amenities": user_prefs['amenities'].split(',') if user_prefs['amenities'] else ["Not specified"],
            "pros": ["Try refining your search criteria", "Consider searching specific apartment websites", "Contact local real estate agents"],
            "cons": ["Search encountered technical difficulties", "Results may not be accurate", "Consider trying again later"]
        }]

def display_results(apartments: List[Dict[str, Any]]):
    """Display the top apartment options in a readable format"""
    if not apartments:
        print(f"{Colors.RED}No suitable apartments found that match your criteria.{Colors.RESET}")
        return
    
    print(f"\n{Colors.BOLD}{'=' * 80}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.GREEN}üè† TOP {len(apartments)} APARTMENT OPTIONS üè†{Colors.RESET}".center(80))
    print(f"{Colors.BOLD}{'=' * 80}{Colors.RESET}")
    
    for i, apt in enumerate(apartments):
        print(f"\n{Colors.BOLD}{Colors.CYAN}üîë OPTION {i+1}: {apt.get('title', 'Apartment')}{Colors.RESET}")
        print(f"{Colors.YELLOW}üí∞ Price: {apt.get('price', 'N/A')}{Colors.RESET}")
        print(f"{Colors.YELLOW}üìç Location: {apt.get('location', 'N/A')}{Colors.RESET}")
        
        print(f"\n{Colors.MAGENTA}üìã Features:{Colors.RESET}")
        for feature in apt.get('features', []):
            print(f"  {Colors.BLUE}‚Ä¢{Colors.RESET} {feature}")
        
        print(f"\n{Colors.MAGENTA}‚ú® Amenities:{Colors.RESET}")
        for amenity in apt.get('amenities', []):
            print(f"  {Colors.BLUE}‚Ä¢{Colors.RESET} {amenity}")
        
        print(f"\n{Colors.GREEN}üëç Pros:{Colors.RESET}")
        for pro in apt.get('pros', []):
            print(f"  {Colors.BLUE}‚Ä¢{Colors.RESET} {pro}")
        
        print(f"\n{Colors.RED}üëé Cons:{Colors.RESET}")
        for con in apt.get('cons', []):
            print(f"  {Colors.BLUE}‚Ä¢{Colors.RESET} {con}")
        
        print(f"\n{Colors.CYAN}{'-' * 80}{Colors.RESET}")

def main():
    # Get user preferences through interactive input
    user_prefs = get_user_preferences()
    
    # Print summary of search criteria
    print(f"\n{Colors.BOLD}{Colors.CYAN}=== Search Criteria ==={Colors.RESET}")
    print(f"{Colors.BLUE}Location: {Colors.YELLOW}{user_prefs['location']}{Colors.RESET}")
    print(f"{Colors.BLUE}Budget: {Colors.YELLOW}{user_prefs['budget']}{Colors.RESET}")
    print(f"{Colors.BLUE}Bedrooms: {Colors.YELLOW}{user_prefs['bedrooms']}{Colors.RESET}")
    print(f"{Colors.BLUE}Amenities: {Colors.YELLOW}{user_prefs['amenities'] or 'None specified'}{Colors.RESET}")
    
    # Build search query
    query = build_search_query(user_prefs)
    
    # Run research
    research_results = research_apartments(query)
    
    # Analyze with Claude
    top_apartments = analyze_with_claude(research_results, user_prefs)
    
    # Display results
    display_results(top_apartments)

if __name__ == "__main__":
    main()

================
File: examples/deep-research-apartment-finder/README.md
================
# Apartment Finder CLI

A command-line tool that uses Firecrawl's Deep Research API and Anthropic's Claude 3.7 to find and analyze apartment listings based on your preferences.

## Features

- Interactive input for apartment search preferences
- Searches apartments by location, budget, bedrooms, and amenities
- Automatically researches apartment listings across multiple websites
- Uses AI to analyze and extract the top 3 options
- Provides detailed information including price, location, features, and pros/cons
- Option to save results as JSON

## Installation

1. Clone this repository:

   ```
   git clone <repository-url>
   cd apartment-finder
   ```

2. Install dependencies:

   ```
   pip install -r requirements.txt
   ```

3. Set up API keys:
   - Copy `.env.example` to `.env`
   - Fill in your Firecrawl API key from [firecrawl.dev](https://firecrawl.dev)
   - Fill in your Anthropic API key from [console.anthropic.com](https://console.anthropic.com)

## Usage

Run the script and follow the interactive prompts:

```bash
python apartment_finder.py
```

The script will prompt you for:

- Location (city or neighborhood)
- Budget (maximum monthly rent)
- Number of bedrooms
- Desired amenities

After searching and analyzing, the tool will display the top apartment options and offer to save the results to a JSON file.

## Notes

- The search process may take a few minutes due to the deep research API.
- Results will vary based on available apartment listings at the time of search.
- API usage may incur costs depending on your Firecrawl and Anthropic subscription plans.

================
File: examples/deep-research-apartment-finder/requirements.txt
================
anthropic==0.18.0
firecrawl==0.2.0
python-dotenv==1.0.0

================
File: examples/deepseek-v3-company-researcher/.gitignore
================
# Environment variables
.env
.env.*

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
ENV/
env/

# Editor files
.idea/
.vscode/
*.swp
*.swo
*~

# OS specific files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Logs
*.log
logs/

================
File: examples/deepseek-v3-company-researcher/deepseek-v3-extract.py
================
import os
import json
import time
import requests
from dotenv import load_dotenv
from serpapi.google_search import GoogleSearch
from openai import OpenAI

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Initialize clients
openrouter_api_key = os.getenv("OPENROUTER_API_KEY")
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
serp_api_key = os.getenv("SERP_API_KEY")

if not openrouter_api_key:
    print(f"{Colors.RED}Warning: OPENROUTER_API_KEY not found in environment variables{Colors.RESET}")
if not firecrawl_api_key:
    print(f"{Colors.RED}Warning: FIRECRAWL_API_KEY not found in environment variables{Colors.RESET}")
if not serp_api_key:
    print(f"{Colors.RED}Warning: SERP_API_KEY not found in environment variables{Colors.RESET}")

# Initialize OpenRouter client
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=openrouter_api_key
)

def clean_url(url):
    """Clean a URL by removing tracking parameters and unnecessary query strings."""
    if not isinstance(url, str):
        return None
    
    # Remove any query parameters
    base_url = url.split('?')[0]
    
    # Remove trailing slashes and cleanup
    cleaned = base_url.rstrip('/')
    cleaned = cleaned.replace('/*', '')
    
    # Ensure it's a valid http(s) URL
    if not cleaned.startswith(('http://', 'https://')):
        return None
        
    return cleaned

def search_google(query):
    """Search Google using SerpAPI and return top results."""
    print(f"{Colors.YELLOW}Searching Google for '{query}'...{Colors.RESET}")
    search = GoogleSearch({"q": query, "api_key": serp_api_key})
    return search.get_dict().get("organic_results", [])

def select_urls_with_claude(company, objective, serp_results):
    """
    Use Claude 3.7 Sonnet to select URLs from SERP results.
    Returns a list of URLs.
    """
    try:
        serp_data = [{"title": r.get("title"), "link": r.get("link"), "snippet": r.get("snippet")} 
                     for r in serp_results if r.get("link")]

        prompt = (
            "Task: Select relevant URLs from search results.\n\n"
            "Instructions:\n"
            "1. Analyze the search results for information about the specified company\n"
            "2. Select URLs that are most likely to contain the requested information\n"
            "3. Return ONLY a JSON object with the following structure: {\"selected_urls\": [\"url1\", \"url2\"]}\n"
            "4. Do not include social media links\n\n"
            f"Company: {company}\n"
            f"Information Needed: {objective}\n"
            f"Search Results: {json.dumps(serp_data, indent=2)}\n\n"
            "Response Format: {\"selected_urls\": [\"https://example.com\", \"https://example2.com\"]}"
        )

        # Using OpenRouter's API to select URLs
        response = client.chat.completions.create(
            model="deepseek/deepseek-chat-v3-0324:free",
            messages=[{
                "role": "system",
                "content": "You are a URL selection assistant. Your task is to select relevant URLs from search results. You MUST return a valid JSON object containing at least one URL."
            }, {
                "role": "user",
                "content": prompt
            }]
        )
        
        result = response.choices[0].message.content.strip()
        
        # Clean the response text
        if result.startswith('```'):
            result = result.split('```')[1]
            if result.startswith('json'):
                result = result[4:]
        result = result.strip()

        try:
            # Parse JSON response
            parsed_result = json.loads(result)
            if isinstance(parsed_result, dict) and "selected_urls" in parsed_result:
                urls = parsed_result["selected_urls"]
            else:
                # Fallback to text parsing
                urls = [line.strip() for line in result.split('\n') 
                       if line.strip().startswith(('http://', 'https://'))]
        except json.JSONDecodeError:
            # Fallback to text parsing
            urls = [line.strip() for line in result.split('\n') 
                   if line.strip().startswith(('http://', 'https://'))]

        # Clean up URLs
        cleaned_urls = [url.replace('/*', '').rstrip('/') for url in urls]
        cleaned_urls = [url for url in cleaned_urls if url]

        if not cleaned_urls:
            print(f"{Colors.YELLOW}No valid URLs found in response.{Colors.RESET}")
            return []

        print(f"{Colors.CYAN}Selected URLs for extraction:{Colors.RESET}")
        for url in cleaned_urls:
            print(f"- {url}")

        return cleaned_urls

    except Exception as e:
        print(f"{Colors.RED}Error selecting URLs: {str(e)}{Colors.RESET}")
        return []

def extract_company_info(urls, prompt, company, api_key):
    """Use requests to call Firecrawl's extract endpoint with selected URLs."""
    print(f"{Colors.YELLOW}Extracting structured data from the provided URLs using Firecrawl...{Colors.RESET}")
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    payload = {
        "urls": urls,
        "prompt": prompt + " for " + company,
        "enableWebSearch": True
    }
    
    try:
        response = requests.post(
            "https://api.firecrawl.dev/v1/extract",
            headers=headers,
            json=payload,
            timeout=30
        )
        
        data = response.json()
        
        if not data.get('success'):
            print(f"{Colors.RED}API returned error: {data.get('error', 'No error message')}{Colors.RESET}")
            return None
        
        extraction_id = data.get('id')
        if not extraction_id:
            print(f"{Colors.RED}No extraction ID found in response.{Colors.RESET}")
            return None

        return poll_firecrawl_result(extraction_id, api_key)

    except requests.exceptions.RequestException as e:
        print(f"{Colors.RED}Request failed: {e}{Colors.RESET}")
        return None
    except json.JSONDecodeError as e:
        print(f"{Colors.RED}Failed to parse response: {e}{Colors.RESET}")
        return None
    except Exception as e:
        print(f"{Colors.RED}Failed to extract data: {e}{Colors.RESET}")
        return None

def poll_firecrawl_result(extraction_id, api_key, interval=10, max_attempts=60):
    """Poll Firecrawl API to get the extraction result."""
    url = f"https://api.firecrawl.dev/v1/extract/{extraction_id}"
    headers = {
        'Authorization': f'Bearer {api_key}'
    }

    print(f"{Colors.YELLOW}Waiting for extraction to complete...{Colors.RESET}")
    
    for attempt in range(1, max_attempts + 1):
        try:
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()

            if data.get('success') and data.get('data'):
                print(f"{Colors.GREEN}Data successfully extracted:{Colors.RESET}")
                print(json.dumps(data['data'], indent=2))
                return data['data']
            elif data.get('success') and not data.get('data'):
                if attempt % 6 == 0:  
                    print(f"{Colors.YELLOW}Still processing... (attempt {attempt}/{max_attempts}){Colors.RESET}")
                time.sleep(interval)
            else:
                print(f"{Colors.RED}API Error: {data.get('error', 'No error message provided')}{Colors.RESET}")
                return None

        except requests.exceptions.RequestException as e:
            print(f"{Colors.RED}Request error: {str(e)}{Colors.RESET}")
            return None
        except json.JSONDecodeError as e:
            print(f"{Colors.RED}JSON parsing error: {str(e)}{Colors.RESET}")
            return None
        except Exception as e:
            print(f"{Colors.RED}Unexpected error: {str(e)}{Colors.RESET}")
            return None

    print(f"{Colors.RED}Max polling attempts reached. Extraction did not complete in time.{Colors.RESET}")
    return None

def main():
    company = input(f"{Colors.BLUE}Enter the company name: {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter what information you want about the company: {Colors.RESET}")
    
    serp_results = search_google(f"{company}")
    if not serp_results:
        print(f"{Colors.RED}No search results found.{Colors.RESET}")
        return
    
    selected_urls = select_urls_with_claude(company, objective, serp_results)
    
    if not selected_urls:
        print(f"{Colors.RED}No URLs were selected.{Colors.RESET}")
        return
    
    data = extract_company_info(selected_urls, objective, company, firecrawl_api_key)
    
    if data:
        print(f"{Colors.GREEN}Extraction completed successfully.{Colors.RESET}")
    else:
        print(f"{Colors.RED}Failed to extract the requested information. Try refining your prompt or choosing a different company.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/deepseek-v3-company-researcher/README.md
================
# DeepSeek V3 Company Researcher

This tool is a powerful company research assistant that combines Google search, DeepSeek Chat V3, and Firecrawl to gather and analyze company information automatically.

## Features

- Automated Google search using SerpAPI
- Intelligent URL selection using DeepSeek Chat V3
- Structured data extraction using Firecrawl
- Real-time progress monitoring and colorized output
- Automated handling of rate limits and polling

## Prerequisites

- Python 3.7+
- API keys for:
  - OpenRouter (for DeepSeek Chat V3 access)
  - Firecrawl
  - SerpAPI

## Setup

1. Clone the repository
2. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Create a `.env` file in the project root with your API keys:
   ```
   OPENROUTER_API_KEY=your_openrouter_api_key
   FIRECRAWL_API_KEY=your_firecrawl_api_key
   SERP_API_KEY=your_serpapi_key
   ```

## Usage

Run the script:

```bash
python deepseek-v3-extract.py
```

Follow the interactive prompts to:

1. Enter the company name you want to research
2. Specify what information you want to gather about the company

The tool will:

- Search for relevant company information
- Select the most promising URLs
- Extract structured data from those URLs
- Present the findings in a clear, formatted output

## Output

The script provides real-time feedback with color-coded status messages:

- üîµ Blue: User prompts
- üü° Yellow: Processing status
- üü¢ Green: Success messages
- üî¥ Red: Error messages
- üü£ Magenta: Special notifications
- üîÖ Cyan: URL selections

## Error Handling

The script includes comprehensive error handling for:

- API failures
- Network issues
- Invalid responses
- Timeout scenarios

## License

MIT License

## Contributing

Feel free to open issues or submit pull requests with improvements.

================
File: examples/deepseek-v3-company-researcher/requirements.txt
================
python-dotenv>=1.0.0
requests>=2.31.0
openai>=1.12.0
google-search-results>=2.4.2
serpapi>=0.1.5

================
File: examples/deepseek-v3-crawler/.gitignore
================
# Environment variables
.env
.env.*

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
ENV/
env/

# Editor files
.idea/
.vscode/
*.swp
*.swo
*~

# OS specific files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Logs
*.log
logs/

================
File: examples/deepseek-v3-crawler/deepseek-v3-crawler.py
================
import os
from firecrawl import FirecrawlApp
import json
from dotenv import load_dotenv
from openai import OpenAI

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
openrouter_api_key = os.getenv("OPENROUTER_API_KEY")

# Initialize the FirecrawlApp and OpenRouter client
app = FirecrawlApp(api_key=firecrawl_api_key)
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=openrouter_api_key
)

def main():
    try:
        # Test the model availability first
        test_response = client.chat.completions.create(
            model="deepseek/deepseek-chat-v3-0324:free",
            messages=[{"role": "user", "content": "test"}]
        )
    except Exception as e:
        print(f"{Colors.RED}Error: Could not connect to the language model. Please try again later.{Colors.RESET}")
        print(f"{Colors.RED}Details: {str(e)}{Colors.RESET}")
        return

    url = input(f"{Colors.BLUE}Enter the website to crawl: {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter your objective: {Colors.RESET}")
    
    print(f"{Colors.YELLOW}Initiating web crawling process...{Colors.RESET}")
    
    relevant_pages = find_relevant_page_via_map(objective, url, app, client)
    
    if not relevant_pages:
        print(f"{Colors.RED}No relevant pages found. Exiting...{Colors.RESET}")
        return
    
    result = find_objective_in_top_pages(relevant_pages, objective, app, client)
    
    if result:
        print(f"{Colors.GREEN}Objective successfully found! Extracted information:{Colors.RESET}")
        print(json.dumps(result, indent=2))
    else:
        print(f"{Colors.RED}Objective could not be fulfilled.{Colors.RESET}")

def find_relevant_page_via_map(objective, url, app, client):
    try:
        print(f"{Colors.CYAN}Understood. Objective: {objective}{Colors.RESET}")
        print(f"{Colors.CYAN}Searching website: {url}{Colors.RESET}")
        
        map_prompt = f"""
        The map function generates a list of URLs from a website and it accepts a search parameter. Based on the objective of: {objective}, come up with a 1-2 word search parameter that will help us find the information we need. Only respond with 1-2 words nothing else.
        """

       
        response = client.chat.completions.create(
            model="deepseek/deepseek-chat-v3-0324:free",
            messages=[{"role": "user", "content": map_prompt}]
        )
        map_search_parameter = response.choices[0].message.content.strip()

        print(f"{Colors.GREEN}Optimal search parameter identified: {map_search_parameter}{Colors.RESET}")

        map_website = app.map_url(url, params={"search": map_search_parameter})
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")
        
        links = map_website.get('urls', []) or map_website.get('links', [])
        
        if not links:
            print(f"{Colors.RED}No links found in map response.{Colors.RESET}")
            return None

        return links
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered: {str(e)}{Colors.RESET}")
        return None

def find_objective_in_top_pages(pages, objective, app, client):
    try:
        for link in pages[:3]:
            print(f"{Colors.YELLOW}Scraping page: {link}{Colors.RESET}")
            scrape_result = app.scrape_url(link, params={'formats': ['markdown']})
            
            check_prompt = f"""
            Given the following scraped content and objective, determine if the objective is met.
            If it is, extract the relevant information in a simple JSON format. 
            If the objective is not met, respond with exactly 'Objective not met'.

            The JSON format should be:
            {{
                "found": true,
                "data": {{
                    // extracted information here
                }}
            }}

            Important: Do not wrap the JSON in markdown code blocks. Just return the raw JSON.

            Objective: {objective}
            Scraped content: {scrape_result['markdown']}
            """
        
            # Using OpenRouter's API to analyze the content
            response = client.chat.completions.create(
                model="deepseek/deepseek-chat-v3-0324:free",
                messages=[{
                    "role": "system",
                    "content": "You are a helpful assistant that extracts information from web pages. Always respond in valid JSON format when information is found. Do not wrap the JSON in markdown code blocks."
                }, {
                    "role": "user",
                    "content": check_prompt
                }]
            )
            result = response.choices[0].message.content.strip()
            
            print(f"{Colors.CYAN}Model response: {result}{Colors.RESET}")  # Debug output
            
            if result == "Objective not met":
                print(f"{Colors.YELLOW}Objective not met in this page, continuing search...{Colors.RESET}")
                continue
            
            try:
                # Clean up the response if it's wrapped in code blocks
                if result.startswith('```'):
                    result = result.split('```')[1]
                    if result.startswith('json'):
                        result = result[4:]
                result = result.strip()
                
                parsed_result = json.loads(result)
                if isinstance(parsed_result, dict) and parsed_result.get('found'):
                    return parsed_result.get('data')
                else:
                    print(f"{Colors.YELLOW}Invalid response format, continuing search...{Colors.RESET}")
            except json.JSONDecodeError as e:
                print(f"{Colors.RED}Error parsing JSON response: {str(e)}{Colors.RESET}")
                print(f"{Colors.RED}Raw response: {result}{Colors.RESET}")
                continue
        
        return None
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered: {str(e)}{Colors.RESET}")
        return None

if __name__ == "__main__":
    main()

================
File: examples/deepseek-v3-crawler/README.md
================
# DeepSeek V3 Web Crawler

This script uses the DeepSeek V3 large language model (via Hugging Face's Inference API) and FireCrawl to crawl websites based on specific objectives.

## Prerequisites

- Python 3.8+
- A FireCrawl API key (get one at [FireCrawl's website](https://firecrawl.app))
- A Hugging Face API key with access to inference API

## Installation

1. Clone this repository:

```bash
git clone <repository-url>
cd <repository-directory>
```

2. Install the required packages:

```bash
pip install -r requirements.txt
```

3. Create a `.env` file in the root directory with your API keys:

```
FIRECRAWL_API_KEY=your_firecrawl_api_key
HUGGINGFACE_API_KEY=your_huggingface_api_key
```

## Usage

Run the script:

```bash
python deepseek-v3-crawler.py
```

The script will prompt you to:

1. Enter a website URL to crawl
2. Enter your objective (what information you're looking for)

The script will then:

- Use DeepSeek V3 to generate optimal search parameters for the website
- Map the website to find relevant pages
- Crawl the most relevant pages to extract information based on your objective
- Output the results in JSON format if successful

## Example

Input:

- Website: https://www.example.com
- Objective: Find information about their pricing plans

Output:

- The script will output structured JSON data containing the pricing information found on the website.

## Notes

- The script uses DeepSeek V3, an advanced language model, to analyze web content.
- The model is accessed via Hugging Face's Inference API.
- You may need to adjust temperature or max_new_tokens parameters in the script based on your needs.

================
File: examples/deepseek-v3-crawler/requirements.txt
================
firecrawl==1.13.5
python-dotenv==1.0.1
huggingface-hub>=0.20.0

================
File: examples/find_internal_link_opportunites/find_internal_link_opportunites.ipynb
================
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "from firecrawl import FirecrawlApp\n",
    "import json\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\") or \"\"\n",
    "firecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\") or \"\"\n",
    "# Set variables\n",
    "blog_url=\"https://mendable.ai/blog\"\n",
    "\n",
    "# Set up anthropic client\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=anthropic_api_key,\n",
    ")\n",
    "\n",
    "# Initialize the FirecrawlApp with your API key\n",
    "app = FirecrawlApp(api_key=firecrawl_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl a website\n",
    "params = {\n",
    "    'crawlOptions': {\n",
    "        'limit': 100\n",
    "    },\n",
    "    \"pageOptions\": {\n",
    "        \"onlyMainContent\": True\n",
    "    }\n",
    "}\n",
    "crawl_result = app.crawl_url(blog_url, params=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting potential links from crawl_result:\n",
      "Collected 36 potential links:\n",
      "URL: https://mendable.ai/blog/coachgtm-mongodb, Title: Meet MongoDBs CoachGTM.ai\n",
      "URL: https://mendable.ai/blog/building-safe-rag, Title: Building Safe RAG systems with the LLM OWASP top 10\n",
      "URL: https://mendable.ai/blog/gdpr-repository-pattern, Title: Navigating the Maze of GDPR Compliance: A Codebase Transformation\n",
      "URL: https://mendable.ai/blog/how-mendable-leverages-langsmith-to-debug-tools-and-actions, Title: How Mendable leverages Langsmith to debug Tools & Actions\n",
      "URL: https://mendable.ai/blog/european-data-storage, Title: Launching European Data Storage powered by MongoDB\n",
      "URL: https://mendable.ai/blog/tools, Title: Introducing Tools and Actions\n",
      "URL: https://mendable.ai/blog/december_update, Title: Mendable.ai December Recap\n",
      "URL: https://mendable.ai/blog/november_update, Title: Mendable.ai November Update\n",
      "URL: https://mendable.ai/blog/october-recap, Title: Mendable.ai October Recap\n",
      "URL: https://mendable.ai/blog/midseptemberupdate, Title: Mendable.ai Mid September 2023 Update\n",
      "URL: https://mendable.ai/blog/getting-started, Title: Everything you need to know about Mendable: Build and deploy AI Chat Search\n",
      "URL: https://mendable.ai/blog/building-copilots, Title: Building context-aware AI copilots with Mendable\n",
      "URL: https://mendable.ai/blog/august2023update, Title: Mendable.ai August 2023 Updates\n",
      "URL: https://mendable.ai/blog/finetuning-gpt35, Title: Early Insights Fine-Tuning GPT 3.5 from Mendable.ai\n",
      "URL: https://mendable.ai/blog/gpt35prompting, Title: Improving GPT-3.5, Insights from Mendable.ai\n",
      "URL: https://mendable.ai/blog/precisemode, Title: Introducing Precise Mode for Mendable.ai\n",
      "URL: https://mendable.ai/blog/customprompt, Title: Customizing Your LLM Model on Mendable.ai\n",
      "URL: https://mendable.ai/blog/mendable-launch, Title: Introducing Mendable.ai\n",
      "URL: https://mendable.ai/blog/european-data-storage, Title: Launching European Data Storage powered by MongoDB\n",
      "URL: https://mendable.ai/blog/customprompt, Title: Customizing Your LLM Model on Mendable.ai\n",
      "URL: https://mendable.ai/blog/precisemode, Title: Introducing Precise Mode for Mendable.ai\n",
      "URL: https://mendable.ai/blog/building-copilots, Title: Building context-aware AI copilots with Mendable\n",
      "URL: https://mendable.ai/blog/coachgtm-mongodb, Title: Meet MongoDBs CoachGTM.ai\n",
      "URL: https://mendable.ai/blog/building-safe-rag, Title: Building Safe RAG systems with the LLM OWASP top 10\n",
      "URL: https://mendable.ai/blog/gdpr-repository-pattern, Title: Navigating the Maze of GDPR Compliance: A Codebase Transformation\n",
      "URL: https://mendable.ai/blog/how-mendable-leverages-langsmith-to-debug-tools-and-actions, Title: How Mendable leverages Langsmith to debug Tools & Actions\n",
      "URL: https://mendable.ai/blog/tools, Title: Introducing Tools and Actions\n",
      "URL: https://mendable.ai/blog/december_update, Title: Mendable.ai December Recap\n",
      "URL: https://mendable.ai/blog/november_update, Title: Mendable.ai November Update\n",
      "URL: https://mendable.ai/blog/october-recap, Title: Mendable.ai October Recap\n",
      "URL: https://mendable.ai/blog/midseptemberupdate, Title: Mendable.ai Mid September 2023 Update\n",
      "URL: https://mendable.ai/blog/getting-started, Title: Everything you need to know about Mendable: Build and deploy AI Chat Search\n",
      "URL: https://mendable.ai/blog/august2023update, Title: Mendable.ai August 2023 Updates\n",
      "URL: https://mendable.ai/blog/finetuning-gpt35, Title: Early Insights Fine-Tuning GPT 3.5 from Mendable.ai\n",
      "URL: https://mendable.ai/blog/gpt35prompting, Title: Improving GPT-3.5, Insights from Mendable.ai\n",
      "URL: https://mendable.ai/blog/mendable-launch, Title: Introducing Mendable.ai\n"
     ]
    }
   ],
   "source": [
    "potential_links = []\n",
    "\n",
    "if crawl_result:\n",
    "    print(\"Collecting potential links from crawl_result:\")\n",
    "    \n",
    "    for item in crawl_result:\n",
    "        metadata = item[\"metadata\"]\n",
    "        og_url = metadata.get(\"ogUrl\")\n",
    "        title = metadata.get(\"title\")\n",
    "        if og_url and title and og_url != blog_url:\n",
    "            potential_links.append({\"url\": og_url, \"title\": title})\n",
    "    \n",
    "    print(f\"Collected {len(potential_links)} potential links:\")\n",
    "    for link in potential_links:\n",
    "        print(f\"URL: {link['url']}, Title: {link['title']}\")\n",
    "        \n",
    "else:\n",
    "    print(\"crawl_result is empty or None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggestion for: Meet MongoDBs CoachGTM.ai\n",
      "Blog phrase: Mendable also provides a Tools\n",
      "Internal Link: https://mendable.ai/blog/tools\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Meet MongoDBs CoachGTM.ai\n",
      "Blog phrase: MongoDB Atlas Vector Search to\n",
      "Internal Link: https://mendable.ai/blog/european-data-storage\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Meet MongoDBs CoachGTM.ai\n",
      "Blog phrase: By harnessing the power of\n",
      "Internal Link: https://mendable.ai/blog/building-copilots\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Building Safe RAG systems with the LLM OWASP top 10\n",
      "Blog phrase: Advantages of RAG\n",
      "Internal Link: https://mendable.ai/blog/building-copilots\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Building Safe RAG systems with the LLM OWASP top 10\n",
      "Blog phrase: Bring Your Model\n",
      "Internal Link: https://mendable.ai/blog/customprompt\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Building Safe RAG systems with the LLM OWASP top 10\n",
      "Blog phrase: Garbage in, Garbage out\n",
      "Internal Link: https://mendable.ai/blog/precisemode\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Navigating the Maze of GDPR Compliance: A Codebase Transformation\n",
      "Blog phrase: European data storage\n",
      "Internal Link: https://mendable.ai/blog/european-data-storage\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Navigating the Maze of GDPR Compliance: A Codebase Transformation\n",
      "Blog phrase: delivering value\n",
      "Internal Link: https://mendable.ai/blog/getting-started\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: How Mendable leverages Langsmith to debug Tools & Actions\n",
      "Blog phrase: introduction of Tools & Actions\n",
      "Internal Link: https://mendable.ai/blog/tools\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: How Mendable leverages Langsmith to debug Tools & Actions\n",
      "Blog phrase: Mendable Tools & Actions\n",
      "Internal Link: https://mendable.ai/blog/tools\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Launching European Data Storage powered by MongoDB\n",
      "Blog phrase: Clean Architecture and Repository pattern\n",
      "Internal Link: https://mendable.ai/blog/gdpr-repository-pattern\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Launching European Data Storage powered by MongoDB\n",
      "Blog phrase: building the best AI Chat\n",
      "Internal Link: https://mendable.ai/blog/building-copilots\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Launching European Data Storage powered by MongoDB\n",
      "Blog phrase: European RAG pipeline, powered by\n",
      "Internal Link: https://mendable.ai/blog/building-safe-rag\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Introducing Tools and Actions\n",
      "Blog phrase: augmentation and actions for automation\n",
      "Internal Link: https://mendable.ai/blog/building-copilots\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Introducing Tools and Actions\n",
      "Blog phrase: Mendable provides an API request\n",
      "Internal Link: https://mendable.ai/blog/getting-started\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Introducing Tools and Actions\n",
      "Blog phrase: AI use it when it\n",
      "Internal Link: https://mendable.ai/blog/how-mendable-leverages-langsmith-to-debug-tools-and-actions\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Mendable.ai December Recap\n",
      "Blog phrase: customizing the model\n",
      "Internal Link: https://mendable.ai/blog/customprompt\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Mendable.ai December Recap\n",
      "Blog phrase: AI sales copilot\n",
      "Internal Link: https://mendable.ai/blog/building-copilots\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Mendable.ai December Recap\n",
      "Blog phrase: Introducing Tools and Actions\n",
      "Internal Link: https://mendable.ai/blog/tools\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Mendable.ai November Update\n",
      "Blog phrase: Auto syncing data sources\n",
      "Internal Link: https://mendable.ai/blog/european-data-storage\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Mendable.ai November Update\n",
      "Blog phrase: Chat insights feature\n",
      "Internal Link: https://mendable.ai/blog/tools\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Mendable.ai November Update\n",
      "Blog phrase: Github private repo support\n",
      "Internal Link: https://mendable.ai/blog/getting-started\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Mendable.ai October Recap\n",
      "Blog phrase: Full Prompt Customization\n",
      "Internal Link: https://mendable.ai/blog/customprompt\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Mendable.ai October Recap\n",
      "Blog phrase: Expanded Model Support\n",
      "Internal Link: https://mendable.ai/blog/gpt35prompting\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Mendable.ai October Recap\n",
      "Blog phrase: AI-Powered Documentation Management\n",
      "Internal Link: https://mendable.ai/blog/building-copilots\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Mendable.ai Mid September 2023 Update\n",
      "Blog phrase: new integration templates\n",
      "Internal Link: https://mendable.ai/blog/tools\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Mendable.ai Mid September 2023 Update\n",
      "Blog phrase: Product Copilot feature\n",
      "Internal Link: https://mendable.ai/blog/building-copilots\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Mendable.ai Mid September 2023 Update\n",
      "Blog phrase: Data Exporting\n",
      "Internal Link: https://mendable.ai/blog/getting-started\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Everything you need to know about Mendable: Build and deploy AI Chat Search\n",
      "Blog phrase: robust API\n",
      "Internal Link: https://mendable.ai/blog/tools\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Everything you need to know about Mendable: Build and deploy AI Chat Search\n",
      "Blog phrase: pre-built components\n",
      "Internal Link: https://mendable.ai/blog/building-copilots\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Everything you need to know about Mendable: Build and deploy AI Chat Search\n",
      "Blog phrase: Customizing Your LLM Model\n",
      "Internal Link: https://mendable.ai/blog/customprompt\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Building context-aware AI copilots with Mendable\n",
      "Blog phrase: registered on our platform\n",
      "Internal Link: https://mendable.ai/blog/getting-started\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Building context-aware AI copilots with Mendable\n",
      "Blog phrase: dynamic context to the AI\n",
      "Internal Link: https://mendable.ai/blog/customprompt\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Building context-aware AI copilots with Mendable\n",
      "Blog phrase: personalized answers to your users\n",
      "Internal Link: https://mendable.ai/blog/precisemode\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Mendable.ai August 2023 Updates\n",
      "Blog phrase: Learn more about how to\n",
      "Internal Link: https://mendable.ai/blog/precisemode\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Mendable.ai August 2023 Updates\n",
      "Blog phrase: Building context-aware AI copilots with\n",
      "Internal Link: https://mendable.ai/blog/building-copilots\n",
      "---\n",
      "\n",
      "\n",
      "Suggestion for: Mendable.ai August 2023 Updates\n",
      "Blog phrase: customizable AI chat components\n",
      "Internal Link: https://mendable.ai/blog/getting-started\n",
      "---\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 57\u001b[0m\n\u001b[1;32m     27\u001b[0m prompt_instructions \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mGiven this blog post from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_blog_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m called \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_blog_title\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, analyze the following blog content. Identify 0 to 3 of phrases (5 words max) from the <blog_content> inside of the middle of the article that could be linked to other blog posts from the list of potential links provided inside of <potential_links>. Return a JSON object structured as follows:\u001b[39m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;124mGO AND ONLY RETURN THE JSON NOTHING ELSE:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclaude-3-5-sonnet-20240620\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_instructions\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Extract the JSON string from the TextBlock\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     json_string \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/anthropic/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/anthropic/resources/messages.py:904\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m,\n\u001b[1;32m    903\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Message \u001b[38;5;241m|\u001b[39m Stream[RawMessageStreamEvent]:\n\u001b[0;32m--> 904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/anthropic/_base_client.py:1249\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1237\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1244\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1245\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1246\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1247\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1248\u001b[0m     )\n\u001b[0;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/anthropic/_base_client.py:931\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    924\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    929\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    930\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/anthropic/_base_client.py:962\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    959\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 962\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    968\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/httpx/_client.py:901\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    893\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    897\u001b[0m )\n\u001b[1;32m    899\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 901\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/httpx/_client.py:929\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    926\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 929\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/httpx/_client.py:966\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    964\u001b[0m     hook(request)\n\u001b[0;32m--> 966\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/httpx/_client.py:1002\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1002\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1006\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/httpx/_transports/default.py:228\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    216\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    217\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    226\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 228\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    233\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    234\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    235\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    236\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    237\u001b[0m )\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/ssl.py:1292\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1291\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/projects/python_projects/agents_testing/.conda/lib/python3.10/ssl.py:1165\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Assuming we have the following variables from the previous code:\n",
    "# crawl_result, client, potential_links\n",
    "\n",
    "# Convert potential_links to a JSON string\n",
    "potential_links_json = json.dumps(potential_links, indent=2)\n",
    "\n",
    "# Prepare CSV file\n",
    "csv_filename = \"link_suggestions.csv\"\n",
    "csv_headers = [\"Source Blog Title\", \"Source Blog URL\", \"Target Phrase\", \"Suggested Link URL\"]\n",
    "\n",
    "# Write headers to the CSV file\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(csv_headers)\n",
    "\n",
    "# Loop through each blog post content\n",
    "for item in crawl_result:\n",
    "    current_blog_url = item[\"metadata\"].get(\"ogUrl\", \"\")\n",
    "    if current_blog_url == blog_url:\n",
    "        continue\n",
    "    current_blog_content = item[\"content\"]\n",
    "    current_blog_title = item[\"metadata\"].get(\"title\", \"\")\n",
    "\n",
    "    prompt_instructions = f\"\"\"Given this blog post from {current_blog_url} called '{current_blog_title}', analyze the following blog content. Identify 0 to 3 of phrases (5 words max) from the <blog_content> inside of the middle of the article that could be linked to other blog posts from the list of potential links provided inside of <potential_links>. Return a JSON object structured as follows:\n",
    "\n",
    "    {{\n",
    "      \"link_suggestions\": [\n",
    "        {{\n",
    "          \"target_phrase\": \"the EXACT phrase from the <blog_content> to be linked to one of the links in <potential_links> (5 words max)\",\n",
    "          \"suggested_link_url\": \"url of the suggested internal link from <potential_links>\",\n",
    "        }}\n",
    "      ],\n",
    "      \"metadata\": {{\n",
    "        \"source_blog_url\": \"{current_blog_url}\",\n",
    "        \"source_blog_title\": \"{current_blog_title}\",\n",
    "      }}\n",
    "    }}\n",
    "\n",
    "    Ensure that you provide the EXACT phrase from <blog_content> in target_phrase (5 words max) to locate each suggestion in the blog content without using character positions. Your target phrases must NOT be a title!\n",
    "\n",
    "    Blog Content:\n",
    "    <blog_content>\n",
    "    {current_blog_content}\n",
    "    </blog_content>\n",
    "\n",
    "    Potential Links:\n",
    "    <potential_links>\n",
    "    {potential_links_json}\n",
    "    </potential_links>\n",
    "\n",
    "    GO AND ONLY RETURN THE JSON NOTHING ELSE:\"\"\"\n",
    "\n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20240620\",\n",
    "            max_tokens=1024,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt_instructions}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract the JSON string from the TextBlock\n",
    "        json_string = message.content[0].text\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        response_json = json.loads(json_string)\n",
    "        \n",
    "        # Write suggestions to CSV\n",
    "        for suggestion in response_json['link_suggestions']:\n",
    "            print(\"Suggestion for: \" + current_blog_title )\n",
    "            print(\"Blog phrase: \" + suggestion['target_phrase']) \n",
    "            print(\"Internal Link: \" + suggestion['suggested_link_url'])\n",
    "            print(\"---\\n\\n\")\n",
    "\n",
    "            # Open the CSV file in append mode and write the new row\n",
    "            with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                csvwriter.writerow([\n",
    "                    response_json['metadata']['source_blog_title'],\n",
    "                    response_json['metadata']['source_blog_url'],\n",
    "                    suggestion['target_phrase'],\n",
    "                    suggestion['suggested_link_url'],\n",
    "                ])\n",
    "      \n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error parsing JSON response for blog {current_blog_title}\")\n",
    "        print(\"Raw response:\", message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing blog {current_blog_title}: {str(e)}\")\n",
    "    \n",
    "\n",
    "print(f\"Finished processing all blog posts. Results saved to {csv_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

================
File: examples/full_example_apps/README.md
================
Full examples apps built with Firecrawl can be found at this repo: https://github.com/mendableai/firecrawl-app-examples

================
File: examples/gemini-2.0-crawler/gemini-2.0-crawler.py
================
import os
from firecrawl import FirecrawlApp
import json
import re
import requests
from requests.exceptions import RequestException
from dotenv import load_dotenv
import google.genai as genai
# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
gemini_api_key = os.getenv("GEMINI_API_KEY")

# Initialize the FirecrawlApp and Gemini client
app = FirecrawlApp(api_key=firecrawl_api_key)
client = genai.Client(api_key=gemini_api_key)  # Create Gemini client
model_name = "gemini-2.0-flash"
types = genai.types

# ANSI color codes


class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'


def pdf_size_in_mb(data: bytes) -> float:
    """Utility function to estimate PDF size in MB from raw bytes."""
    return len(data) / (1024 * 1024)


def gemini_extract_pdf_content(pdf_url, objective):
    """
    Downloads a PDF from pdf_url, then calls Gemini to extract text.
    Returns a string with the extracted text only.
    """
    try:
        pdf_data = requests.get(pdf_url, timeout=15).content
        size_mb = pdf_size_in_mb(pdf_data)
        if size_mb > 15:
            print(
                f"{Colors.YELLOW}Warning: PDF size is {size_mb} MB. Skipping PDF extraction.{Colors.RESET}")
            return ""

        prompt = f"""
        The objective is: {objective}.
        From this PDF, extract only the text that helps address this objective.
        If it contains no relevant info, return an empty string.
        """
        response = client.models.generate_content(
            model=model_name,
            contents=[
                types.Part.from_bytes(
                    data=pdf_data, mime_type="application/pdf"),
                prompt
            ]
        )
        return response.text.strip()
    except Exception as e:
        print(f"Error using Gemini to process PDF '{pdf_url}': {str(e)}")
        return ""


def gemini_extract_image_data(image_url):
    """
    Downloads an image from image_url, then calls Gemini to:
      1) Summarize what's in the image
    Returns a string with the summary.
    """
    try:
        print(f"Gemini IMAGE extraction from: {image_url}")
        image_data = requests.get(image_url, timeout=15).content
        # 1) Summarize
        resp_summary = client.models.generate_content([
            "Describe the contents of this image in a short paragraph.",
            types.Part.from_bytes(data=image_data, mime_type="image/jpeg"),
        ])
        summary_text = resp_summary.text.strip()

        return f"**Image Summary**:\n{summary_text}"
    except Exception as e:
        print(f"Error using Gemini to process Image '{image_url}': {str(e)}")
        return ""


def extract_urls_from_markdown(markdown_text):
    """
    Simple regex-based approach to extract potential URLs from a markdown string.
    We look for http(s)://someurl up until a space or parenthesis or quote, etc.
    """
    pattern = r'(https?://[^\s\'")]+)'
    found = re.findall(pattern, markdown_text)
    return list(set(found))  # unique them


def detect_mime_type(url, timeout=8):
    """
    Attempt a HEAD request to detect the Content-Type. Return 'pdf', 'image' or None if undetermined.
    Also validates image extensions for supported formats.
    """
    try:
        resp = requests.head(url, timeout=timeout, allow_redirects=True)
        ctype = resp.headers.get('Content-Type', '').lower()
        exts = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.heic', '.heif']

        if 'pdf' in ctype:
            return 'pdf'
        elif ctype.startswith('image/') and any(url.lower().endswith(ext) for ext in exts):
            return 'image'
        else:
            return None
    except RequestException as e:
        print(f"Warning: HEAD request failed for {url}. Error: {e}")
        return None


def find_relevant_page_via_map(objective, url, app):
    try:
        print(f"{Colors.CYAN}Understood. The objective is: {objective}{Colors.RESET}")
        print(f"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}")

        map_prompt = f"""
        Based on the objective of: {objective}, provide a 1-2 word search parameter that will help find the information.
        Respond with ONLY 1-2 words, no other text or formatting.
        """

        print(
            f"{Colors.YELLOW}Analyzing objective to determine optimal search parameter...{Colors.RESET}")
        # Use gemini-pro instead of gemini-2.0-flash
        response = client.models.generate_content(
            model=model_name,
            contents=[map_prompt]
        )

        map_search_parameter = response.text.strip()
        print(
            f"{Colors.GREEN}Optimal search parameter identified: {map_search_parameter}{Colors.RESET}")

        print(
            f"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}")
        map_website = app.map_url(url, params={"search": map_search_parameter})

        print(f"{Colors.MAGENTA}Debug - Map response structure: {json.dumps(map_website, indent=2)}{Colors.RESET}")
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")

        if isinstance(map_website, dict):
            links = map_website.get('urls', []) or map_website.get('links', [])
        elif isinstance(map_website, str):
            try:
                parsed = json.loads(map_website)
                links = parsed.get('urls', []) or parsed.get('links', [])
            except json.JSONDecodeError:
                links = []
        else:
            links = map_website if isinstance(map_website, list) else []

        if not links:
            print(f"{Colors.RED}No links found in map response.{Colors.RESET}")
            return None

        rank_prompt = f"""RESPOND ONLY WITH JSON. 
        Analyze these URLs and rank the top 3 most relevant ones for finding information about: {objective}

        Return ONLY a JSON array in this exact format - no other text or explanation:
        [
            {{
                "url": "http://example.com",
                "relevance_score": 95,
                "reason": "Main about page with company information"
            }},
            {{
                "url": "http://example2.com",
                "relevance_score": 85,
                "reason": "Team page with details"
            }},
            {{
                "url": "http://example3.com",
                "relevance_score": 75,
                "reason": "Blog post about company"
            }}
        ]

        URLs to analyze:
        {json.dumps(links, indent=2)}"""

        print(f"{Colors.YELLOW}Ranking URLs by relevance to objective...{Colors.RESET}")
        response = client.models.generate_content(
            model=model_name,
            contents=[rank_prompt]
        )

        print(f"{Colors.MAGENTA}Debug - Raw Gemini response:{Colors.RESET}")
        print(response.text)

        try:
            response_text = response.text.strip()
            print(f"{Colors.MAGENTA}Debug - Cleaned response:{Colors.RESET}")
            print(response_text)

            if '[' in response_text and ']' in response_text:
                start_idx = response_text.find('[')
                end_idx = response_text.rfind(']') + 1
                json_str = response_text[start_idx:end_idx]

                print(
                    f"{Colors.MAGENTA}Debug - Extracted JSON string:{Colors.RESET}")
                print(json_str)

                ranked_results = json.loads(json_str)
            else:
                print(f"{Colors.RED}No JSON array found in response{Colors.RESET}")
                return None

            links = [result["url"] for result in ranked_results]

            print(f"{Colors.CYAN}Top 3 ranked URLs:{Colors.RESET}")
            for result in ranked_results:
                print(f"{Colors.GREEN}URL: {result['url']}{Colors.RESET}")
                print(
                    f"{Colors.YELLOW}Relevance Score: {result['relevance_score']}{Colors.RESET}")
                print(f"{Colors.BLUE}Reason: {result['reason']}{Colors.RESET}")
                print("---")

            if not links:
                print(f"{Colors.RED}No relevant links identified.{Colors.RESET}")
                return None

        except json.JSONDecodeError as e:
            print(f"{Colors.RED}Error parsing ranked results: {str(e)}{Colors.RESET}")
            print(f"{Colors.RED}Failed JSON string: {response_text}{Colors.RESET}")
            return None
        except Exception as e:
            print(f"{Colors.RED}Unexpected error: {str(e)}{Colors.RESET}")
            return None

        print(f"{Colors.GREEN}Located {len(links)} relevant links.{Colors.RESET}")
        return links

    except Exception as e:
        print(
            f"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}")
        return None


def find_objective_in_top_pages(map_website, objective, app):
    try:
        if not map_website:
            print(f"{Colors.RED}No links found to analyze.{Colors.RESET}")
            return None

        top_links = map_website[:3]
        print(
            f"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}")

        for link in top_links:
            print(f"{Colors.YELLOW}Initiating scrape of page: {link}{Colors.RESET}")
            scrape_result = app.scrape_url(
                link, params={'formats': ['markdown']})
            print(
                f"{Colors.GREEN}Page scraping completed successfully.{Colors.RESET}")

            # Now detect any PDF or image URLs in the Markdown text
            page_markdown = scrape_result.get('markdown', '')
            if not page_markdown:
                print(
                    f"{Colors.RED}No markdown returned for {link}, skipping...{Colors.RESET}")
                continue

            found_urls = extract_urls_from_markdown(page_markdown)
            pdf_image_append = ""

            for sub_url in found_urls:
                mime_type_short = detect_mime_type(sub_url)
                if mime_type_short == 'pdf':
                    print(
                        f"{Colors.YELLOW} Detected PDF: {sub_url}. Extracting content...{Colors.RESET}")
                    pdf_content = gemini_extract_pdf_content(sub_url)
                    if pdf_content:
                        pdf_image_append += f"\n\n---\n[PDF from {sub_url}]:\n{pdf_content}"
                elif mime_type_short == 'image':
                    print(
                        f"{Colors.YELLOW} Detected Image: {sub_url}. Extracting content...{Colors.RESET}")
                    image_content = gemini_extract_image_data(sub_url)
                    if image_content:
                        pdf_image_append += f"\n\n---\n[Image from {sub_url}]:\n{image_content}"

            # Append extracted PDF/image text to the main markdown for the page
            if pdf_image_append:
                scrape_result[
                    'markdown'] += f"\n\n---\n**Additional Gemini Extraction:**\n{pdf_image_append}\n"

            check_prompt = f"""
            Analyze this content to find: {objective}
            If found, return ONLY a JSON object with information related to the objective. If not found, respond EXACTLY with: Objective not met
            
            Content to analyze:
            {scrape_result['markdown']}
            
            Remember:
            - Return valid JSON if information is found
            - Return EXACTLY "Objective not met" if not found
            - No other text or explanations
            """

            response = client.models.generate_content(
                model=model_name,
                contents=[check_prompt]
            )

            result = response.text.strip()

            print(f"{Colors.MAGENTA}Debug - Check response:{Colors.RESET}")
            print(result)

            if result != "Objective not met":
                print(
                    f"{Colors.GREEN}Objective potentially fulfilled. Relevant information identified.{Colors.RESET}")
                try:
                    if '{' in result and '}' in result:
                        start_idx = result.find('{')
                        end_idx = result.rfind('}') + 1
                        json_str = result[start_idx:end_idx]
                        return json.loads(json_str)
                    else:
                        print(
                            f"{Colors.RED}No JSON object found in response{Colors.RESET}")
                except json.JSONDecodeError:
                    print(
                        f"{Colors.RED}Error in parsing response. Proceeding to next page...{Colors.RESET}")
            else:
                print(
                    f"{Colors.YELLOW}Objective not met on this page. Proceeding to next link...{Colors.RESET}")

        print(f"{Colors.RED}All available pages analyzed. Objective not fulfilled in examined content.{Colors.RESET}")
        return None

    except Exception as e:
        print(
            f"{Colors.RED}Error encountered during page analysis: {str(e)}{Colors.RESET}")
        return None


def main():
    url = input(f"{Colors.BLUE}Enter the website to crawl : {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter your objective: {Colors.RESET}")

    print(f"{Colors.YELLOW}Initiating web crawling process...{Colors.RESET}")
    map_website = find_relevant_page_via_map(objective, url, app)

    if map_website:
        print(f"{Colors.GREEN}Relevant pages identified. Proceeding with detailed analysis using gemini-pro...{Colors.RESET}")
        result = find_objective_in_top_pages(map_website, objective, app)

        if result:
            print(
                f"{Colors.GREEN}Objective successfully fulfilled. Extracted information:{Colors.RESET}")
            print(f"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}")
        else:
            print(
                f"{Colors.RED}Unable to fulfill the objective with the available content.{Colors.RESET}")
    else:
        print(f"{Colors.RED}No relevant pages identified. Consider refining the search parameters or trying a different website.{Colors.RESET}")


if __name__ == "__main__":
    main()

================
File: examples/gemini-2.0-web-extractor/gemini-2.0-web-extractor.py
================
import os
import json
import time
import requests
from dotenv import load_dotenv
from serpapi.google_search import GoogleSearch
from google import genai

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Initialize clients
client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
serp_api_key = os.getenv("SERP_API_KEY")


if not firecrawl_api_key:
    print(f"{Colors.RED}Warning: FIRECRAWL_API_KEY not found in environment variables{Colors.RESET}")

def search_google(query):
    """Search Google using SerpAPI and return top results."""
    print(f"{Colors.YELLOW}Searching Google for '{query}'...{Colors.RESET}")
    search = GoogleSearch({"q": query, "api_key": serp_api_key})
    return search.get_dict().get("organic_results", [])

def select_urls_with_gemini(company, objective, serp_results):
    """
    Use Gemini 2.0 Flash to select URLs from SERP results.
    Returns a list of URLs.
    """
    try:
        serp_data = [{"title": r.get("title"), "link": r.get("link"), "snippet": r.get("snippet")} 
                     for r in serp_results if r.get("link")]

        prompt = (
            "Task: Select relevant URLs from search results.\n\n"
            "Instructions:\n"
            "1. Analyze the search results for information about the specified company\n"
            "2. Select URLs that are most likely to contain the requested information\n"
            "3. Return ONLY a JSON object with the following structure: {\"selected_urls\": [\"url1\", \"url2\"]}\n"
            "4. Do not include social media links\n\n"
            f"Company: {company}\n"
            f"Information Needed: {objective}\n"
            f"Search Results: {json.dumps(serp_data, indent=2)}\n\n"
            "Response Format: {\"selected_urls\": [\"https://example.com\", \"https://example2.com\"]}"
        )

        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt
        )

        # Clean the response text
        cleaned_response = response.text.strip()
        if cleaned_response.startswith('```'):
            cleaned_response = cleaned_response.split('```')[1]
            if cleaned_response.startswith('json'):
                cleaned_response = cleaned_response[4:]
        cleaned_response = cleaned_response.strip()

        try:
            # Parse JSON response
            result = json.loads(cleaned_response)
            if isinstance(result, dict) and "selected_urls" in result:
                urls = result["selected_urls"]
            else:
                # Fallback to text parsing
                urls = [line.strip() for line in cleaned_response.split('\n') 
                       if line.strip().startswith(('http://', 'https://'))]
        except json.JSONDecodeError:
            # Fallback to text parsing
            urls = [line.strip() for line in cleaned_response.split('\n') 
                   if line.strip().startswith(('http://', 'https://'))]

        # Clean up URLs
        cleaned_urls = [url.replace('/*', '').rstrip('/') for url in urls]
        cleaned_urls = [url for url in cleaned_urls if url]

        if not cleaned_urls:
            print(f"{Colors.YELLOW}No valid URLs found in response.{Colors.RESET}")
            return []

        print(f"{Colors.CYAN}Selected URLs for extraction:{Colors.RESET}")
        for url in cleaned_urls:
            print(f"- {url}")

        return cleaned_urls

    except Exception as e:
        print(f"{Colors.RED}Error selecting URLs: {str(e)}{Colors.RESET}")
        return []

def extract_company_info(urls, prompt, company, api_key):
    """Use requests to call Firecrawl's extract endpoint with selected URLs."""
    print(f"{Colors.YELLOW}Extracting structured data from the provided URLs using Firecrawl...{Colors.RESET}")
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    payload = {
        "urls": urls,
        "prompt": prompt + " for " + company,
        "enableWebSearch": True
    }
    
    try:
        response = requests.post(
            "https://api.firecrawl.dev/v1/extract",
            headers=headers,
            json=payload,
            timeout=30
        )
        
        data = response.json()
        
        if not data.get('success'):
            print(f"{Colors.RED}API returned error: {data.get('error', 'No error message')}{Colors.RESET}")
            return None
        
        extraction_id = data.get('id')
        if not extraction_id:
            print(f"{Colors.RED}No extraction ID found in response.{Colors.RESET}")
            return None

        return poll_firecrawl_result(extraction_id, api_key)

    except requests.exceptions.RequestException as e:
        print(f"{Colors.RED}Request failed: {e}{Colors.RESET}")
        return None
    except json.JSONDecodeError as e:
        print(f"{Colors.RED}Failed to parse response: {e}{Colors.RESET}")
        return None
    except Exception as e:
        print(f"{Colors.RED}Failed to extract data: {e}{Colors.RESET}")
        return None

def poll_firecrawl_result(extraction_id, api_key, interval=10, max_attempts=60):
    """Poll Firecrawl API to get the extraction result."""
    url = f"https://api.firecrawl.dev/v1/extract/{extraction_id}"
    headers = {
        'Authorization': f'Bearer {api_key}'
    }

    print(f"{Colors.YELLOW}Waiting for extraction to complete...{Colors.RESET}")
    
    for attempt in range(1, max_attempts + 1):
        try:
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()

            if data.get('success') and data.get('data'):
                print(f"{Colors.GREEN}Data successfully extracted:{Colors.RESET}")
                print(json.dumps(data['data'], indent=2))
                return data['data']
            elif data.get('success') and not data.get('data'):
                if attempt % 6 == 0:  
                    print(f"{Colors.YELLOW}Still processing... (attempt {attempt}/{max_attempts}){Colors.RESET}")
                time.sleep(interval)
            else:
                print(f"{Colors.RED}API Error: {data.get('error', 'No error message provided')}{Colors.RESET}")
                return None

        except requests.exceptions.RequestException as e:
            print(f"{Colors.RED}Request error: {str(e)}{Colors.RESET}")
            return None
        except json.JSONDecodeError as e:
            print(f"{Colors.RED}JSON parsing error: {str(e)}{Colors.RESET}")
            return None
        except Exception as e:
            print(f"{Colors.RED}Unexpected error: {str(e)}{Colors.RESET}")
            return None

    print(f"{Colors.RED}Max polling attempts reached. Extraction did not complete in time.{Colors.RESET}")
    return None

def main():
    company = input(f"{Colors.BLUE}Enter the company name: {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter what information you want about the company: {Colors.RESET}")
    
    serp_results = search_google(f"{company}")
    if not serp_results:
        print(f"{Colors.RED}No search results found.{Colors.RESET}")
        return
    
    selected_urls = select_urls_with_gemini(company, objective, serp_results)
    
    if not selected_urls:
        print(f"{Colors.RED}No URLs were selected.{Colors.RESET}")
        return
    
    data = extract_company_info(selected_urls, objective, company, firecrawl_api_key)
    
    if data:
        print(f"{Colors.GREEN}Extraction completed successfully.{Colors.RESET}")
    else:
        print(f"{Colors.RED}Failed to extract the requested information. Try refining your prompt or choosing a different company.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/gemini-2.5-crawler/.env.example
================
# Firecrawl API key from your Firecrawl account
FIRECRAWL_API_KEY=your_firecrawl_api_key_here

# Google Cloud API key with Gemini API access
# Get this from Google Cloud Console: https://console.cloud.google.com/
GEMINI_API_KEY=your_gemini_api_key_here

================
File: examples/gemini-2.5-crawler/gemini-2.5-crawler.py
================
import os
from firecrawl import FirecrawlApp
import json
import re
import requests
from requests.exceptions import RequestException
from dotenv import load_dotenv
import google.genai as genai
# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
gemini_api_key = os.getenv("GEMINI_API_KEY")

# Initialize the FirecrawlApp and Gemini client
app = FirecrawlApp(api_key=firecrawl_api_key)
client = genai.Client(api_key=gemini_api_key)  # Create Gemini client
model_name = "gemini-2.5-pro-exp-03-25"
types = genai.types

# ANSI color codes


class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'


def pdf_size_in_mb(data: bytes) -> float:
    """Utility function to estimate PDF size in MB from raw bytes."""
    return len(data) / (1024 * 1024)


def gemini_extract_pdf_content(pdf_url, objective):
    """
    Downloads a PDF from pdf_url, then calls Gemini to extract text.
    Returns a string with the extracted text only.
    """
    try:
        pdf_data = requests.get(pdf_url, timeout=15).content
        size_mb = pdf_size_in_mb(pdf_data)
        if size_mb > 15:
            print(
                f"{Colors.YELLOW}Warning: PDF size is {size_mb} MB. Skipping PDF extraction.{Colors.RESET}")
            return ""

        prompt = f"""
        The objective is: {objective}.
        From this PDF, extract only the text that helps address this objective.
        If it contains no relevant info, return an empty string.
        """
        response = client.models.generate_content(
            model=model_name,
            contents=[
                types.Part.from_bytes(
                    data=pdf_data, mime_type="application/pdf"),
                prompt
            ]
        )
        return response.text.strip()
    except Exception as e:
        print(f"Error using Gemini to process PDF '{pdf_url}': {str(e)}")
        return ""


def gemini_extract_image_data(image_url):
    """
    Downloads an image from image_url, then calls Gemini to:
      1) Summarize what's in the image
    Returns a string with the summary.
    """
    try:
        print(f"Gemini IMAGE extraction from: {image_url}")
        image_data = requests.get(image_url, timeout=15).content
        # 1) Summarize
        resp_summary = client.models.generate_content([
            "Describe the contents of this image in a short paragraph.",
            types.Part.from_bytes(data=image_data, mime_type="image/jpeg"),
        ])
        summary_text = resp_summary.text.strip()

        return f"**Image Summary**:\n{summary_text}"
    except Exception as e:
        print(f"Error using Gemini to process Image '{image_url}': {str(e)}")
        return ""


def extract_urls_from_markdown(markdown_text):
    """
    Simple regex-based approach to extract potential URLs from a markdown string.
    We look for http(s)://someurl up until a space or parenthesis or quote, etc.
    """
    pattern = r'(https?://[^\s\'")]+)'
    found = re.findall(pattern, markdown_text)
    return list(set(found))  # unique them


def detect_mime_type(url, timeout=8):
    """
    Attempt a HEAD request to detect the Content-Type. Return 'pdf', 'image' or None if undetermined.
    Also validates image extensions for supported formats.
    """
    try:
        resp = requests.head(url, timeout=timeout, allow_redirects=True)
        ctype = resp.headers.get('Content-Type', '').lower()
        exts = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.heic', '.heif']

        if 'pdf' in ctype:
            return 'pdf'
        elif ctype.startswith('image/') and any(url.lower().endswith(ext) for ext in exts):
            return 'image'
        else:
            return None
    except RequestException as e:
        print(f"Warning: HEAD request failed for {url}. Error: {e}")
        return None


def find_relevant_page_via_map(objective, url, app):
    try:
        print(f"{Colors.CYAN}Understood. The objective is: {objective}{Colors.RESET}")
        print(f"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}")

        map_prompt = f"""
        Based on the objective of: {objective}, provide a 1-2 word search parameter that will help find the information.
        Respond with ONLY 1-2 words, no other text or formatting.
        """

        print(
            f"{Colors.YELLOW}Analyzing objective to determine optimal search parameter...{Colors.RESET}")

        response = client.models.generate_content(
            model=model_name,
            contents=[map_prompt]
        )

        map_search_parameter = response.text.strip()
        print(
            f"{Colors.GREEN}Optimal search parameter identified: {map_search_parameter}{Colors.RESET}")

        print(
            f"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}")
        map_website = app.map_url(url, params={"search": map_search_parameter})

        print(f"{Colors.MAGENTA}Debug - Map response structure: {json.dumps(map_website, indent=2)}{Colors.RESET}")
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")

        if isinstance(map_website, dict):
            links = map_website.get('urls', []) or map_website.get('links', [])
        elif isinstance(map_website, str):
            try:
                parsed = json.loads(map_website)
                links = parsed.get('urls', []) or parsed.get('links', [])
            except json.JSONDecodeError:
                links = []
        else:
            links = map_website if isinstance(map_website, list) else []

        if not links:
            print(f"{Colors.RED}No links found in map response.{Colors.RESET}")
            return None

        rank_prompt = f"""RESPOND ONLY WITH JSON. 
        Analyze these URLs and rank the top 3 most relevant ones for finding information about: {objective}

        Return ONLY a JSON array in this exact format - no other text or explanation:
        [
            {{
                "url": "http://example.com",
                "relevance_score": 95,
                "reason": "Main about page with company information"
            }},
            {{
                "url": "http://example2.com",
                "relevance_score": 85,
                "reason": "Team page with details"
            }},
            {{
                "url": "http://example3.com",
                "relevance_score": 75,
                "reason": "Blog post about company"
            }}
        ]

        URLs to analyze:
        {json.dumps(links, indent=2)}"""

        print(f"{Colors.YELLOW}Ranking URLs by relevance to objective...{Colors.RESET}")
        response = client.models.generate_content(
            model=model_name,
            contents=[rank_prompt]
        )

        print(f"{Colors.MAGENTA}Debug - Raw Gemini response:{Colors.RESET}")
        print(response.text)

        try:
            response_text = response.text.strip()
            print(f"{Colors.MAGENTA}Debug - Cleaned response:{Colors.RESET}")
            print(response_text)

            if '[' in response_text and ']' in response_text:
                start_idx = response_text.find('[')
                end_idx = response_text.rfind(']') + 1
                json_str = response_text[start_idx:end_idx]

                print(
                    f"{Colors.MAGENTA}Debug - Extracted JSON string:{Colors.RESET}")
                print(json_str)

                ranked_results = json.loads(json_str)
            else:
                print(f"{Colors.RED}No JSON array found in response{Colors.RESET}")
                return None

            links = [result["url"] for result in ranked_results]

            print(f"{Colors.CYAN}Top 3 ranked URLs:{Colors.RESET}")
            for result in ranked_results:
                print(f"{Colors.GREEN}URL: {result['url']}{Colors.RESET}")
                print(
                    f"{Colors.YELLOW}Relevance Score: {result['relevance_score']}{Colors.RESET}")
                print(f"{Colors.BLUE}Reason: {result['reason']}{Colors.RESET}")
                print("---")

            if not links:
                print(f"{Colors.RED}No relevant links identified.{Colors.RESET}")
                return None

        except json.JSONDecodeError as e:
            print(f"{Colors.RED}Error parsing ranked results: {str(e)}{Colors.RESET}")
            print(f"{Colors.RED}Failed JSON string: {response_text}{Colors.RESET}")
            return None
        except Exception as e:
            print(f"{Colors.RED}Unexpected error: {str(e)}{Colors.RESET}")
            return None

        print(f"{Colors.GREEN}Located {len(links)} relevant links.{Colors.RESET}")
        return links

    except Exception as e:
        print(
            f"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}")
        return None


def find_objective_in_top_pages(map_website, objective, app):
    try:
        if not map_website:
            print(f"{Colors.RED}No links found to analyze.{Colors.RESET}")
            return None

        top_links = map_website[:3]
        print(
            f"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}")

        for link in top_links:
            print(f"{Colors.YELLOW}Initiating scrape of page: {link}{Colors.RESET}")
            scrape_result = app.scrape_url(
                link, params={'formats': ['markdown']})
            print(
                f"{Colors.GREEN}Page scraping completed successfully.{Colors.RESET}")

            # Now detect any PDF or image URLs in the Markdown text
            page_markdown = scrape_result.get('markdown', '')
            if not page_markdown:
                print(
                    f"{Colors.RED}No markdown returned for {link}, skipping...{Colors.RESET}")
                continue

            found_urls = extract_urls_from_markdown(page_markdown)
            pdf_image_append = ""

            for sub_url in found_urls:
                mime_type_short = detect_mime_type(sub_url)
                if mime_type_short == 'pdf':
                    print(
                        f"{Colors.YELLOW} Detected PDF: {sub_url}. Extracting content...{Colors.RESET}")
                    pdf_content = gemini_extract_pdf_content(sub_url)
                    if pdf_content:
                        pdf_image_append += f"\n\n---\n[PDF from {sub_url}]:\n{pdf_content}"
                elif mime_type_short == 'image':
                    print(
                        f"{Colors.YELLOW} Detected Image: {sub_url}. Extracting content...{Colors.RESET}")
                    image_content = gemini_extract_image_data(sub_url)
                    if image_content:
                        pdf_image_append += f"\n\n---\n[Image from {sub_url}]:\n{image_content}"

            # Append extracted PDF/image text to the main markdown for the page
            if pdf_image_append:
                scrape_result[
                    'markdown'] += f"\n\n---\n**Additional Gemini Extraction:**\n{pdf_image_append}\n"

            check_prompt = f"""
            Analyze this content to find: {objective}
            If found, return ONLY a JSON object with information related to the objective. If not found, respond EXACTLY with: Objective not met
            
            Content to analyze:
            {scrape_result['markdown']}
            
            Remember:
            - Return valid JSON if information is found
            - Return EXACTLY "Objective not met" if not found
            - No other text or explanations
            """

            response = client.models.generate_content(
                model=model_name,
                contents=[check_prompt]
            )

            result = response.text.strip()

            print(f"{Colors.MAGENTA}Debug - Check response:{Colors.RESET}")
            print(result)

            if result != "Objective not met":
                print(
                    f"{Colors.GREEN}Objective potentially fulfilled. Relevant information identified.{Colors.RESET}")
                try:
                    if '{' in result and '}' in result:
                        start_idx = result.find('{')
                        end_idx = result.rfind('}') + 1
                        json_str = result[start_idx:end_idx]
                        return json.loads(json_str)
                    else:
                        print(
                            f"{Colors.RED}No JSON object found in response{Colors.RESET}")
                except json.JSONDecodeError:
                    print(
                        f"{Colors.RED}Error in parsing response. Proceeding to next page...{Colors.RESET}")
            else:
                print(
                    f"{Colors.YELLOW}Objective not met on this page. Proceeding to next link...{Colors.RESET}")

        print(f"{Colors.RED}All available pages analyzed. Objective not fulfilled in examined content.{Colors.RESET}")
        return None

    except Exception as e:
        print(
            f"{Colors.RED}Error encountered during page analysis: {str(e)}{Colors.RESET}")
        return None


def main():
    url = input(f"{Colors.BLUE}Enter the website to crawl : {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter your objective: {Colors.RESET}")

    print(f"{Colors.YELLOW}Initiating web crawling process...{Colors.RESET}")
    map_website = find_relevant_page_via_map(objective, url, app)

    if map_website:
        print(f"{Colors.GREEN}Relevant pages identified. Proceeding with detailed analysis using gemini-pro...{Colors.RESET}")
        result = find_objective_in_top_pages(map_website, objective, app)

        if result:
            print(
                f"{Colors.GREEN}Objective successfully fulfilled. Extracted information:{Colors.RESET}")
            print(f"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}")
        else:
            print(
                f"{Colors.RED}Unable to fulfill the objective with the available content.{Colors.RESET}")
    else:
        print(f"{Colors.RED}No relevant pages identified. Consider refining the search parameters or trying a different website.{Colors.RESET}")


if __name__ == "__main__":
    main()

================
File: examples/gemini-2.5-crawler/README.md
================
# Gemini 2.5 Web Crawler

A powerful web crawler that uses Google's Gemini 2.5 Pro model to intelligently analyze web content, PDFs, and images based on user-defined objectives.

## Features

- Intelligent URL mapping and ranking based on relevance to search objective
- PDF content extraction and analysis
- Image content analysis and description
- Smart content filtering based on user objectives
- Support for multiple content types (markdown, PDFs, images)
- Color-coded console output for better readability

## Prerequisites

- Python 3.8+
- Google Cloud API key with Gemini API access
- Firecrawl API key

## Installation

1. Clone the repository:

```bash
git clone <your-repo-url>
cd <your-repo-directory>
```

2. Install the required dependencies:

```bash
pip install -r requirements.txt
```

3. Create a `.env` file based on `.env.example`:

```bash
cp .env.example .env
```

4. Add your API keys to the `.env` file:

```
FIRECRAWL_API_KEY=your_firecrawl_api_key
GEMINI_API_KEY=your_gemini_api_key
```

## Usage

Run the script:

```bash
python gemini-2.5-crawler.py
```

The script will prompt you for:

1. The website URL to crawl
2. Your search objective

The crawler will then:

1. Map the website and find relevant pages
2. Analyze the content using Gemini 2.5 Pro
3. Extract and analyze any PDFs or images found
4. Return structured information related to your objective

## Output

The script provides color-coded console output for:

- Process steps and progress
- Debug information
- Success and error messages
- Final results in JSON format

## Error Handling

The script includes comprehensive error handling for:

- API failures
- Content extraction issues
- Invalid URLs
- Timeouts
- JSON parsing errors

## Note

This script uses the experimental Gemini 2.5 Pro model (`gemini-2.5-pro-exp-03-25`). Make sure you have appropriate access and quota for using this model.

================
File: examples/gemini-2.5-crawler/requirements.txt
================
google-cloud-aiplatform>=1.36.0
google-generativeai>=0.3.2
python-dotenv>=1.0.0
requests>=2.31.0
firecrawl>=0.1.0

================
File: examples/gemini-2.5-web-extractor/.env.example
================
# Google Gemini API Key
GOOGLE_API_KEY=your_google_api_key_here

# Firecrawl API Key
FIRECRAWL_API_KEY=your_firecrawl_api_key_here

# SerpAPI Key
SERP_API_KEY=your_serp_api_key_here

================
File: examples/gemini-2.5-web-extractor/.gitignore
================
# Environment variables
.env

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# IDE
.idea/
.vscode/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

================
File: examples/gemini-2.5-web-extractor/gemini-2.5-web-extractor.py
================
import os
import json
import time
import requests
from dotenv import load_dotenv
from serpapi.google_search import GoogleSearch
from google import genai

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Initialize clients
client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
serp_api_key = os.getenv("SERP_API_KEY")


if not firecrawl_api_key:
    print(f"{Colors.RED}Warning: FIRECRAWL_API_KEY not found in environment variables{Colors.RESET}")

def search_google(query):
    """Search Google using SerpAPI and return top results."""
    print(f"{Colors.YELLOW}Searching Google for '{query}'...{Colors.RESET}")
    search = GoogleSearch({"q": query, "api_key": serp_api_key})
    return search.get_dict().get("organic_results", [])

def select_urls_with_gemini(company, objective, serp_results):
    """
    Use Gemini 2.5 Flash to select URLs from SERP results.
    Returns a list of URLs.
    """
    try:
        serp_data = [{"title": r.get("title"), "link": r.get("link"), "snippet": r.get("snippet")} 
                     for r in serp_results if r.get("link")]

        prompt = (
            "Task: Select relevant URLs from search results.\n\n"
            "Instructions:\n"
            "1. Analyze the search results for information about the specified company\n"
            "2. Select URLs that are most likely to contain the requested information\n"
            "3. Return ONLY a JSON object with the following structure: {\"selected_urls\": [\"url1\", \"url2\"]}\n"
            "4. Do not include social media links\n\n"
            f"Company: {company}\n"
            f"Information Needed: {objective}\n"
            f"Search Results: {json.dumps(serp_data, indent=2)}\n\n"
            "Response Format: {\"selected_urls\": [\"https://example.com\", \"https://example2.com\"]}"
        )

        response = client.models.generate_content(
            model="gemini-2.5-pro-exp-03-25",
            contents=prompt
        )

        # Clean the response text
        cleaned_response = response.text.strip()
        if cleaned_response.startswith('```'):
            cleaned_response = cleaned_response.split('```')[1]
            if cleaned_response.startswith('json'):
                cleaned_response = cleaned_response[4:]
        cleaned_response = cleaned_response.strip()

        try:
            # Parse JSON response
            result = json.loads(cleaned_response)
            if isinstance(result, dict) and "selected_urls" in result:
                urls = result["selected_urls"]
            else:
                # Fallback to text parsing
                urls = [line.strip() for line in cleaned_response.split('\n') 
                       if line.strip().startswith(('http://', 'https://'))]
        except json.JSONDecodeError:
            # Fallback to text parsing
            urls = [line.strip() for line in cleaned_response.split('\n') 
                   if line.strip().startswith(('http://', 'https://'))]

        # Clean up URLs
        cleaned_urls = [url.replace('/*', '').rstrip('/') for url in urls]
        cleaned_urls = [url for url in cleaned_urls if url]

        if not cleaned_urls:
            print(f"{Colors.YELLOW}No valid URLs found in response.{Colors.RESET}")
            return []

        print(f"{Colors.CYAN}Selected URLs for extraction:{Colors.RESET}")
        for url in cleaned_urls:
            print(f"- {url}")

        return cleaned_urls

    except Exception as e:
        print(f"{Colors.RED}Error selecting URLs: {str(e)}{Colors.RESET}")
        return []

def extract_company_info(urls, prompt, company, api_key):
    """Use requests to call Firecrawl's extract endpoint with selected URLs."""
    print(f"{Colors.YELLOW}Extracting structured data from the provided URLs using Firecrawl...{Colors.RESET}")
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    payload = {
        "urls": urls,
        "prompt": prompt + " for " + company,
        "enableWebSearch": True
    }
    
    try:
        response = requests.post(
            "https://api.firecrawl.dev/v1/extract",
            headers=headers,
            json=payload,
            timeout=30
        )
        
        data = response.json()
        
        if not data.get('success'):
            print(f"{Colors.RED}API returned error: {data.get('error', 'No error message')}{Colors.RESET}")
            return None
        
        extraction_id = data.get('id')
        if not extraction_id:
            print(f"{Colors.RED}No extraction ID found in response.{Colors.RESET}")
            return None

        return poll_firecrawl_result(extraction_id, api_key)

    except requests.exceptions.RequestException as e:
        print(f"{Colors.RED}Request failed: {e}{Colors.RESET}")
        return None
    except json.JSONDecodeError as e:
        print(f"{Colors.RED}Failed to parse response: {e}{Colors.RESET}")
        return None
    except Exception as e:
        print(f"{Colors.RED}Failed to extract data: {e}{Colors.RESET}")
        return None

def poll_firecrawl_result(extraction_id, api_key, interval=10, max_attempts=60):
    """Poll Firecrawl API to get the extraction result."""
    url = f"https://api.firecrawl.dev/v1/extract/{extraction_id}"
    headers = {
        'Authorization': f'Bearer {api_key}'
    }

    print(f"{Colors.YELLOW}Waiting for extraction to complete...{Colors.RESET}")
    
    for attempt in range(1, max_attempts + 1):
        try:
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()

            if data.get('success') and data.get('data'):
                print(f"{Colors.GREEN}Data successfully extracted:{Colors.RESET}")
                print(json.dumps(data['data'], indent=2))
                return data['data']
            elif data.get('success') and not data.get('data'):
                if attempt % 6 == 0:  
                    print(f"{Colors.YELLOW}Still processing... (attempt {attempt}/{max_attempts}){Colors.RESET}")
                time.sleep(interval)
            else:
                print(f"{Colors.RED}API Error: {data.get('error', 'No error message provided')}{Colors.RESET}")
                return None

        except requests.exceptions.RequestException as e:
            print(f"{Colors.RED}Request error: {str(e)}{Colors.RESET}")
            return None
        except json.JSONDecodeError as e:
            print(f"{Colors.RED}JSON parsing error: {str(e)}{Colors.RESET}")
            return None
        except Exception as e:
            print(f"{Colors.RED}Unexpected error: {str(e)}{Colors.RESET}")
            return None

    print(f"{Colors.RED}Max polling attempts reached. Extraction did not complete in time.{Colors.RESET}")
    return None

def main():
    company = input(f"{Colors.BLUE}Enter the company name: {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter what information you want about the company: {Colors.RESET}")
    
    serp_results = search_google(f"{company}")
    if not serp_results:
        print(f"{Colors.RED}No search results found.{Colors.RESET}")
        return
    
    selected_urls = select_urls_with_gemini(company, objective, serp_results)
    
    if not selected_urls:
        print(f"{Colors.RED}No URLs were selected.{Colors.RESET}")
        return
    
    data = extract_company_info(selected_urls, objective, company, firecrawl_api_key)
    
    if data:
        print(f"{Colors.GREEN}Extraction completed successfully.{Colors.RESET}")
    else:
        print(f"{Colors.RED}Failed to extract the requested information. Try refining your prompt or choosing a different company.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/gemini-2.5-web-extractor/README.md
================
# Gemini 2.5 Web Extractor

A powerful web information extraction tool that combines Google's Gemini 2.5 Pro (Experimental) model with Firecrawl's web extraction capabilities to gather structured information about companies from the web.

## Features

- Uses Google Search (via SerpAPI) to find relevant web pages
- Leverages Gemini 2.5 Pro (Experimental) to intelligently select the most relevant URLs
- Extracts structured information using Firecrawl's advanced web extraction
- Real-time progress monitoring and colorized console output

## Prerequisites

- Python 3.8 or higher
- Google API Key (Gemini)
- Firecrawl API Key
- SerpAPI Key

## Setup

1. Clone the repository:

```bash
git clone <repository-url>
cd gemini-2.5-web-extractor
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Set up environment variables:
   - Copy `.env.example` to `.env`
   - Fill in your API keys in the `.env` file:
     - `GOOGLE_API_KEY`: Your Google API key for Gemini
     - `FIRECRAWL_API_KEY`: Your Firecrawl API key
     - `SERP_API_KEY`: Your SerpAPI key

## Usage

Run the script:

```bash
python gemini-2.5-web-extractor.py
```

The script will:

1. Prompt you for a company name
2. Ask what information you want to extract about the company
3. Search for relevant web pages
4. Use Gemini to select the most relevant URLs
5. Extract structured information using Firecrawl
6. Display the results in a formatted JSON output

## Example

```bash
Enter the company name: Tesla
Enter what information you want about the company: latest electric vehicle models and their specifications
```

The script will then:

1. Search for relevant Tesla information
2. Select the most informative URLs about Tesla's current EV lineup
3. Extract and structure the vehicle specifications
4. Present the data in a clean, organized format

## Error Handling

The script includes comprehensive error handling for:

- API failures
- Network issues
- Invalid responses
- Timeout scenarios

All errors are clearly displayed with colored output for better visibility.

## License

[Add your license information here]

================
File: examples/gemini-2.5-web-extractor/requirements.txt
================
python-dotenv==1.0.0
google-generativeai==0.3.2
requests==2.31.0
serpapi==0.1.5

================
File: examples/gemini-github-analyzer/gemini-github-analyzer.py
================
import os
import json
import time
import requests
from dotenv import load_dotenv
from google import genai
from datetime import datetime

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    RESET = '\033[0m'

# Emojis for different sections
class Emojis:
    GITHUB = "üêô"
    STATS = "üìä"
    CALENDAR = "üìÖ"
    SKILLS = "üíª"
    STAR = "‚≠ê"
    ROCKET = "üöÄ"
    CHART = "üìà"
    BULB = "üí°"
    WARNING = "‚ö†Ô∏è"
    CHECK = "‚úÖ"
    FIRE = "üî•"
    BOOK = "üìö"
    TOOLS = "üõ†Ô∏è"
    GRAPH = "üìä"
    TARGET = "üéØ"

# Load environment variables
load_dotenv()

# Initialize clients
client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")

if not firecrawl_api_key:
    print(f"{Colors.RED}{Emojis.WARNING} Warning: FIRECRAWL_API_KEY not found in environment variables{Colors.RESET}")

def print_header(text, emoji, color=Colors.BLUE):
    """Print a formatted section header with emoji."""
    width = 70
    print("\n" + "‚ïê" * width)
    print(f"{color}{Colors.BOLD}{emoji}  {text.center(width-4)}  {emoji}{Colors.RESET}")
    print("‚ïê" * width + "\n")

def print_section(title, content, emoji):
    """Print a formatted section with title, content, and emoji."""
    print(f"\n{Colors.CYAN}{Colors.BOLD}{emoji} {title}{Colors.RESET}")
    print(f"{content}")

def poll_extraction_result(extraction_id, api_key, interval=2, max_attempts=15):
    """Poll Firecrawl API for extraction results with shorter intervals."""
    url = f"https://api.firecrawl.dev/v1/extract/{extraction_id}"
    headers = {'Authorization': f'Bearer {api_key}'}

    print(f"{Colors.YELLOW}Processing profile data...{Colors.RESET}")

    for attempt in range(max_attempts):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            data = response.json()

            if data.get('success') and data.get('data'):
                print(f"{Colors.GREEN}Data extracted successfully!{Colors.RESET}")
                return data['data']
            elif data.get('success'):
                if attempt % 3 == 0:  # Print progress less frequently
                    print(".", end="", flush=True)
                time.sleep(interval)
            else:
                print(f"\n{Colors.RED}API Error: {data.get('error', 'Unknown error')}{Colors.RESET}")
                return None

        except requests.exceptions.Timeout:
            print(f"\n{Colors.RED}Request timed out. Retrying...{Colors.RESET}")
            continue
        except Exception as e:
            print(f"\n{Colors.RED}Error polling results: {e}{Colors.RESET}")
            return None

    print(f"\n{Colors.RED}Extraction timed out after {max_attempts} attempts.{Colors.RESET}")
    return None

def extract_github_profile(username, api_key):
    """Extract GitHub profile data using Firecrawl with optimized settings."""
    if not api_key:
        print(f"{Colors.RED}Error: Firecrawl API key is missing{Colors.RESET}")
        return None

    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }

    github_url = f"https://github.com/{username}"
    
    # Simplified prompt for faster extraction
    payload = {
        "urls": [github_url],
        "prompt": """Extract key GitHub profile data:
        - Basic profile information (company, location, bio)
        - Repository list and details
        - Contribution statistics
        - Recent activity
        - Social stats""",
        "enableWebSearch": False
    }

    try:
        print(f"{Colors.YELLOW}Starting extraction for: {username}{Colors.RESET}")
        response = requests.post(
            "https://api.firecrawl.dev/v1/extract",
            headers=headers,
            json=payload,
            timeout=15
        )

        if response.status_code != 200:
            print(f"{Colors.RED}API Error ({response.status_code}): {response.text}{Colors.RESET}")
            return None

        data = response.json()
        if not data.get('success'):
            print(f"{Colors.RED}API Error: {data.get('error', 'Unknown error')}{Colors.RESET}")
            return None

        extraction_id = data.get('id')
        if not extraction_id:
            print(f"{Colors.RED}No extraction ID received{Colors.RESET}")
            return None

        return poll_extraction_result(extraction_id, api_key)

    except requests.exceptions.Timeout:
        print(f"{Colors.RED}Initial request timed out{Colors.RESET}")
        return None
    except Exception as e:
        print(f"{Colors.RED}Extraction failed: {e}{Colors.RESET}")
        return None

def analyze_with_gemini(profile_data, username):
    """Use Gemini to analyze GitHub profile data with focus on comprehensive insights."""
    prompt = f"""
    Analyze this GitHub profile and provide detailed insights from the available data.
    Focus on concrete information and metrics.
    
    Structure your response in these sections:
    1. Professional Background
    - Current company/organization (if available)
    - Role/position (if available)
    - Professional website or blog links
    - Location (if available)

    2. Activity Analysis
    - Total repositories and forks
    - Most active repositories (top 3)
    - Contribution frequency
    - Recent activity trends
    - Streak information

    3. Technical Portfolio
    - Primary programming languages
    - Most used technologies/frameworks
    - Top contributed repositories
    - Notable project themes

    4. Community Engagement
    - Followers and following count
    - Public contributions
    - Pull requests and issues
    - Project collaborations

    Rules:
    - Include only verifiable information from the profile
    - List specific repository names and their purposes
    - Include contribution statistics where available
    - Focus on recent activity (last 6 months)
    - Skip sections only if completely unavailable
    
    Profile Data: {json.dumps(profile_data, indent=2)}
    """

    try:
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt
        )
        
        # Clean up response
        analysis = response.text.strip()
        analysis_lines = [line for line in analysis.split('\n') 
                         if not any(word in line.lower() 
                                  for word in ['undetermined', 'unknown', 'limited', 
                                             'not available', 'needs', 'requires', 'unclear'])]
        cleaned_analysis = '\n'.join(line for line in analysis_lines if line.strip())
        
        return format_report(cleaned_analysis, username)

    except Exception as e:
        print(f"{Colors.RED}Analysis failed: {e}{Colors.RESET}")
        return None

def format_report(raw_analysis, username):
    """Format the analysis into a clean, professional report."""
    report = f"""
{Colors.BOLD}GitHub Profile Analysis: {username}{Colors.RESET}
{Colors.CYAN}{'‚îÄ' * 40}{Colors.RESET}\n"""

    sections = raw_analysis.split('\n')
    current_section = None

    for line in sections:
        line = line.strip()
        if not line:
            continue

        if any(section in line for section in ["Professional Background", "Activity Analysis", 
                                             "Technical Portfolio", "Community Engagement"]):
            report += f"\n{Colors.BOLD}{Colors.BLUE}{line}{Colors.RESET}\n"
        elif line.startswith('-'):
            report += f"‚Ä¢ {line[1:].strip()}\n"
        elif line and not line.startswith(('#', '‚Ä¢')):
            report += f"  {line}\n"

    return report

def save_report(report, username):
    """Save the report to a file."""
    filename = f"github_analysis_{username}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            # Strip ANSI color codes when saving to file
            clean_report = report
            for color in vars(Colors).values():
                if isinstance(color, str) and color.startswith('\033'):
                    clean_report = clean_report.replace(color, '')
            f.write(clean_report)
        return filename
    except Exception as e:
        print(f"{Colors.RED}{Emojis.WARNING} Error saving report: {e}{Colors.RESET}")
        return None

def main():
    username = input(f"{Colors.GREEN}GitHub username: {Colors.RESET}").strip()

    if not username:
        print(f"{Colors.RED}Please provide a valid username.{Colors.RESET}")
        return

    print("Analyzing profile...")
    profile_data = extract_github_profile(username, firecrawl_api_key)
    
    if not profile_data:
        print(f"{Colors.RED}Profile analysis failed.{Colors.RESET}")
        return

    report = analyze_with_gemini(profile_data, username)

    if report:
        print(report)
    else:
        print(f"{Colors.RED}Could not generate insights.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/gpt-4.1-company-researcher/.env.example
================
# API Keys
OPENAI_API_KEY=your_openai_api_key_here
FIRECRAWL_API_KEY=your_firecrawl_api_key_here
SERP_API_KEY=your_serpapi_key_here

================
File: examples/gpt-4.1-company-researcher/.gitignore
================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# PyInstaller
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# Environment variables
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# VSCode
.vscode/

# PyCharm
.idea/

================
File: examples/gpt-4.1-company-researcher/gpt-4.1-company-researcher.py
================
import os
import json
import time
import requests
from dotenv import load_dotenv
from serpapi.google_search import GoogleSearch
from openai import OpenAI

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Initialize clients
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    print(f"{Colors.RED}Error: OPENAI_API_KEY not found in environment variables{Colors.RESET}")
    
client = OpenAI(api_key=openai_api_key)
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
serp_api_key = os.getenv("SERP_API_KEY")

if not firecrawl_api_key:
    print(f"{Colors.RED}Warning: FIRECRAWL_API_KEY not found in environment variables{Colors.RESET}")

if not serp_api_key:
    print(f"{Colors.RED}Error: SERP_API_KEY not found in environment variables{Colors.RESET}")

def search_google(query, company):
    """Search Google using SerpAPI and return top results."""
    print(f"{Colors.YELLOW}Searching Google for information about {company}...{Colors.RESET}")
    if not serp_api_key:
        print(f"{Colors.RED}Cannot search Google: SERP_API_KEY is missing{Colors.RESET}")
        return []
    
    # Create a more effective search query
    search_query = f"{company} company {query}"
    
    params = {
        "q": search_query,
        "api_key": serp_api_key,
        "engine": "google",
        "google_domain": "google.com",
        "gl": "us",
        "hl": "en",
        "num": 10  # Request more results
    }
    
    try:
        search = GoogleSearch(params)
        results = search.get_dict()
        
        if "error" in results:
            print(f"{Colors.RED}SerpAPI Error: {results['error']}{Colors.RESET}")
            return []
        
        organic_results = results.get("organic_results", [])
        
        if not organic_results:
            print(f"{Colors.YELLOW}No organic results found, trying alternative search...{Colors.RESET}")
            # Try an alternative search
            alt_params = params.copy()
            alt_params["q"] = company
            
            try:
                alt_search = GoogleSearch(alt_params)
                alt_results = alt_search.get_dict()
                
                if "error" not in alt_results:
                    organic_results = alt_results.get("organic_results", [])
            except Exception as e:
                print(f"{Colors.RED}Error in alternative search: {str(e)}{Colors.RESET}")
        
        print(f"{Colors.GREEN}Found {len(organic_results)} search results{Colors.RESET}")
        return organic_results
    except Exception as e:
        print(f"{Colors.RED}Error in search_google: {str(e)}{Colors.RESET}")
        return []

def validate_official_source(url, company):
    """Check if a URL is likely an official company source."""
    company_name = company.lower().replace(" ", "")
    url_lower = url.lower()
    
    # Special cases
    if "ycombinator.com/companies/" in url_lower:
        return True
        
    if "workatastartup.com/companies/" in url_lower:
        return True
        
    if "crunchbase.com/organization/" in url_lower:
        return True
        
    if "producthunt.com" in url_lower:
        return True
        
    # Main domain check - more flexible approach
    domain = url_lower.split("//")[1].split("/")[0] if "//" in url_lower else url_lower
    
    # Company website usually has company name in domain
    company_terms = company_name.split()
    for term in company_terms:
        if len(term) > 3 and term in domain:  # Only match on significant terms
            return True
    
    # Common TLDs for tech companies
    if ".com" in url_lower or ".org" in url_lower or ".net" in url_lower or ".dev" in url_lower or ".io" in url_lower or ".ai" in url_lower:
        domain_without_tld = domain.split(".")[0]
        if company_name.replace(" ", "") in domain_without_tld.replace("-", "").replace(".", ""):
            return True
    
    # Explicitly non-official sources
    non_official_patterns = [
        "linkedin.com", "facebook.com", "twitter.com", 
        "instagram.com", "medium.com", "bloomberg.com"
    ]
    
    for pattern in non_official_patterns:
        if pattern in url_lower:
            return False
    
    # For any other domain that got through the filters, consider it potentially official
    return True

def select_urls_with_gpt(company, objective, serp_results):
    """
    Use GPT-4.1 to select URLs from SERP results.
    Returns a list of URLs.
    """
    try:
        serp_data = [{"title": r.get("title"), "link": r.get("link"), "snippet": r.get("snippet")} 
                     for r in serp_results if r.get("link")]
        
        print(f"{Colors.CYAN}Found {len(serp_data)} search results to analyze{Colors.RESET}")
        
        if not serp_data:
            print(f"{Colors.YELLOW}No search results found to analyze{Colors.RESET}")
            return []

        prompt = (
            "Task: Select the most relevant URLs from search results that contain factual information about the company.\n\n"
            "Instructions:\n"
            "1. Prioritize official company websites and documentation\n"
            "2. Select URLs that directly contain information about the requested topic\n"
            "3. Return ONLY a JSON object with the following structure: {\"selected_urls\": [\"url1\", \"url2\"]}\n"
            "4. Include up to 3 most relevant URLs\n"
            "5. Consider startup directories like Crunchbase, YCombinator, ProductHunt as good sources\n"
            "6. If official website is available, prioritize it first\n"
            f"Company: {company}\n"
            f"Information Needed: {objective}\n"
            f"Search Results: {json.dumps(serp_data, indent=2)}\n\n"
            "Response Format: {\"selected_urls\": [\"https://example.com\", \"https://example2.com\"]}\n"
        )

        try:
            print(f"{Colors.YELLOW}Calling OpenAI model...{Colors.RESET}")
            response = client.chat.completions.create(
                model="gpt-4.1",
                messages=[{"role": "user", "content": prompt}],
            )
            
            cleaned_response = response.choices[0].message.content.strip()

            import re
            json_match = re.search(r'\{[\s\S]*"selected_urls"[\s\S]*\}', cleaned_response)
            if json_match:
                cleaned_response = json_match.group(0)

            if cleaned_response.startswith('```'):
                cleaned_response = cleaned_response.split('```')[1]
                if cleaned_response.startswith('json'):
                    cleaned_response = cleaned_response[4:]
            cleaned_response = cleaned_response.strip()

            try:
                result = json.loads(cleaned_response)
                if isinstance(result, dict) and "selected_urls" in result:
                    urls = result["selected_urls"]
                else:
                    urls = []
            except json.JSONDecodeError:
                urls = [line.strip() for line in cleaned_response.split('\n') 
                       if line.strip().startswith(('http://', 'https://'))]

            cleaned_urls = [url.replace('/*', '').rstrip('/') for url in urls]
            cleaned_urls = [url for url in cleaned_urls if url]

            if not cleaned_urls:
                print(f"{Colors.YELLOW}No valid URLs found in response.{Colors.RESET}")
                return []

            for url in cleaned_urls:
                print(f"- {url} {Colors.RESET}")

            # Consider all selected URLs as valid sources
            return cleaned_urls[:3]  # Limit to top 3

        except Exception as e:
            print(f"{Colors.RED}Error calling OpenAI: {str(e)}{Colors.RESET}")
            return []

    except Exception as e:
        print(f"{Colors.RED}Error selecting URLs: {str(e)}{Colors.RESET}")
        return []

def extract_company_info(urls, prompt, company, api_key):
    """Use requests to call Firecrawl's extract endpoint with selected URLs."""
    print(f"{Colors.YELLOW}Extracting structured data from the provided URLs using Firecrawl...{Colors.RESET}")
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    enhanced_prompt = (
        f"Extract factual information about {prompt} for {company}.\n"
        f"Stick STRICTLY to information found on the provided URLs and DO NOT add any additional facts that are not explicitly mentioned.\n"
        f"Only extract information EXACTLY as stated in the source - no inferences or additions.\n"
        f"If information on {prompt} is not clearly provided in the source documents, just leave fields empty."
    )
    
    payload = {
        "urls": urls,
        "prompt": enhanced_prompt,
        "enableWebSearch": False
    }
    
    try:
        response = requests.post(
            "https://api.firecrawl.dev/v1/extract",
            headers=headers,
            json=payload,
            timeout=120
        )
        
        if response.status_code != 200:
            print(f"{Colors.RED}API returned status code {response.status_code}: {response.text}{Colors.RESET}")
            return None
            
        data = response.json()
        
        if not data.get('success'):
            print(f"{Colors.RED}API returned error: {data.get('error', 'No error message')}{Colors.RESET}")
            return None
        
        extraction_id = data.get('id')
        if not extraction_id:
            print(f"{Colors.RED}No extraction ID found in response.{Colors.RESET}")
            return None

        return poll_firecrawl_result(extraction_id, api_key, interval=5, max_attempts=120)

    except requests.exceptions.Timeout:
        print(f"{Colors.RED}Request timed out. The operation might still be processing in the background.{Colors.RESET}")
        print(f"{Colors.YELLOW}You may want to try again with fewer URLs or a more specific prompt.{Colors.RESET}")
        return None
    except requests.exceptions.RequestException as e:
        print(f"{Colors.RED}Request failed: {e}{Colors.RESET}")
        return None
    except json.JSONDecodeError as e:
        print(f"{Colors.RED}Failed to parse response: {e}{Colors.RESET}")
        return None
    except Exception as e:
        print(f"{Colors.RED}Failed to extract data: {e}{Colors.RESET}")
        return None

def poll_firecrawl_result(extraction_id, api_key, interval=10, max_attempts=60):
    """Poll Firecrawl API to get the extraction result."""
    url = f"https://api.firecrawl.dev/v1/extract/{extraction_id}"
    headers = {
        'Authorization': f'Bearer {api_key}'
    }

    print(f"{Colors.YELLOW}Waiting for extraction to complete...{Colors.RESET}")
    
    for attempt in range(1, max_attempts + 1):
        try:
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()

            if data.get('success') and data.get('data'):
                print(f"{Colors.GREEN}Data successfully extracted{Colors.RESET}")
                return data['data']
            elif data.get('success') and not data.get('data'):
                if attempt % 6 == 0:  
                    print(f"{Colors.YELLOW}Still processing... (attempt {attempt}/{max_attempts}){Colors.RESET}")
                time.sleep(interval)
            else:
                print(f"{Colors.RED}API Error: {data.get('error', 'No error message provided')}{Colors.RESET}")
                return None

        except requests.exceptions.RequestException as e:
            print(f"{Colors.RED}Request error: {str(e)}{Colors.RESET}")
            return None
        except json.JSONDecodeError as e:
            print(f"{Colors.RED}JSON parsing error: {str(e)}{Colors.RESET}")
            return None
        except Exception as e:
            print(f"{Colors.RED}Unexpected error: {str(e)}{Colors.RESET}")
            return None

    print(f"{Colors.RED}Max polling attempts reached. Extraction did not complete in time.{Colors.RESET}")
    return None

def deduplicate_data(data):
    """Deduplicate data from the extraction results."""
    if not data:
        return data
    
    print(f"{Colors.YELLOW}Deduplicating extracted data...{Colors.RESET}")
    
    for key, value in data.items():
        if isinstance(value, list):
            if value and isinstance(value[0], dict):
                seen = set()
                unique_items = []
                
                for item in value:
                    item_tuple = tuple(sorted((k, str(v)) for k, v in item.items()))
                    
                    if item_tuple not in seen:
                        seen.add(item_tuple)
                        unique_items.append(item)
                
                data[key] = unique_items
                print(f"{Colors.GREEN}Deduplicated '{key}': removed {len(value) - len(unique_items)} duplicate entries{Colors.RESET}")
            
            else:
                unique_items = list(dict.fromkeys(value))
                data[key] = unique_items
                print(f"{Colors.GREEN}Deduplicated '{key}': removed {len(value) - len(unique_items)} duplicate entries{Colors.RESET}")
    
    return data

def consolidate_data(data, company):
    """Consolidate data by filling in missing fields and removing lower quality entries."""
    if not data:
        return data
    
    print(f"{Colors.YELLOW}Consolidating and validating data...{Colors.RESET}")
    
    for key, value in data.items():
        if isinstance(value, list) and value and isinstance(value[0], dict):
            if 'name' in value[0]:
                consolidated = {}
                
                for item in value:
                    name = item.get('name', '').strip().lower()
                    if not name:
                        continue
                        
                    if len(name) < 2 or len(name) > 50:
                        continue
                    
                    name_parts = name.split()
                    if len(name_parts) == 1:
                        found = False
                        for full_name in list(consolidated.keys()):
                            if full_name.startswith(name) or full_name.endswith(name):
                                found = True
                                break
                        if found:
                            continue
                    
                    if name not in consolidated or len(item) > len(consolidated[name]):
                        consolidated[name] = item
                    
                data[key] = list(consolidated.values())
                print(f"{Colors.GREEN}Consolidated '{key}': {len(value)} entries into {len(data[key])} unique entries{Colors.RESET}")
    
    # Clean up output data - remove empty fields
    for key, value in data.items():
        if isinstance(value, list):
            for item in value:
                if isinstance(item, dict):
                    # Remove empty string values
                    for field_key in list(item.keys()):
                        if item[field_key] == "":
                            # For 'role' field, set default to "Founder" if empty
                            if field_key == 'role':
                                item[field_key] = "Founder"
                            else:
                                del item[field_key]
    
    return data

def main():
    company = input(f"{Colors.BLUE}Enter the company name: {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter what information you want about the company: {Colors.RESET}")
    
    serp_results = search_google(objective, company)
    if not serp_results:
        print(f"{Colors.RED}No search results found.{Colors.RESET}")
        return
    
    selected_urls = select_urls_with_gpt(company, objective, serp_results)
    
    if not selected_urls:
        print(f"{Colors.RED}No URLs were selected.{Colors.RESET}")
        return
    
    raw_data = extract_company_info(selected_urls, objective, company, firecrawl_api_key)
    
    if raw_data:
        deduped_data = deduplicate_data(raw_data)
        final_data = consolidate_data(deduped_data, company)
        print(json.dumps(final_data, indent=2))
        print(f"{Colors.GREEN}Extraction completed successfully.{Colors.RESET}")
    else:
        print(f"{Colors.RED}Failed to extract the requested information. Try refining your prompt or choosing a different company.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/gpt-4.1-company-researcher/README.md
================
# GPT-4.1 Company Researcher

A Python tool that uses GPT-4.1, Firecrawl, and SerpAPI to research companies and extract structured information.

## Features

- Search for company information using Google (via SerpAPI)
- Analyze search results with GPT-4.1 to identify relevant URLs
- Extract structured data from websites using Firecrawl
- Deduplicate and consolidate information for higher quality results
- Interactive command-line interface

## Requirements

- Python 3.8+
- OpenAI API key (with GPT-4.1 access)
- Firecrawl API key
- SerpAPI key

## Installation

1. Clone this repository
2. Install dependencies:
   ```
   pip install -r requirements.txt
   ```
3. Copy the `.env.example` file to `.env` and add your API keys:
   ```
   cp .env.example .env
   ```
4. Edit the `.env` file with your actual API keys

## Usage

Run the script:

```bash
python gpt-4.1-company-researcher.py
```

You will be prompted to:

1. Enter a company name
2. Specify what information you want about the company

The tool will then:

- Search for relevant information
- Select the most appropriate URLs using GPT-4.1
- Extract structured data using Firecrawl
- Deduplicate and consolidate the information
- Display the results in JSON format

## Example

```
Enter the company name: Anthropic
Enter what information you want about the company: founders and funding details

# Results will display structured information about Anthropic's founders and funding
```

## License

MIT

================
File: examples/gpt-4.1-company-researcher/requirements.txt
================
python-dotenv==1.0.1
requests==2.31.0
serpapi-python==0.1.5
openai==1.12.0
firecrawl==0.1.2

================
File: examples/gpt-4.1-web-crawler/.env.example
================
# Firecrawl API key
FIRECRAWL_API_KEY=your_firecrawl_api_key_here

# OpenAI API key
OPENAI_API_KEY=your_openai_api_key_here

================
File: examples/gpt-4.1-web-crawler/.gitignore
================
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
*.egg-info/
.installed.cfg
*.egg

# Virtual environment
venv/
ENV/
.env

# IDE specific files
.idea/
.vscode/
*.swp
*.swo

# Logs
*.log

# OS specific files
.DS_Store
Thumbs.db

================
File: examples/gpt-4.1-web-crawler/gpt-4.1-web-crawler.py
================
import os
from firecrawl import FirecrawlApp
import json
from dotenv import load_dotenv
from openai import OpenAI

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")

# Initialize the FirecrawlApp and OpenAI client
app = FirecrawlApp(api_key=firecrawl_api_key)
client = OpenAI(api_key=openai_api_key)

# Find the page that most likely contains the objective
def find_relevant_page_via_map(objective, url, app, client):
    try:
        print(f"{Colors.CYAN}Understood. The objective is: {objective}{Colors.RESET}")
        print(f"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}")
        
        map_prompt = f"""
        The map function generates a list of URLs from a website and it accepts a search parameter. Based on the objective of: {objective}, come up with a 1-2 word search parameter that will help us find the information we need. Only respond with 1-2 words nothing else.
        """

        print(f"{Colors.YELLOW}Analyzing objective to determine optimal search parameter...{Colors.RESET}")
        completion = client.chat.completions.create(
            model="gpt-4.1-2025-04-14",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": map_prompt
                        }
                    ]
                }
            ]
        )

        map_search_parameter = completion.choices[0].message.content
        print(f"{Colors.GREEN}Optimal search parameter identified: {map_search_parameter}{Colors.RESET}")

        print(f"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}")
        map_website = app.map_url(url, params={"search": map_search_parameter})
        
        # Debug print to see the response structure
        print(f"{Colors.MAGENTA}Debug - Map response structure: {json.dumps(map_website, indent=2)}{Colors.RESET}")
        
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")
        
        # Handle the response based on its structure
        if isinstance(map_website, dict):
            # Assuming the links are in a 'urls' or similar key
            links = map_website.get('urls', []) or map_website.get('links', [])
        elif isinstance(map_website, str):
            try:
                parsed = json.loads(map_website)
                links = parsed.get('urls', []) or parsed.get('links', [])
            except json.JSONDecodeError:
                links = []
        else:
            links = map_website if isinstance(map_website, list) else []

        if not links:
            print(f"{Colors.RED}No links found in map response.{Colors.RESET}")
            return None

        rank_prompt = f"""
        Given this list of URLs and the objective: {objective}
        Analyze each URL and rank the top 3 most relevant ones that are most likely to contain the information we need.
        Return your response as a JSON array with exactly 3 objects, each containing:
        - "url": the full URL
        - "relevance_score": number between 0-100 indicating relevance to objective
        - "reason": brief explanation of why this URL is relevant

        Example output:
        [
            {{
                "url": "https://example.com/about",
                "relevance_score": 95,
                "reason": "Main about page containing company information"
            }},
            {{
                "url": "https://example.com/team",
                "relevance_score": 80,
                "reason": "Team page with leadership details"
            }},
            {{
                "url": "https://example.com/contact",
                "relevance_score": 70,
                "reason": "Contact page with location information"
            }}
        ]

        URLs to analyze:
        {json.dumps(links, indent=2)}
        """

        print(f"{Colors.YELLOW}Ranking URLs by relevance to objective...{Colors.RESET}")
        completion = client.chat.completions.create(
            model="gpt-4.1-2025-04-14",
            messages=[
                {
                    "role": "user", 
                    "content": [
                        {
                            "type": "text",
                            "text": rank_prompt
                        }
                    ]
                }
            ]
        )

        try:
            ranked_results = json.loads(completion.choices[0].message.content)
            links = [result["url"] for result in ranked_results]
            
            # Print detailed ranking info
            print(f"{Colors.CYAN}Top 3 ranked URLs:{Colors.RESET}")
            for result in ranked_results:
                print(f"{Colors.GREEN}URL: {result['url']}{Colors.RESET}")
                print(f"{Colors.YELLOW}Relevance Score: {result['relevance_score']}{Colors.RESET}")
                print(f"{Colors.BLUE}Reason: {result['reason']}{Colors.RESET}")
                print("---")

            if not links:
                print(f"{Colors.RED}No relevant links identified.{Colors.RESET}")
                return None

        except (json.JSONDecodeError, KeyError) as e:
            print(f"{Colors.RED}Error parsing ranked results: {str(e)}{Colors.RESET}")
            return None
            
        print(f"{Colors.GREEN}Located {len(links)} relevant links.{Colors.RESET}")
        return links
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}")
        return None
    
# Scrape the top 3 pages and see if the objective is met, if so return in json format else return None
def find_objective_in_top_pages(map_website, objective, app, client):
    try:
        # Get top 3 links from the map result
        if not map_website:
            print(f"{Colors.RED}No links found to analyze.{Colors.RESET}")
            return None
            
        top_links = map_website[:3]
        print(f"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}")
        
        for link in top_links:
            print(f"{Colors.YELLOW}Initiating scrape of page: {link}{Colors.RESET}")
            # Scrape the page
            scrape_result = app.scrape_url(link, params={'formats': ['markdown']})
            print(f"{Colors.GREEN}Page scraping completed successfully.{Colors.RESET}")
     
            
            # Check if objective is met
            check_prompt = f"""
            Given the following scraped content and objective, determine if the objective is met.
            If it is, extract the relevant information in a simple and concise JSON format. Use only the necessary fields and avoid nested structures if possible.
            If the objective is not met with confidence, respond with 'Objective not met'.

            Objective: {objective}
            Scraped content: {scrape_result['markdown']}

            Remember:
            1. Only return JSON if you are confident the objective is fully met.
            2. Keep the JSON structure as simple and flat as possible.
            3. Do not include any explanations or markdown formatting in your response.
            """
        
            completion = client.chat.completions.create(
                model="gpt-4.1-2025-04-14",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": check_prompt
                            }
                        ]
                    }
                ]
            )
            
            result = completion.choices[0].message.content
            
            if result != "Objective not met":
                print(f"{Colors.GREEN}Objective potentially fulfilled. Relevant information identified.{Colors.RESET}")
                try:
                    # Clean up potential markdown formatting or extra text
                    if "```json" in result:
                        result = result.split("```json")[1].split("```")[0].strip()
                    elif "```" in result:
                        result = result.split("```")[1].split("```")[0].strip()
                    
                    # Try to find JSON content if there's explanatory text
                    if "{" in result and "}" in result:
                        start_idx = result.find("{")
                        end_idx = result.rfind("}") + 1
                        if start_idx >= 0 and end_idx > start_idx:
                            result = result[start_idx:end_idx]
                    
                    return json.loads(result)
                except json.JSONDecodeError as e:
                    print(f"{Colors.RED}Error in parsing response: {str(e)}. Proceeding to next page...{Colors.RESET}")
                    # Optionally print the raw response for debugging
                    # print(f"{Colors.MAGENTA}Raw response: {result}{Colors.RESET}")
            else:
                print(f"{Colors.YELLOW}Objective not met on this page. Proceeding to next link...{Colors.RESET}")
        
        print(f"{Colors.RED}All available pages analyzed. Objective not fulfilled in examined content.{Colors.RESET}")
        return None
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during page analysis: {str(e)}{Colors.RESET}")
        return None

# Main function to execute the process
def main():
    # Get user input
    url = input(f"{Colors.BLUE}Enter the website to crawl : {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter your objective: {Colors.RESET}")
    
    print(f"{Colors.YELLOW}Initiating web crawling process...{Colors.RESET}")
    # Find the relevant page
    map_website = find_relevant_page_via_map(objective, url, app, client)
    
    if map_website:
        print(f"{Colors.GREEN}Relevant pages identified. Proceeding with detailed analysis using GPT-4.1...{Colors.RESET}")
        # Find objective in top pages
        result = find_objective_in_top_pages(map_website, objective, app, client)
        
        if result:
            print(f"{Colors.GREEN}Objective successfully fulfilled. Extracted information :{Colors.RESET}")
            print(f"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}")
        else:
            print(f"{Colors.RED}Unable to fulfill the objective with the available content.{Colors.RESET}")
    else:
        print(f"{Colors.RED}No relevant pages identified. Consider refining the search parameters or trying a different website.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/gpt-4.1-web-crawler/README.md
================
# GPT-4.1 Web Crawler

A smart web crawler powered by GPT-4.1 that intelligently searches websites to find specific information based on user objectives.

## Features

- Intelligently maps website content using semantic search
- Ranks website pages by relevance to your objective
- Extracts structured information using GPT-4.1
- Returns results in clean JSON format

## Prerequisites

- Python 3.8+
- Firecrawl API key
- OpenAI API key (with access to GPT-4.1 models)

## Installation

1. Clone this repository:

   ```
   git clone https://github.com/yourusername/gpt-4.1-web-crawler.git
   cd gpt-4.1-web-crawler
   ```

2. Install the required dependencies:

   ```
   pip install -r requirements.txt
   ```

3. Set up environment variables:
   ```
   cp .env.example .env
   ```
   Then edit the `.env` file and add your API keys.

## Usage

Run the script:

```
python gpt-4.1-web-crawler.py
```

The program will prompt you for:

1. The website URL to crawl
2. Your specific objective (what information you want to find)

Example:

```
Enter the website to crawl: https://example.com
Enter your objective: Find the company's leadership team with their roles and short bios
```

The crawler will then:

1. Map the website
2. Identify the most relevant pages
3. Scrape and analyze those pages
4. Return structured information if the objective is met

## How It Works

1. **Mapping**: The crawler uses Firecrawl to map the website structure and find relevant pages based on search terms derived from your objective.

2. **Ranking**: GPT-4.1 analyzes the URLs to determine which pages are most likely to contain the information you're looking for.

3. **Extraction**: The top pages are scraped and analyzed to extract the specific information requested in your objective.

4. **Results**: If found, the information is returned in a clean, structured JSON format.

## License

[MIT License](LICENSE)

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

================
File: examples/gpt-4.1-web-crawler/requirements.txt
================
firecrawl==0.11.0
openai==1.14.0
python-dotenv==1.0.0

================
File: examples/gpt-4.5-web-crawler/gpt-4.5-crawler.py
================
import os
from firecrawl import FirecrawlApp
import json
from dotenv import load_dotenv
from openai import OpenAI

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")

# Initialize the FirecrawlApp and OpenAI client
app = FirecrawlApp(api_key=firecrawl_api_key)
client = OpenAI(api_key=openai_api_key)

# Find the page that most likely contains the objective
def find_relevant_page_via_map(objective, url, app, client):
    try:
        print(f"{Colors.CYAN}Understood. The objective is: {objective}{Colors.RESET}")
        print(f"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}")
        
        map_prompt = f"""
        The map function generates a list of URLs from a website and it accepts a search parameter. Based on the objective of: {objective}, come up with a 1-2 word search parameter that will help us find the information we need. Only respond with 1-2 words nothing else.
        """

        print(f"{Colors.YELLOW}Analyzing objective to determine optimal search parameter...{Colors.RESET}")
        completion = client.chat.completions.create(
            model="gpt-4.5-preview",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": map_prompt
                        }
                    ]
                }
            ]
        )

        map_search_parameter = completion.choices[0].message.content
        print(f"{Colors.GREEN}Optimal search parameter identified: {map_search_parameter}{Colors.RESET}")

        print(f"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}")
        map_website = app.map_url(url, params={"search": map_search_parameter})
        
        # Debug print to see the response structure
        print(f"{Colors.MAGENTA}Debug - Map response structure: {json.dumps(map_website, indent=2)}{Colors.RESET}")
        
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")
        
        # Handle the response based on its structure
        if isinstance(map_website, dict):
            # Assuming the links are in a 'urls' or similar key
            links = map_website.get('urls', []) or map_website.get('links', [])
        elif isinstance(map_website, str):
            try:
                parsed = json.loads(map_website)
                links = parsed.get('urls', []) or parsed.get('links', [])
            except json.JSONDecodeError:
                links = []
        else:
            links = map_website if isinstance(map_website, list) else []

        if not links:
            print(f"{Colors.RED}No links found in map response.{Colors.RESET}")
            return None

        rank_prompt = f"""
        Given this list of URLs and the objective: {objective}
        Analyze each URL and rank the top 3 most relevant ones that are most likely to contain the information we need.
        Return your response as a JSON array with exactly 3 objects, each containing:
        - "url": the full URL
        - "relevance_score": number between 0-100 indicating relevance to objective
        - "reason": brief explanation of why this URL is relevant

        Example output:
        [
            {{
                "url": "https://example.com/about",
                "relevance_score": 95,
                "reason": "Main about page containing company information"
            }},
            {{
                "url": "https://example.com/team",
                "relevance_score": 80,
                "reason": "Team page with leadership details"
            }},
            {{
                "url": "https://example.com/contact",
                "relevance_score": 70,
                "reason": "Contact page with location information"
            }}
        ]

        URLs to analyze:
        {json.dumps(links, indent=2)}
        """

        print(f"{Colors.YELLOW}Ranking URLs by relevance to objective...{Colors.RESET}")
        completion = client.chat.completions.create(
            model="gpt-4.5-preview",
            messages=[
                {
                    "role": "user", 
                    "content": [
                        {
                            "type": "text",
                            "text": rank_prompt
                        }
                    ]
                }
            ]
        )

        try:
            ranked_results = json.loads(completion.choices[0].message.content)
            links = [result["url"] for result in ranked_results]
            
            # Print detailed ranking info
            print(f"{Colors.CYAN}Top 3 ranked URLs:{Colors.RESET}")
            for result in ranked_results:
                print(f"{Colors.GREEN}URL: {result['url']}{Colors.RESET}")
                print(f"{Colors.YELLOW}Relevance Score: {result['relevance_score']}{Colors.RESET}")
                print(f"{Colors.BLUE}Reason: {result['reason']}{Colors.RESET}")
                print("---")

            if not links:
                print(f"{Colors.RED}No relevant links identified.{Colors.RESET}")
                return None

        except (json.JSONDecodeError, KeyError) as e:
            print(f"{Colors.RED}Error parsing ranked results: {str(e)}{Colors.RESET}")
            return None
            
        print(f"{Colors.GREEN}Located {len(links)} relevant links.{Colors.RESET}")
        return links
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}")
        return None
    
# Scrape the top 3 pages and see if the objective is met, if so return in json format else return None
def find_objective_in_top_pages(map_website, objective, app, client):
    try:
        # Get top 3 links from the map result
        if not map_website:
            print(f"{Colors.RED}No links found to analyze.{Colors.RESET}")
            return None
            
        top_links = map_website[:3]
        print(f"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}")
        
        for link in top_links:
            print(f"{Colors.YELLOW}Initiating scrape of page: {link}{Colors.RESET}")
            # Scrape the page
            scrape_result = app.scrape_url(link, params={'formats': ['markdown']})
            print(f"{Colors.GREEN}Page scraping completed successfully.{Colors.RESET}")
     
            
            # Check if objective is met
            check_prompt = f"""
            Given the following scraped content and objective, determine if the objective is met.
            If it is, extract the relevant information in a simple and concise JSON format. Use only the necessary fields and avoid nested structures if possible.
            If the objective is not met with confidence, respond with 'Objective not met'.

            Objective: {objective}
            Scraped content: {scrape_result['markdown']}

            Remember:
            1. Only return JSON if you are confident the objective is fully met.
            2. Keep the JSON structure as simple and flat as possible.
            3. Do not include any explanations or markdown formatting in your response.
            """
        
            completion = client.chat.completions.create(
                model="gpt-4.5-preview",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": check_prompt
                            }
                        ]
                    }
                ]
            )
            
            result = completion.choices[0].message.content
            
            if result != "Objective not met":
                print(f"{Colors.GREEN}Objective potentially fulfilled. Relevant information identified.{Colors.RESET}")
                try:
                    # Clean up potential markdown formatting or extra text
                    if "```json" in result:
                        result = result.split("```json")[1].split("```")[0].strip()
                    elif "```" in result:
                        result = result.split("```")[1].split("```")[0].strip()
                    
                    # Try to find JSON content if there's explanatory text
                    if "{" in result and "}" in result:
                        start_idx = result.find("{")
                        end_idx = result.rfind("}") + 1
                        if start_idx >= 0 and end_idx > start_idx:
                            result = result[start_idx:end_idx]
                    
                    return json.loads(result)
                except json.JSONDecodeError as e:
                    print(f"{Colors.RED}Error in parsing response: {str(e)}. Proceeding to next page...{Colors.RESET}")
                    # Optionally print the raw response for debugging
                    # print(f"{Colors.MAGENTA}Raw response: {result}{Colors.RESET}")
            else:
                print(f"{Colors.YELLOW}Objective not met on this page. Proceeding to next link...{Colors.RESET}")
        
        print(f"{Colors.RED}All available pages analyzed. Objective not fulfilled in examined content.{Colors.RESET}")
        return None
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during page analysis: {str(e)}{Colors.RESET}")
        return None

# Main function to execute the process
def main():
    # Get user input
    url = input(f"{Colors.BLUE}Enter the website to crawl : {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter your objective: {Colors.RESET}")
    
    print(f"{Colors.YELLOW}Initiating web crawling process...{Colors.RESET}")
    # Find the relevant page
    map_website = find_relevant_page_via_map(objective, url, app, client)
    
    if map_website:
        print(f"{Colors.GREEN}Relevant pages identified. Proceeding with detailed analysis using GPT-4.5...{Colors.RESET}")
        # Find objective in top pages
        result = find_objective_in_top_pages(map_website, objective, app, client)
        
        if result:
            print(f"{Colors.GREEN}Objective successfully fulfilled. Extracted information :{Colors.RESET}")
            print(f"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}")
        else:
            print(f"{Colors.RED}Unable to fulfill the objective with the available content.{Colors.RESET}")
    else:
        print(f"{Colors.RED}No relevant pages identified. Consider refining the search parameters or trying a different website.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/grok_web_crawler/grok_web_crawler.py
================
import os
from firecrawl import FirecrawlApp
import json
from dotenv import load_dotenv
import requests

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
grok_api_key = os.getenv("GROK_API_KEY")

# Initialize the FirecrawlApp
app = FirecrawlApp(api_key=firecrawl_api_key)

# Function to make Grok API calls
def grok_completion(prompt):
    url = "https://api.x.ai/v1/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {grok_api_key}"
    }
    data = {
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        "model": "grok-beta",
        "stream": False,
        "temperature": 0
    }
    response = requests.post(url, headers=headers, json=data)
    return response.json()['choices'][0]['message']['content']

# Find the page that most likely contains the objective
def find_relevant_page_via_map(objective, url, app):
    try:
        print(f"{Colors.CYAN}Understood. The objective is: {objective}{Colors.RESET}")
        print(f"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}")
        
        map_prompt = f"""
        The map function generates a list of URLs from a website and it accepts a search parameter. Based on the objective of: {objective}, come up with a 1-2 word search parameter that will help us find the information we need. Only respond with 1-2 words nothing else.
        """

        print(f"{Colors.YELLOW}Analyzing objective to determine optimal search parameter...{Colors.RESET}")
        map_search_parameter = grok_completion(map_prompt)
        print(f"{Colors.GREEN}Optimal search parameter identified: {map_search_parameter}{Colors.RESET}")

        print(f"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}")
        print(f"{Colors.MAGENTA}{map_search_parameter}{Colors.RESET}")
        map_website = app.map_url(url, params={"search": map_search_parameter})
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")
        print(f"{Colors.GREEN}Located {len(map_website['links'])} relevant links.{Colors.RESET}")
        print(f"{Colors.MAGENTA}{map_website}{Colors.RESET}")
        return map_website["links"]
    except Exception as e:
        print(f"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}")
        return None
    
# Scrape the top 3 pages and see if the objective is met, if so return in json format else return None
def find_objective_in_top_pages(map_website, objective, app):
    try:
        print(f"{Colors.MAGENTA}{map_website}{Colors.RESET}")
        # Get top 3 links from the map result
        top_links = map_website[:3] if isinstance(map_website, list) else []
        print(f"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}")
        
        for link in top_links:
            print(f"{Colors.YELLOW}Initiating scrape of page: {link}{Colors.RESET}")
            # Scrape the page
            scrape_result = app.scrape_url(link, params={'formats': ['markdown']})
            print(f"{Colors.GREEN}Page scraping completed successfully.{Colors.RESET}")
     
            
            # Check if objective is met
            check_prompt = f"""
            Given the following scraped content and objective, determine if the objective is met.
            If it is, extract the relevant information in a simple and concise JSON format. Use only the necessary fields and avoid nested structures if possible.
            If the objective is not met with confidence, respond with 'Objective not met'.

            Objective: {objective}
            Scraped content: {scrape_result['markdown']}

            Remember:
            1. Only return JSON if you are confident the objective is fully met.
            2. Keep the JSON structure as simple and flat as possible.
            3. Do not include any explanations or markdown formatting in your response.
            """
        
            result = grok_completion(check_prompt)
            print(f"{Colors.MAGENTA}{result}{Colors.RESET}")
            if result != "Objective not met":
                print(f"{Colors.GREEN}Objective potentially fulfilled. Relevant information identified.{Colors.RESET}")
                try:
                    result = result.replace("```json", "").replace("```", "")
                    return json.loads(result)
                except json.JSONDecodeError:
                    print(f"{Colors.RED}Error in parsing response. Proceeding to next page...{Colors.RESET}")
            else:
                print(f"{Colors.YELLOW}Objective not met on this page. Proceeding to next link...{Colors.RESET}")
        
        print(f"{Colors.RED}All available pages analyzed. Objective not fulfilled in examined content.{Colors.RESET}")
        return None
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during page analysis: {str(e)}{Colors.RESET}")
        return None

# Main function to execute the process
def main():
    # Get user input
    url = input(f"{Colors.BLUE}Enter the website to crawl: {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter your objective: {Colors.RESET}")
    
    print(f"{Colors.YELLOW}Initiating web crawling process...{Colors.RESET}")
    # Find the relevant page
    map_website = find_relevant_page_via_map(objective, url, app)
    
    if map_website:
        print(f"{Colors.GREEN}Relevant pages identified. Proceeding with detailed analysis...{Colors.RESET}")
        # Find objective in top pages
        result = find_objective_in_top_pages(map_website, objective, app)
        
        if result:
            print(f"{Colors.GREEN}Objective successfully fulfilled. Extracted information:{Colors.RESET}")
            print(f"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}")
        else:
            print(f"{Colors.RED}Unable to fulfill the objective with the available content.{Colors.RESET}")
    else:
        print(f"{Colors.RED}No relevant pages identified. Consider refining the search parameters or trying a different website.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/groq_web_crawler/groq_website_analyzer.py
================
import os
from firecrawl import FirecrawlApp
from groq import Groq
from dotenv import load_dotenv

# ANSI color codes for pretty terminal output
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
groq_api_key = os.getenv("GROQ_API_KEY")

# Initialize the FirecrawlApp and Groq client
app = FirecrawlApp(api_key=firecrawl_api_key)
groq_client = Groq(api_key=groq_api_key)

def scrape_website(url):
    """
    Scrape a website using Firecrawl.

    Args:
        url (str): The URL to scrape

    Returns:
        dict: The scraped data
    """
    try:
        print(f"{Colors.YELLOW}Scraping website: {url}{Colors.RESET}")
        scrape_result = app.scrape_url(url, params={'formats': ['markdown']})
        print(f"{Colors.GREEN}Website scraped successfully.{Colors.RESET}")
        return scrape_result
    except Exception as e:
        print(f"{Colors.RED}Error scraping website: {str(e)}{Colors.RESET}")
        return None

def summarize_content(content, model="deepseek-r1-distill-llama-70b"):
    """
    Summarize content using Groq's API.

    Args:
        content (str): The content to summarize
        model (str): The model to use for summarization

    Returns:
        str: The generated summary
    """
    try:
        print(f"{Colors.YELLOW}Generating summary using Groq's {model} model...{Colors.RESET}")

        prompt = f"""
        Please provide a concise summary of the following website content.
        The summary should:
        - Be around 3-5 paragraphs
        - Highlight the main purpose of the website
        - Include key features or offerings
        - Mention any unique selling points

        Content:
        {content}
        """

        completion = groq_client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that specializes in creating concise website summaries."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5,
            max_tokens=1000
        )

        summary = completion.choices[0].message.content
        print(f"{Colors.GREEN}Summary generated successfully.{Colors.RESET}")
        return summary
    except Exception as e:
        print(f"{Colors.RED}Error generating summary: {str(e)}{Colors.RESET}")
        return None

def analyze_website_sentiment(content, model="deepseek-r1-distill-llama-70b"):
    """
    Analyze the sentiment and tone of the website content using Groq's API.

    Args:
        content (str): The content to analyze
        model (str): The model to use for analysis

    Returns:
        dict: The sentiment analysis result
    """
    try:
        print(f"{Colors.YELLOW}Analyzing website sentiment using Groq's {model} model...{Colors.RESET}")

        prompt = f"""
        Please analyze the sentiment and tone of the following website content.
        Return your analysis as a JSON object with the following fields:
        - sentiment: the overall sentiment (positive, neutral, negative)
        - tone_descriptors: an array of 3-5 adjectives describing the tone
        - formality_level: an estimate of how formal the language is (1-10 scale)
        - target_audience: your estimate of who the content is aimed at

        Content:
        {content}
        """

        completion = groq_client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that specializes in content and sentiment analysis."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
            max_tokens=800
        )

        analysis_text = completion.choices[0].message.content
        print(f"{Colors.GREEN}Sentiment analysis completed.{Colors.RESET}")

        # Extract the JSON from the response
        try:
            import re
            import json
            json_match = re.search(r'({.*})', analysis_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                analysis = json.loads(json_str)
                return analysis
            return {"error": "Could not parse JSON from response"}
        except Exception as json_err:
            print(f"{Colors.RED}Error parsing JSON response: {str(json_err)}{Colors.RESET}")
            return {"error": "Could not parse JSON", "raw_response": analysis_text}
    except Exception as e:
        print(f"{Colors.RED}Error analyzing sentiment: {str(e)}{Colors.RESET}")
        return None

def extract_key_topics(content, model="deepseek-r1-distill-llama-70b"):
    """
    Extract key topics and concepts from the website content using Groq's API.

    Args:
        content (str): The content to analyze
        model (str): The model to use for extraction

    Returns:
        list: The extracted key topics
    """
    try:
        print(f"{Colors.YELLOW}Extracting key topics using Groq's {model} model...{Colors.RESET}")

        prompt = f"""
        Extract the 5-8 most important topics or concepts from the following website content.
        For each topic, provide:
        1. A short name (1-3 words)
        2. A brief description (10-15 words)

        Return your response as a simple list in the following format:
        1. [Topic name]: [Brief description]
        2. [Topic name]: [Brief description]

        Content:
        {content}
        """

        completion = groq_client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that specializes in extracting key topics from content."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=800
        )

        topics_text = completion.choices[0].message.content
        print(f"{Colors.GREEN}Key topics extracted successfully.{Colors.RESET}")
        return topics_text
    except Exception as e:
        print(f"{Colors.RED}Error extracting key topics: {str(e)}{Colors.RESET}")
        return None

def main():
    """
    Main function to run the website analysis.
    """
    # Get user input
    url = input(f"{Colors.BLUE}Enter the website URL to analyze: {Colors.RESET}")

    if not url.strip():
        print(f"{Colors.RED}No URL entered. Exiting.{Colors.RESET}")
        return

    # Add http:// prefix if not present
    if not url.startswith('http'):
        url = 'https://' + url

    # Scrape the website
    scrape_result = scrape_website(url)

    if not scrape_result or 'markdown' not in scrape_result:
        print(f"{Colors.RED}Failed to scrape website. Exiting.{Colors.RESET}")
        return

    content = scrape_result['markdown']

    # Ask user which analysis to perform
    print(f"\n{Colors.BLUE}Select an analysis option:{Colors.RESET}")
    print(f"1. Generate a concise summary of the website")
    print(f"2. Analyze the sentiment and tone of the website")
    print(f"3. Extract key topics from the website")
    print(f"4. Perform all analyses")

    option = input(f"{Colors.BLUE}Enter your choice (1-4): {Colors.RESET}")

    # Perform the selected analysis
    if option == '1' or option == '4':
        summary = summarize_content(content)
        if summary:
            print(f"\n{Colors.CYAN}Website Summary:{Colors.RESET}")
            print(f"{Colors.MAGENTA}{summary}{Colors.RESET}")
            print("\n")

    if option == '2' or option == '4':
        sentiment = analyze_website_sentiment(content)
        if sentiment:
            print(f"\n{Colors.CYAN}Sentiment Analysis:{Colors.RESET}")
            print(f"{Colors.MAGENTA}{sentiment}{Colors.RESET}")
            print("\n")

    if option == '3' or option == '4':
        topics = extract_key_topics(content)
        if topics:
            print(f"\n{Colors.CYAN}Key Topics:{Colors.RESET}")
            print(f"{Colors.MAGENTA}{topics}{Colors.RESET}")
            print("\n")

    print(f"{Colors.GREEN}Analysis complete!{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/groq_web_crawler/requirements.txt
================
firecrawl-py
groq
python-dotenv

================
File: examples/hacker_news_scraper/bs4_scraper.py
================
import json
import requests

from bs4 import BeautifulSoup
from pydantic import BaseModel
from datetime import datetime


class NewsItem(BaseModel):
    title: str
    source_url: str
    author: str
    rank: str
    upvotes: str
    date: str


BASE_URL = "https://news.ycombinator.com/"


def get_page_content():
    """
    Send a GET request to the Hacker News homepage and return the HTML content.
    """
    response = requests.get(BASE_URL)
    return response.text


def get_title_rows(html_content, class_name):
    """
    Parse the HTML content and return the first table row.
    """
    soup = BeautifulSoup(html_content, "html.parser")
    title_rows = soup.find("table").find_all("tr", {"class": class_name})
    return title_rows


def get_subtext_rows(html_content):
    """
    Parse the HTML content and return the subtext row.
    """
    soup = BeautifulSoup(html_content, "html.parser")
    subtext_rows = soup.find("table").find_all("td", {"class": "subtext"})
    return subtext_rows


def get_news_data():
    """
    Extract the news data from the table row.
    """
    title_rows = get_title_rows(get_page_content(), "athing submission")
    subtext_rows = get_subtext_rows(get_page_content())

    news_data = []

    for title_row, subtext_row in zip(title_rows, subtext_rows):
        # Extract title information from the title row
        title_span = title_row.find("span", {"class": "titleline"})
        title = title_span.a.text
        url = title_span.a["href"]
        rank = title_row.find("span", {"class": "rank"}).text

        # Extract metadata from the subtext row
        author = BASE_URL + subtext_row.find("a", {"class": "hnuser"})["href"]
        upvotes = subtext_row.find("span", {"class": "score"}).text
        date = subtext_row.find("span", {"class": "age"}).get("title").split(" ")[0]

        news_data.append(
            NewsItem(
                title=title,
                source_url=url,
                author=author,
                rank=rank,
                upvotes=upvotes,
                date=date,
            )
        )

    return news_data


def save_news_data():
    """
    Save the scraped news data to a JSON file with the current date in the filename.
    """

    news_data = get_news_data()
    current_date = datetime.now().strftime("%Y_%m_%d_%H_%M")
    filename = f"hacker_news_data_{current_date}.json"

    with open(filename, "w") as f:
        json.dump([item.dict() for item in news_data], f, indent=4)

    print(f"{datetime.now()}: Successfully saved the news data.")


if __name__ == "__main__":
    save_news_data()

================
File: examples/hacker_news_scraper/firecrawl_scraper.py
================
# firecrawl_scraper.py
import json
from firecrawl import FirecrawlApp
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from typing import List
from datetime import datetime

load_dotenv()

BASE_URL = "https://news.ycombinator.com/"


class NewsItem(BaseModel):
    title: str = Field(description="The title of the news item")
    source_url: str = Field(description="The URL of the news item")
    author: str = Field(
        description="The URL of the post author's profile concatenated with the base URL."
    )
    rank: str = Field(description="The rank of the news item")
    upvotes: str = Field(description="The number of upvotes of the news item")
    date: str = Field(description="The date of the news item.")


class NewsData(BaseModel):
    news_items: List[NewsItem]


def get_firecrawl_news_data():
    app = FirecrawlApp()

    data = app.scrape_url(
        BASE_URL,
        params={
            "formats": ["extract"],
            "extract": {"schema": NewsData.model_json_schema()},
        },
    )

    return data


def save_firecrawl_news_data():
    """
    Save the scraped news data to a JSON file with the current date in the filename.
    """
    # Get the data
    data = get_firecrawl_news_data()
    # Format current date for filename
    date_str = datetime.now().strftime("%Y_%m_%d_%H_%M")
    filename = f"firecrawl_hacker_news_data_{date_str}.json"

    # Save the news items to JSON file
    with open(filename, "w") as f:
        json.dump(data["extract"]["news_items"], f, indent=4)

    print(f"{datetime.now()}: Successfully saved the news data.")


if __name__ == "__main__":
    save_firecrawl_news_data()

================
File: examples/hacker_news_scraper/requirements.txt
================
requests
beautifulsoup4
firecrawl
pydantic
python-dotenv
firecrawl-py

================
File: examples/haiku_web_crawler/haiku_web_crawler.py
================
import os
from firecrawl import FirecrawlApp
import json
from dotenv import load_dotenv
import anthropic
import agentops

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")

# Initialize the FirecrawlApp and OpenAI client
app = FirecrawlApp(api_key=firecrawl_api_key)
client = anthropic.Anthropic(api_key=anthropic_api_key)

# Find the page that most likely contains the objective
def find_relevant_page_via_map(objective, url, app, client):
    try:
        print(f"{Colors.CYAN}Understood. The objective is: {objective}{Colors.RESET}")
        print(f"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}")
        
        map_prompt = f"""
        The map function generates a list of URLs from a website and it accepts a search parameter. Based on the objective of: {objective}, come up with a 1-2 word search parameter that will help us find the information we need. Only respond with 1-2 words nothing else.
        """

        print(f"{Colors.YELLOW}Analyzing objective to determine optimal search parameter...{Colors.RESET}")
        completion = client.messages.create(
            model="claude-3-5-haiku-20241022",
            max_tokens=1000,
            temperature=0,
            system="You are an expert web crawler. Respond with the best search parameter.",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": map_prompt
                        }
                    ]
                }
            ]
        )

        map_search_parameter = completion.content[0].text
        print(f"{Colors.GREEN}Optimal search parameter identified: {map_search_parameter}{Colors.RESET}")

        print(f"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}")
        map_website = app.map_url(url, params={"search": map_search_parameter})
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")
        print(f"{Colors.GREEN}Located {len(map_website['links'])} relevant links.{Colors.RESET}")
        return map_website['links']
    except Exception as e:
        print(f"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}")
        return None
    
# Scrape the top 3 pages and see if the objective is met, if so return in json format else return None
def find_objective_in_top_pages(map_website, objective, app, client):
    try:
        # Get top 2 links from the map result
        top_links = map_website[:2]
        print(f"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}")
        
        # Scrape the pages in batch
        batch_scrape_result = app.batch_scrape_urls(top_links, {'formats': ['markdown']})
        print(f"{Colors.GREEN}Batch page scraping completed successfully.{Colors.RESET}")
        
        
        for scrape_result in batch_scrape_result['data']:

            # Check if objective is met
            check_prompt = f"""
            Given the following scraped content and objective, determine if the objective is met.
            If it is, extract the relevant information in a simple and concise JSON format. Use only the necessary fields and avoid nested structures if possible.
            If the objective is not met with confidence, respond with 'Objective not met'.

            Objective: {objective}
            Scraped content: {scrape_result['markdown']}

            Remember:
            1. Only return JSON if you are confident the objective is fully met.
            2. Keep the JSON structure as simple and flat as possible.
            3. Do not include any explanations or markdown formatting in your response.
            """
        
            completion = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1000,
                temperature=0,
                system="You are an expert web crawler. Respond with the relevant information in JSON format.",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": check_prompt
                            }
                        ]
                    }
                ]
            )
            
            result = completion.content[0].text
            
            if result != "Objective not met":
                print(f"{Colors.GREEN}Objective potentially fulfilled. Relevant information identified.{Colors.RESET}")
                try:
                    return json.loads(result)
                except json.JSONDecodeError:
                    print(f"{Colors.RED}Error in parsing response. Proceeding to next page...{Colors.RESET}")
            else:
                print(f"{Colors.YELLOW}Objective not met on this page. Proceeding to next link...{Colors.RESET}")
        
        print(f"{Colors.RED}All available pages analyzed. Objective not fulfilled in examined content.{Colors.RESET}")
        return None
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during page analysis: {str(e)}{Colors.RESET}")
        return None

# Main function to execute the process
def main():
    # Get user input
    url = input(f"{Colors.BLUE}Enter the website to crawl: {Colors.RESET}")
    if not url.strip():
        url = "https://www.firecrawl.dev/"
    
    objective = input(f"{Colors.BLUE}Enter your objective: {Colors.RESET}")
    if not objective.strip():
        objective = "find me the pricing plans"
    
    print(f"{Colors.YELLOW}Initiating web crawling process...{Colors.RESET}")
    # Find the relevant page
    map_website = find_relevant_page_via_map(objective, url, app, client)
    print(map_website)
    
    if map_website:
        print(f"{Colors.GREEN}Relevant pages identified. Proceeding with detailed analysis...{Colors.RESET}")
        # Find objective in top pages
        result = find_objective_in_top_pages(map_website, objective, app, client)
        
        if result:
            print(f"{Colors.GREEN}Objective successfully fulfilled. Extracted information:{Colors.RESET}")
            print(f"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}")
        else:
            print(f"{Colors.RED}Unable to fulfill the objective with the available content.{Colors.RESET}")
    else:
        print(f"{Colors.RED}No relevant pages identified. Consider refining the search parameters or trying a different website.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/internal_link_assistant/internal_link_assistant.py
================
import os
import json
from firecrawl import FirecrawlApp
from dotenv import load_dotenv
from openai import OpenAI

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")

# Initialize the FirecrawlApp and set OpenAI API key
app = FirecrawlApp(api_key=firecrawl_api_key)
client = OpenAI(api_key=openai_api_key)

def main():
    # Get user input
    blog_url = input("Enter the blog URL: ")

    if not blog_url.strip():
        blog_url = "https://www.firecrawl.dev/blog/how-to-use-openai-o1-reasoning-models-in-applications"

    # Scrape the blog content
    print("Scraping the blog content...")
    blog_scrape_result = app.scrape_url(blog_url, params={'formats': ['markdown']})

    # Get the blog content in markdown format
    blog_content = blog_scrape_result.get('markdown', '')

    # Turn the blog URL into a top-level domain
    top_level_domain = '/'.join(blog_url.split('/')[:3])

    # Map the website to get all links
    print("Mapping the website to get all links...")
    site_map = app.map_url(top_level_domain)

    # Get the list of URLs from the site map
    site_links = site_map.get('links', [])


    prompt = f"""
You are an AI assistant helping to improve a blog post.

Here is the original blog post content:

{blog_content}

Here is a list of other pages on the website:

{json.dumps(site_links, indent=2)}

Please revise the blog post to include internal links to some of these pages where appropriate. Make sure the internal links are relevant and enhance the content.

Only return the revised blog post in markdown format.
"""

    import re

    # Function to count links in a markdown content
    def count_links(markdown_content):
        return len(re.findall(r'\[.*?\]\(.*?\)', markdown_content))

    # Use OpenAI API to get the revised blog post
    print("Generating the revised blog post with internal links...")
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "user",
                "content": prompt
            }
        ],
        prediction={
            "type": "content",
            "content": blog_content
        }
    );

    revised_blog_post = completion.choices[0].message.content

    # Count links in the original and revised blog post
    original_links_count = count_links(blog_content)
    revised_links_count = count_links(revised_blog_post)

    # Output a portion of the revised blog post and link counts
    print("\nRevised blog post (first 500 characters):")
    print(revised_blog_post[:500])
    print(f"\nNumber of links in the original blog post: {original_links_count}")
    print(f"Number of links in the revised blog post: {revised_links_count}")

if __name__ == "__main__":
    main()

================
File: examples/job-resource-analyzer/job-resources-analyzer.py
================
import os
import json
import time
import requests
from dotenv import load_dotenv
from openai import OpenAI
from serpapi.google_search import GoogleSearch

class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    RESET = '\033[0m'

load_dotenv()

# Initialize clients
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
serp_api_key = os.getenv("SERP_API_KEY")

def extract_job_requirements(url, api_key):
    """Extract essential job requirements using Firecrawl."""
    print(f"{Colors.YELLOW}Extracting job requirements...{Colors.RESET}")
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    prompt = """
    Extract only:
    - job_title: position title (string)
    - required_skills: top 5 technical skills (array)
    - experience_level: years required (string)
    """
    
    payload = {
        "urls": [url],
        "prompt": prompt,
        "enableWebSearch": False
    }
    
    try:
        response = requests.post(
            "https://api.firecrawl.dev/v1/extract",
            headers=headers,
            json=payload,
            timeout=30
        )
        
        data = response.json()
        if not data.get('success'):
            return None
        
        return poll_extraction_result(data.get('id'), api_key)

    except Exception as e:
        print(f"{Colors.RED}Error extracting job requirements: {str(e)}{Colors.RESET}")
        return None

def poll_extraction_result(extraction_id, api_key, interval=5, max_attempts=12):
    """Poll for extraction results."""
    url = f"https://api.firecrawl.dev/v1/extract/{extraction_id}"
    headers = {'Authorization': f'Bearer {api_key}'}

    for _ in range(max_attempts):
        try:
            response = requests.get(url, headers=headers, timeout=30)
            data = response.json()
            if data.get('success') and data.get('data'):
                return data['data']
            time.sleep(interval)
        except Exception as e:
            print(f"{Colors.YELLOW}Polling attempt failed, retrying...{Colors.RESET}")
            continue
    return None

def rank_and_summarize_resources(resources, skills):
    """Use OpenAI to rank and summarize learning resources."""
    try:
        # Prepare resources for ranking
        all_resources = []
        for category, items in resources.items():
            for item in items:
                all_resources.append({
                    "category": category,
                    "title": item["title"],
                    "url": item["url"]
                })
        
        # Create prompt for OpenAI
        skills_str = ", ".join(skills)
        prompt = f"""Given these learning resources for skills ({skills_str}), 
        rank them by relevance and quality, and provide a brief summary:

        Resources:
        {json.dumps(all_resources, indent=2)}

        For each resource, provide:
        1. Relevance score (1-10)
        2. Brief summary (max 2 sentences)
        3. Why it's useful for the target skills

        Format as JSON with structure:
        {{
            "ranked_resources": [
                {{
                    "category": "...",
                    "title": "...",
                    "url": "...",
                    "relevance_score": X,
                    "summary": "...",
                    "usefulness": "..."
                }}
            ]
        }}"""

        response = client.chat.completions.create(
            model="o3-mini",
            messages=[
                {"role": "system", "content": "You are a technical learning resource curator."},
                {"role": "user", "content": prompt}
            ],
        )
        
        # Parse and return ranked resources
        ranked_data = json.loads(response.choices[0].message.content)
        return ranked_data["ranked_resources"]

    except Exception as e:
        print(f"{Colors.RED}Error in ranking resources: {str(e)}{Colors.RESET}")
        return None

def get_prep_resources(skills):
    """Get and rank learning resources for top skills."""
    try:
        core_resources = {
            "Tutorials": [],
            "Practice": [],
            "Documentation": []
        }
        
        # Search for top 2 skills to reduce API usage
        top_skills = skills[:2]
        search = GoogleSearch({
            "q": f"learn {' '.join(top_skills)} tutorial practice exercises documentation",
            "api_key": serp_api_key,
            "num": 6
        })
        results = search.get_dict().get("organic_results", [])
        
        for result in results[:6]:
            url = result.get("link", "")
            title = result.get("title", "")
            
            if "tutorial" in title.lower() or "guide" in title.lower():
                core_resources["Tutorials"].append({"title": title, "url": url})
            elif "practice" in title.lower() or "exercise" in title.lower():
                core_resources["Practice"].append({"title": title, "url": url})
            elif "doc" in title.lower() or "reference" in title.lower():
                core_resources["Documentation"].append({"title": title, "url": url})
        
        # Rank and summarize resources
        ranked_resources = rank_and_summarize_resources(core_resources, top_skills)
        return ranked_resources

    except Exception as e:
        print(f"{Colors.RED}Error getting resources: {str(e)}{Colors.RESET}")
        return None

def generate_weekly_plan(skills):
    """Generate a concise weekly preparation plan."""
    weeks = []
    total_skills = len(skills)
    
    # Week 1: Fundamentals
    weeks.append({
        "focus": "Fundamentals",
        "skills": skills[:2] if total_skills >= 2 else skills,
        "tasks": ["Study core concepts", "Complete basic tutorials"]
    })
    
    # Week 2: Advanced Concepts
    if total_skills > 2:
        weeks.append({
            "focus": "Advanced Topics",
            "skills": skills[2:4],
            "tasks": ["Deep dive into advanced features", "Practice exercises"]
        })
    
    # Week 3: Projects & Practice
    weeks.append({
        "focus": "Projects",
        "skills": "All core skills",
        "tasks": ["Build small projects", "Solve practice problems"]
    })
    
    # Week 4: Interview Prep
    weeks.append({
        "focus": "Interview Prep",
        "skills": "All skills",
        "tasks": ["Mock interviews", "Code reviews"]
    })
    
    return weeks

def format_output(job_info, ranked_resources, weeks):
    """Format output in a concise way with ranked resources."""
    output = f"\n{Colors.GREEN}=== Job Preparation Guide ==={Colors.RESET}\n"
    
    # Job Requirements
    output += f"\n{Colors.CYAN}Position:{Colors.RESET} {job_info.get('job_title', 'N/A')}"
    output += f"\n{Colors.CYAN}Experience:{Colors.RESET} {job_info.get('experience_level', 'N/A')}"
    output += f"\n{Colors.CYAN}Key Skills:{Colors.RESET}"
    for skill in job_info.get('required_skills', []):
        output += f"\n- {skill}"

    # Weekly Plan
    output += f"\n\n{Colors.CYAN}4-Week Plan:{Colors.RESET}"
    for i, week in enumerate(weeks, 1):
        output += f"\n\nüìÖ Week {i}: {week['focus']}"
        output += f"\n   Skills: {', '.join(week['skills']) if isinstance(week['skills'], list) else week['skills']}"
        output += f"\n   Tasks: {' ‚Üí '.join(week['tasks'])}"

    # Ranked Learning Resources
    if ranked_resources:
        output += f"\n\n{Colors.CYAN}Top Recommended Resources:{Colors.RESET}"
        
        # Sort resources by relevance score
        sorted_resources = sorted(ranked_resources, key=lambda x: x['relevance_score'], reverse=True)
        
        for res in sorted_resources[:5]:  # Show top 5 resources
            output += f"\n\nüìö {res['title']} (Score: {res['relevance_score']}/10)"
            output += f"\n   {res['summary']}"
            output += f"\n   Why useful: {res['usefulness']}"
            output += f"\n   URL: {res['url']}"

    return output

def main():
    """Main execution function."""
    try:
        job_url = input(f"{Colors.YELLOW}Enter job posting URL: {Colors.RESET}")
        
        # Extract requirements
        job_info = extract_job_requirements(job_url, firecrawl_api_key)
        if not job_info:
            print(f"{Colors.RED}Failed to extract job requirements.{Colors.RESET}")
            return
        
        # Get resources and generate plan
        print(f"{Colors.YELLOW}Finding and ranking preparation resources...{Colors.RESET}")
        resources = get_prep_resources(job_info.get('required_skills', []))
        weeks = generate_weekly_plan(job_info.get('required_skills', []))
        
        # Display results
        print(format_output(job_info, resources, weeks))

    except Exception as e:
        print(f"{Colors.RED}An error occurred: {str(e)}{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/kubernetes/cluster-install/api.yaml
================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api
spec:
  replicas: 1
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
    spec:
      imagePullSecrets:
        - name: docker-registry-secret
      containers:
        - name: api
          image: ghcr.io/winkk-dev/firecrawl:latest
          imagePullPolicy: Always
          args: [ "pnpm", "run", "start:production" ]
          ports:
            - containerPort: 3002
          env:
            - name: FLY_PROCESS_GROUP
              value: "app"
          envFrom:
            - configMapRef:
                name: firecrawl-config
            - secretRef:
                name: firecrawl-secret
          livenessProbe:
            httpGet:
              path: /v0/health/liveness
              port: 3002
            initialDelaySeconds: 30
            periodSeconds: 30
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /v0/health/readiness
              port: 3002
            initialDelaySeconds: 30
            periodSeconds: 30
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3
---
apiVersion: v1
kind: Service
metadata:
  name: api
spec:
  selector:
    app: api
  ports:
    - protocol: TCP
      port: 3002
      targetPort: 3002

================
File: examples/kubernetes/cluster-install/configmap.yaml
================
apiVersion: v1
kind: ConfigMap
metadata:
  name: firecrawl-config
data:
  NUM_WORKERS_PER_QUEUE: "8"
  PORT: "3002"
  HOST: "0.0.0.0"
  REDIS_URL: "redis://redis:6379"
  REDIS_RATE_LIMIT_URL: "redis://redis:6379"
  PLAYWRIGHT_MICROSERVICE_URL: "http://playwright-service:3000"
  USE_DB_AUTHENTICATION: "false"
  HDX_NODE_BETA_MODE: "1"

================
File: examples/kubernetes/cluster-install/playwright-service.yaml
================
apiVersion: v1
kind: ConfigMap
metadata:
  name: playwright-service-config
data:
  PORT: "3000"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: playwright-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: playwright-service
  template:
    metadata:
      labels:
        app: playwright-service
    spec:
      imagePullSecrets:
        - name: docker-registry-secret
      containers:
        - name: playwright-service
          image: ghcr.io/winkk-dev/firecrawl-playwright:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 3000
          envFrom:
            - configMapRef:
                name: playwright-service-config
          livenessProbe:
            httpGet:
              path: /health/liveness
              port: 3000
            initialDelaySeconds: 30
            periodSeconds: 30
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health/readiness
              port: 3000
            initialDelaySeconds: 30
            periodSeconds: 30
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3
---
apiVersion: v1
kind: Service
metadata:
  name: playwright-service
spec:
  selector:
    app: playwright-service
  ports:
    - protocol: TCP
      port: 3000
      targetPort: 3000

================
File: examples/kubernetes/cluster-install/README.md
================
# Install Firecrawl on a Kubernetes Cluster (Simple Version)
# Before installing
1. Set [secret.yaml](secret.yaml) and [configmap.yaml](configmap.yaml) and do not check in secrets
   - **Note**: If `REDIS_PASSWORD` is configured in the secret, please modify the ConfigMap to reflect the following format for `REDIS_URL` and `REDIS_RATE_LIMIT_URL`:
     ```yaml
     REDIS_URL: "redis://:password@host:port"
     REDIS_RATE_LIMIT_URL: "redis://:password@host:port"
     ```
     Replace `password`, `host`, and `port` with the appropriate values.


2. Build Docker images, and host it in your Docker Registry (replace the target registry with your own)
   1. API (which is also used as a worker image)
      1. ```bash
         docker build --no-cache --platform linux/amd64 -t ghcr.io/winkk-dev/firecrawl:latest ../../../apps/api
         docker push ghcr.io/winkk-dev/firecrawl:latest
         ```
   2. Playwright 
      1. ```bash
            docker build --no-cache --platform linux/amd64 -t ghcr.io/winkk-dev/firecrawl-playwright:latest ../../../apps/playwright-service
            docker push ghcr.io/winkk-dev/firecrawl-playwright:latest
         ```
3. Replace the image in [worker.yaml](worker.yaml), [api.yaml](api.yaml) and [playwright-service.yaml](playwright-service.yaml)

## Install
```bash
kubectl apply -f configmap.yaml
kubectl apply -f secret.yaml
kubectl apply -f playwright-service.yaml
kubectl apply -f api.yaml
kubectl apply -f worker.yaml
kubectl apply -f redis.yaml
```


# Port Forwarding for Testing
```bash
kubectl port-forward svc/api 3002:3002 -n dev
```

# Delete Firecrawl
```bash
kubectl delete -f configmap.yaml
kubectl delete -f secret.yaml
kubectl delete -f playwright-service.yaml
kubectl delete -f api.yaml
kubectl delete -f worker.yaml
kubectl delete -f redis.yaml
```

================
File: examples/kubernetes/cluster-install/redis.yaml
================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:alpine
        command: [ "/bin/sh", "-c" ]  # Run a shell script as entrypoint
        args:
          - |
            if [ -n "$REDIS_PASSWORD" ]; then
              echo "Starting Redis with authentication"
              exec redis-server --bind 0.0.0.0 --requirepass "$REDIS_PASSWORD"
            else
              echo "Starting Redis without authentication"
              exec redis-server --bind 0.0.0.0
            fi
        env:
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                name: firecrawl-secret
                key: REDIS_PASSWORD
---
apiVersion: v1
kind: Service
metadata:
  name: redis
spec:
  selector:
    app: redis
  ports:
    - protocol: TCP
      port: 6379
      targetPort: 6379

================
File: examples/kubernetes/cluster-install/secret.yaml
================
apiVersion: v1
kind: Secret
metadata:
  name: firecrawl-secret
type: Opaque
data:
  OPENAI_API_KEY: ""
  SLACK_WEBHOOK_URL: ""
  LLAMAPARSE_API_KEY: ""
  BULL_AUTH_KEY: ""
  TEST_API_KEY: ""
  POSTHOG_API_KEY: ""
  POSTHOG_HOST: ""
  STRIPE_PRICE_ID_STANDARD: ""
  STRIPE_PRICE_ID_SCALE: ""
  FIRE_ENGINE_BETA_URL: ""
  REDIS_PASSWORD: ""

================
File: examples/kubernetes/cluster-install/worker.yaml
================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: worker
spec:
  replicas: 1
  selector:
    matchLabels:
      app: worker
  template:
    metadata:
      labels:
        app: worker
    spec:
      imagePullSecrets:
        - name: docker-registry-secret
      containers:
        - name: worker
          image: ghcr.io/winkk-dev/firecrawl:latest
          imagePullPolicy: Always
          args: [ "pnpm", "run", "workers" ]
          env:
            - name: FLY_PROCESS_GROUP
              value: "worker"
          envFrom:
            - configMapRef:
                name: firecrawl-config
            - secretRef:
                name: firecrawl-secret

================
File: examples/llama-4-maverick-web-crawler/.env.example
================
# Firecrawl API Key
FIRECRAWL_API_KEY=your_firecrawl_api_key_here

# Together AI API Key
TOGETHER_API_KEY=your_together_api_key_here

================
File: examples/llama-4-maverick-web-crawler/.gitignore
================
# Dependencies
node_modules/
venv/
.env
.env.local
.env.*.local

# Build outputs
dist/
build/
*.pyc
__pycache__/
.cache/
.pytest_cache/

# IDE and editor files
.idea/
.vscode/
*.swp
*.swo
.DS_Store
Thumbs.db

# Logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Coverage and test reports
coverage/
.coverage
htmlcov/

# Temporary files
*.tmp
*.temp
.tmp/
temp/

# System files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

================
File: examples/llama-4-maverick-web-crawler/llama4-maverick-web-crawler.py
================
import os
from firecrawl import FirecrawlApp
import json
from dotenv import load_dotenv
from together import Together

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
together_api_key = os.getenv("TOGETHER_API_KEY")

# Initialize the FirecrawlApp and Together client
app = FirecrawlApp(api_key=firecrawl_api_key)
client = Together(api_key=together_api_key)

# Find the page that most likely contains the objective
def find_relevant_page_via_map(objective, url, app, client):
    try:
        print(f"{Colors.CYAN}Understood. The objective is: {objective}{Colors.RESET}")
        print(f"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}")
        
        map_prompt = f"""
        The map function generates a list of URLs from a website and it accepts a search parameter. Based on the objective of: {objective}, come up with a 1-2 word search parameter that will help us find the information we need. Only respond with 1-2 words nothing else.
        """

        print(f"{Colors.YELLOW}Analyzing objective to determine optimal search parameter...{Colors.RESET}")
        completion = client.chat.completions.create(
            model="meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
            messages=[
                {
                    "role": "user",
                    "content": map_prompt
                }
            ]
        )

        map_search_parameter = completion.choices[0].message.content
        print(f"{Colors.GREEN}Optimal search parameter identified: {map_search_parameter}{Colors.RESET}")

        print(f"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}")
        map_website = app.map_url(url, params={"search": map_search_parameter})
        
        # Debug print to see the response structure
        print(f"{Colors.MAGENTA}Debug - Map response structure: {json.dumps(map_website, indent=2)}{Colors.RESET}")
        
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")
        
        # Handle the response based on its structure
        if isinstance(map_website, dict):
            # Assuming the links are in a 'urls' or similar key
            links = map_website.get('urls', []) or map_website.get('links', [])
        elif isinstance(map_website, str):
            try:
                parsed = json.loads(map_website)
                links = parsed.get('urls', []) or parsed.get('links', [])
            except json.JSONDecodeError:
                links = []
        else:
            links = map_website if isinstance(map_website, list) else []

        if not links:
            print(f"{Colors.RED}No links found in map response.{Colors.RESET}")
            return None

        rank_prompt = f"""
        Given this list of URLs and the objective: {objective}
        Analyze each URL and rank the top 3 most relevant ones that are most likely to contain the information we need.
        
        IMPORTANT: You must ONLY return a JSON array with exactly 3 objects. Do not include ANY explanation text.
        Do not include markdown formatting or ```json blocks. Return ONLY the raw JSON array.
        
        Each object in the array must have exactly these fields:
        - "url": the full URL
        - "relevance_score": number between 0-100
        - "reason": brief explanation of why this URL is relevant

        URLs to analyze:
        {json.dumps(links, indent=2)}
        """

        print(f"{Colors.YELLOW}Ranking URLs by relevance to objective...{Colors.RESET}")
        completion = client.chat.completions.create(
            model="meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
            messages=[
                {
                    "role": "user", 
                    "content": rank_prompt
                }
            ]
        )

        # Debug print to see LLM's raw response
        print(f"{Colors.MAGENTA}Debug - LLM raw response:{Colors.RESET}")
        print(f"{Colors.MAGENTA}{completion.choices[0].message.content}{Colors.RESET}")

        try:
            # Try to clean the response by stripping any potential markdown or extra whitespace
            cleaned_response = completion.choices[0].message.content.strip()
            if cleaned_response.startswith("```json"):
                cleaned_response = cleaned_response.split("```json")[1]
            if cleaned_response.endswith("```"):
                cleaned_response = cleaned_response.rsplit("```", 1)[0]
            cleaned_response = cleaned_response.strip()
            
            ranked_results = json.loads(cleaned_response)
            
            # Validate the structure of the results
            if not isinstance(ranked_results, list):
                raise ValueError("Response is not a list")
            
            for result in ranked_results:
                if not all(key in result for key in ["url", "relevance_score", "reason"]):
                    raise ValueError("Response items missing required fields")
            
            links = [result["url"] for result in ranked_results]
            
            # Print detailed ranking info
            print(f"{Colors.CYAN}Top 3 ranked URLs:{Colors.RESET}")
            for result in ranked_results:
                print(f"{Colors.GREEN}URL: {result['url']}{Colors.RESET}")
                print(f"{Colors.YELLOW}Relevance Score: {result['relevance_score']}{Colors.RESET}")
                print(f"{Colors.BLUE}Reason: {result['reason']}{Colors.RESET}")
                print("---")

            if not links:
                print(f"{Colors.RED}No relevant links identified.{Colors.RESET}")
                return None

        except (json.JSONDecodeError, KeyError) as e:
            print(f"{Colors.RED}Error parsing ranked results: {str(e)}{Colors.RESET}")
            return None
            
        print(f"{Colors.GREEN}Located {len(links)} relevant links.{Colors.RESET}")
        return links
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}")
        return None
    
# Scrape the top 3 pages and see if the objective is met, if so return in json format else return None
def find_objective_in_top_pages(map_website, objective, app, client):
    try:
        # Get top 3 links from the map result
        if not map_website:
            print(f"{Colors.RED}No links found to analyze.{Colors.RESET}")
            return None
            
        top_links = map_website[:3]
        print(f"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}")
        
        for link in top_links:
            print(f"{Colors.YELLOW}Initiating scrape of page: {link}{Colors.RESET}")
            scrape_result = app.scrape_url(link, params={'formats': ['markdown']})
            print(f"{Colors.GREEN}Page scraping completed successfully.{Colors.RESET}")
     
            check_prompt = f"""
            Given the following scraped content and objective, determine if the objective is met.
            
            IMPORTANT: You must ONLY return one of two possible responses:
            1. If objective is NOT met, respond with exactly: Objective not met
            2. If objective IS met, respond with ONLY a JSON object containing the relevant information.
               Do not include ANY explanation text, markdown formatting, or ```json blocks.
               Return ONLY the raw JSON object.

            Objective: {objective}
            Scraped content: {scrape_result['markdown']}
            """
        
            completion = client.chat.completions.create(
                model="meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
                messages=[{"role": "user", "content": check_prompt}]
            )
            
            result = completion.choices[0].message.content.strip()
            
            # Clean up the response if it contains markdown formatting
            if result.startswith("```json"):
                result = result.split("```json")[1]
            if result.endswith("```"):
                result = result.rsplit("```", 1)[0]
            result = result.strip()
            
            if result == "Objective not met":
                print(f"{Colors.YELLOW}Objective not met on this page. Proceeding to next link...{Colors.RESET}")
                continue
                
            try:
                json_result = json.loads(result)
                print(f"{Colors.GREEN}Objective fulfilled. Relevant information found.{Colors.RESET}")
                return json_result
            except json.JSONDecodeError as e:
                print(f"{Colors.RED}Error parsing JSON response: {str(e)}{Colors.RESET}")
                print(f"{Colors.MAGENTA}Raw response: {result}{Colors.RESET}")
                continue

        print(f"{Colors.RED}All available pages analyzed. Objective not fulfilled in examined content.{Colors.RESET}")
        return None
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during page analysis: {str(e)}{Colors.RESET}")
        return None

# Main function to execute the process
def main():
    # Get user input
    url = input(f"{Colors.BLUE}Enter the website to crawl : {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter your objective: {Colors.RESET}")
    
    print(f"{Colors.YELLOW}Initiating web crawling process...{Colors.RESET}")
    # Find the relevant page
    map_website = find_relevant_page_via_map(objective, url, app, client)
    
    if map_website:
        print(f"{Colors.GREEN}Relevant pages identified. Proceeding with detailed analysis using Llama 4 Maverick...{Colors.RESET}")
        # Find objective in top pages
        result = find_objective_in_top_pages(map_website, objective, app, client)
        
        if result:
            print(f"{Colors.GREEN}Objective successfully fulfilled. Extracted information :{Colors.RESET}")
            print(f"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}")
        else:
            print(f"{Colors.RED}Unable to fulfill the objective with the available content.{Colors.RESET}")
    else:
        print(f"{Colors.RED}No relevant pages identified. Consider refining the search parameters or trying a different website.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/llama-4-maverick-web-crawler/README.md
================
# Llama 4 Maverick Web Crawler

This project combines the power of Firecrawl for web crawling and Llama 4 Maverick (via Together AI) for intelligent content analysis. It helps you find specific information on websites by crawling pages and analyzing their content using advanced language models.

## Features

- Intelligent URL mapping and relevance ranking
- Content analysis using Llama 4 Maverick model
- Automatic extraction of relevant information
- Color-coded console output for better readability

## Prerequisites

- Python 3.8 or higher
- Firecrawl API key
- Together AI API key

## Installation

1. Clone this repository
2. Install the required packages:
   ```bash
   pip install -r requirements.txt
   ```
3. Copy the `.env.example` file to `.env`:
   ```bash
   cp .env.example .env
   ```
4. Add your API keys to the `.env` file:
   ```
   FIRECRAWL_API_KEY=your_firecrawl_api_key_here
   TOGETHER_API_KEY=your_together_api_key_here
   ```

## Usage

Run the script using:

```bash
python llama4-maverick-web-crawler.py
```

You will be prompted to:

1. Enter the website URL to crawl
2. Specify your objective/what information you're looking for

The script will then:

1. Map the website and find relevant pages
2. Analyze the content using Llama 4 Maverick
3. Extract and return the requested information in JSON format

## Example

```bash
Enter the website to crawl: https://example.com
Enter your objective: Find the company's contact information
```

## Error Handling

The script includes comprehensive error handling and will provide clear feedback if:

- API keys are missing
- Website is inaccessible
- No relevant information is found
- Any other errors occur during execution

## Dependencies

- firecrawl: For web crawling and content extraction
- together: For accessing the Llama 4 Maverick model
- python-dotenv: For environment variable management

## License

[Your chosen license]

================
File: examples/llama-4-maverick-web-crawler/requirements.txt
================
firecrawl>=0.1.0
together>=0.2.0
python-dotenv>=0.19.0

================
File: examples/mistral-small-3.1-crawler/mistral-small-3.1-crawler.py
================
import os
from firecrawl import FirecrawlApp
import json
from dotenv import load_dotenv
from mistralai import Mistral

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
mistral_api_key = os.getenv("MISTRAL_API_KEY")

# Initialize the FirecrawlApp and Mistral client
app = FirecrawlApp(api_key=firecrawl_api_key)
client = Mistral(api_key=mistral_api_key)

# Find the page that most likely contains the objective
def find_relevant_page_via_map(objective, url, app, client):
    try:
        print(f"{Colors.CYAN}Understood. The objective is: {objective}{Colors.RESET}")
        print(f"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}")
        
        map_prompt = f"""
        The map function generates a list of URLs from a website and it accepts a search parameter. Based on the objective of: {objective}, come up with a 1-2 word search parameter that will help us find the information we need. Only respond with 1-2 words nothing else.
        """

        print(f"{Colors.YELLOW}Analyzing objective to determine optimal search parameter...{Colors.RESET}")
        completion = client.chat.complete(
            model="mistral-small-latest",
            messages=[
                {
                    "role": "user",
                    "content": map_prompt
                }
            ]
        )

        map_search_parameter = completion.choices[0].message.content
        print(f"{Colors.GREEN}Optimal search parameter identified: {map_search_parameter}{Colors.RESET}")

        print(f"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}")
        map_website = app.map_url(url, params={"search": map_search_parameter})
        
        # Debug print to see the response structure
        print(f"{Colors.MAGENTA}Debug - Map response structure: {json.dumps(map_website, indent=2)}{Colors.RESET}")
        
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")
        
        # Handle the response based on its structure
        if isinstance(map_website, dict):
            # Assuming the links are in a 'urls' or similar key
            links = map_website.get('urls', []) or map_website.get('links', [])
        elif isinstance(map_website, str):
            try:
                parsed = json.loads(map_website)
                links = parsed.get('urls', []) or parsed.get('links', [])
            except json.JSONDecodeError:
                links = []
        else:
            links = map_website if isinstance(map_website, list) else []

        if not links:
            print(f"{Colors.RED}No links found in map response.{Colors.RESET}")
            return None

        rank_prompt = f"""
        Given this list of URLs and the objective: {objective}
        Analyze each URL and rank the top 3 most relevant ones that are most likely to contain the information we need.
        Return your response as a JSON array with exactly 3 objects, each containing:
        - "url": the full URL
        - "relevance_score": number between 0-100 indicating relevance to objective
        - "reason": brief explanation of why this URL is relevant

        Example output:
        [
            {{
                "url": "https://example.com/about",
                "relevance_score": 95,
                "reason": "Main about page containing company information"
            }},
            {{
                "url": "https://example.com/team",
                "relevance_score": 80,
                "reason": "Team page with leadership details"
            }},
            {{
                "url": "https://example.com/contact",
                "relevance_score": 70,
                "reason": "Contact page with location information"
            }}
        ]

        URLs to analyze:
        {json.dumps(links, indent=2)}
        """

        print(f"{Colors.YELLOW}Ranking URLs by relevance to objective...{Colors.RESET}")
        completion = client.chat.complete(
            model="mistral-small-latest",
            messages=[
                {
                    "role": "user", 
                    "content": rank_prompt
                }
            ]
        )

        # Debug print to see Mistral's raw response
        print(f"{Colors.MAGENTA}Debug - Mistral's raw response:{Colors.RESET}")
        print(f"{Colors.MAGENTA}{completion.choices[0].message.content}{Colors.RESET}")

        try:
            # Try to clean the response by stripping any potential markdown or extra whitespace
            cleaned_response = completion.choices[0].message.content.strip()
            print(f"{Colors.YELLOW}Attempting to extract JSON from response...{Colors.RESET}")
            
            # Extract JSON block if it exists between triple backticks
            if "```json" in cleaned_response and "```" in cleaned_response.split("```json", 1)[1]:
                print(f"{Colors.GREEN}Found JSON code block, extracting...{Colors.RESET}")
                cleaned_response = cleaned_response.split("```json", 1)[1].split("```", 1)[0].strip()
            # If no code blocks but contains square brackets, try to extract just the JSON array
            elif "[" in cleaned_response and "]" in cleaned_response:
                print(f"{Colors.GREEN}Found JSON array markers, extracting...{Colors.RESET}")
                start_idx = cleaned_response.find("[")
                end_idx = cleaned_response.rfind("]") + 1
                cleaned_response = cleaned_response[start_idx:end_idx].strip()
            
            print(f"{Colors.YELLOW}Parsing extracted content: {cleaned_response[:100]}...{Colors.RESET}")
            ranked_results = json.loads(cleaned_response)
            print(f"{Colors.GREEN}Successfully parsed JSON response{Colors.RESET}")
            
            # Validate the structure of the results
            if not isinstance(ranked_results, list):
                raise ValueError("Response is not a list")
            
            for result in ranked_results:
                if not all(key in result for key in ["url", "relevance_score", "reason"]):
                    raise ValueError("Response items missing required fields")
            
            links = [result["url"] for result in ranked_results]
            
            # Print detailed ranking info
            print(f"{Colors.CYAN}Top 3 ranked URLs:{Colors.RESET}")
            for result in ranked_results:
                print(f"{Colors.GREEN}URL: {result['url']}{Colors.RESET}")
                print(f"{Colors.YELLOW}Relevance Score: {result['relevance_score']}{Colors.RESET}")
                print(f"{Colors.BLUE}Reason: {result['reason']}{Colors.RESET}")
                print("---")

            if not links:
                print(f"{Colors.RED}No relevant links identified.{Colors.RESET}")
                return None

        except (json.JSONDecodeError, KeyError) as e:
            print(f"{Colors.RED}Error parsing ranked results: {str(e)}{Colors.RESET}")
            return None
            
        print(f"{Colors.GREEN}Located {len(links)} relevant links.{Colors.RESET}")
        return links
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}")
        return None
    
# Scrape the top 3 pages and see if the objective is met, if so return in json format else return None
def find_objective_in_top_pages(map_website, objective, app, client):
    try:
        # Get top 3 links from the map result
        if not map_website:
            print(f"{Colors.RED}No links found to analyze.{Colors.RESET}")
            return None
            
        top_links = map_website[:3]
        print(f"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}")
        
        for link in top_links:
            print(f"{Colors.YELLOW}Initiating scrape of page: {link}{Colors.RESET}")
            scrape_result = app.scrape_url(link, params={'formats': ['markdown']})
            print(f"{Colors.GREEN}Page scraping completed successfully.{Colors.RESET}")
     
            check_prompt = f"""
            Given the following scraped content and objective, determine if the objective is met.
            If it is, extract the relevant information in a simple and concise JSON format. Use only the necessary fields and avoid nested structures if possible.
            If the objective is not met with confidence, respond with exactly 'Objective not met'.

            Objective: {objective}
            Scraped content: {scrape_result['markdown']}

            Remember:
            1. Only return JSON if you are confident the objective is fully met.
            2. Keep the JSON structure as simple and flat as possible.
            3. If returning JSON, ensure it's valid JSON format without any markdown formatting.
            4. If objective is not met, respond only with 'Objective not met'.
            """
        
            completion = client.chat.complete(
                model="mistral-small-latest",
                messages=[{"role": "user", "content": check_prompt}]
            )
            
            result = completion.choices[0].message.content.strip()
            
            # Clean up the response if it contains markdown formatting
            # Extract JSON block if it exists between triple backticks
            if "```json" in result and "```" in result.split("```json", 1)[1]:
                print(f"{Colors.GREEN}Found JSON code block, extracting...{Colors.RESET}")
                result = result.split("```json", 1)[1].split("```", 1)[0].strip()
            # If no code blocks but contains curly braces, try to extract just the JSON object
            elif result != "Objective not met" and "{" in result and "}" in result:
                print(f"{Colors.GREEN}Found JSON object markers, extracting...{Colors.RESET}")
                start_idx = result.find("{")
                end_idx = result.rfind("}") + 1
                result = result[start_idx:end_idx].strip()
            
            if result == "Objective not met":
                print(f"{Colors.YELLOW}Objective not met on this page. Proceeding to next link...{Colors.RESET}")
                continue
                
            try:
                print(f"{Colors.YELLOW}Parsing extracted content: {result[:100]}...{Colors.RESET}")
                json_result = json.loads(result)
                print(f"{Colors.GREEN}Successfully parsed JSON response{Colors.RESET}")
                print(f"{Colors.GREEN}Objective fulfilled. Relevant information found.{Colors.RESET}")
                return json_result
            except json.JSONDecodeError as e:
                print(f"{Colors.RED}Error parsing JSON response: {str(e)}{Colors.RESET}")
                print(f"{Colors.MAGENTA}Raw response: {result}{Colors.RESET}")
                continue

        print(f"{Colors.RED}All available pages analyzed. Objective not fulfilled in examined content.{Colors.RESET}")
        return None
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during page analysis: {str(e)}{Colors.RESET}")
        return None

# Main function to execute the process
def main():
    # Get user input
    url = input(f"{Colors.BLUE}Enter the website to crawl : {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter your objective: {Colors.RESET}")
    
    print(f"{Colors.YELLOW}Initiating web crawling process...{Colors.RESET}")
    # Find the relevant page
    map_website = find_relevant_page_via_map(objective, url, app, client)
    
    if map_website:
        print(f"{Colors.GREEN}Relevant pages identified. Proceeding with detailed analysis using Mistral Small 3.1...{Colors.RESET}")
        # Find objective in top pages
        result = find_objective_in_top_pages(map_website, objective, app, client)
        
        if result:
            print(f"{Colors.GREEN}Objective successfully fulfilled. Extracted information :{Colors.RESET}")
            print(f"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}")
        else:
            print(f"{Colors.RED}Unable to fulfill the objective with the available content.{Colors.RESET}")
    else:
        print(f"{Colors.RED}No relevant pages identified. Consider refining the search parameters or trying a different website.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/mistral-small-3.1-extractor/mistral-small-3.1-extractor.py
================
import os
import json
import time
import requests
from dotenv import load_dotenv
from serpapi.google_search import GoogleSearch
from mistralai import Mistral

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Initialize clients
mistral_client = Mistral(api_key=os.getenv("MISTRAL_API_KEY"))
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
serp_api_key = os.getenv("SERP_API_KEY")


if not firecrawl_api_key:
    print(f"{Colors.RED}Warning: FIRECRAWL_API_KEY not found in environment variables{Colors.RESET}")

if not os.getenv("MISTRAL_API_KEY"):
    print(f"{Colors.RED}Warning: MISTRAL_API_KEY not found in environment variables{Colors.RESET}")

def search_google(query):
    """Search Google using SerpAPI and return top results."""
    print(f"{Colors.YELLOW}Searching Google for '{query}'...{Colors.RESET}")
    search = GoogleSearch({"q": query, "api_key": serp_api_key})
    return search.get_dict().get("organic_results", [])

def select_urls_with_mistral(company, objective, serp_results):
    """
    Use Mistral Small 3.1 to select URLs from SERP results with enhanced criteria.
    Returns a list of URLs with confidence scores and justifications.
    """
    try:
        serp_data = [{"title": r.get("title"), "link": r.get("link"), "snippet": r.get("snippet")} 
                     for r in serp_results if r.get("link")]

        prompt = (
            "Task: Select the MOST RELIABLE and RELEVANT URLs that contain VERIFIABLE information about the specified company.\n\n"
            "Instructions:\n"
            "1. Analyze the search results for information SPECIFICALLY about the requested objective\n"
            "2. Select ONLY official and highly reliable URLs that DIRECTLY address the requested information\n"
            "3. Prioritize in this exact order:\n"
            "   a. The company's official website sections that specifically address the requested information\n"
            "   b. Official company documents (annual reports, SEC filings, press releases) that contain verifiable data\n"
            "   c. Government databases or regulatory filings that contain verified information\n"
            "   d. Trusted industry databases with cited sources (e.g., Bloomberg, Reuters, industry associations)\n"
            "4. EXCLUDE any sources that:\n"
            "   a. Contain primarily opinions or analysis rather than facts\n"
            "   b. Are outdated (older than 1 year unless historical information is requested)\n"
            "   c. Are from general news sites without specific expertise in the topic\n"
            "   d. Do not cite their sources or methodology\n"
            "   e. Are social media links or user-generated content\n"
            "5. For each URL selected, provide a confidence score (1-10) and brief justification\n"
            "6. Limit selection to 3-5 of the MOST RELIABLE and RELEVANT sources only\n"
            "7. Return a JSON object with the following structure: {\"selected_urls\": [{\"url\": \"url1\", \"confidence\": 9, \"justification\": \"Official company annual report with audited figures\"}]}\n\n"
            f"Company: {company}\n"
            f"Information Needed: {objective}\n"
            f"Search Results: {json.dumps(serp_data, indent=2)}\n\n"
            "Response Format: {\"selected_urls\": [{\"url\": \"https://example.com\", \"confidence\": 9, \"justification\": \"Reason this is reliable\"}]}"
        )

        response = mistral_client.chat.complete(
            model="mistral-small-latest",
            messages=[
                {"role": "user", "content": prompt}
            ]
        )

        # Clean the response text
        cleaned_response = response.choices[0].message.content.strip()
        if cleaned_response.startswith('```'):
            cleaned_response = cleaned_response.split('```')[1]
            if cleaned_response.startswith('json'):
                cleaned_response = cleaned_response[4:]
        cleaned_response = cleaned_response.strip()

        try:
            # Parse JSON response
            result = json.loads(cleaned_response)
            if isinstance(result, dict) and "selected_urls" in result:
                url_data = result["selected_urls"]
                # Extract just the URLs for compatibility with existing code
                urls = [item["url"] for item in url_data if "url" in item]
                
                # Print detailed information about selected URLs
                print(f"{Colors.CYAN}Selected URLs with confidence scores:{Colors.RESET}")
                for item in url_data:
                    if "url" in item and "confidence" in item and "justification" in item:
                        print(f"- {item['url']} (Confidence: {item['confidence']}/10)")
                        print(f"  Justification: {item['justification']}")
            else:
                # Fallback to text parsing
                urls = [line.strip() for line in cleaned_response.split('\n') 
                       if line.strip().startswith(('http://', 'https://'))]
        except json.JSONDecodeError:
            # Fallback to text parsing
            urls = [line.strip() for line in cleaned_response.split('\n') 
                   if line.strip().startswith(('http://', 'https://'))]

        # Clean up URLs
        cleaned_urls = [url.replace('/*', '').rstrip('/') for url in urls]
        cleaned_urls = [url for url in cleaned_urls if url]

        # Limit to top 5 URLs to ensure quality over quantity
        cleaned_urls = cleaned_urls[:5]

        if not cleaned_urls:
            print(f"{Colors.YELLOW}No valid URLs found in response.{Colors.RESET}")
            return []

        # Return the URLs for cross-verification
        return cleaned_urls

    except Exception as e:
        print(f"{Colors.RED}Error selecting URLs: {str(e)}{Colors.RESET}")
        return []

def cross_verify_sources(urls, company, objective):
    """Use Mistral to cross-verify information across selected sources."""
    
    print(f"{Colors.YELLOW}Cross-verifying selected sources...{Colors.RESET}")
    
    verification_prompt = (
        f"Task: Evaluate the reliability and consistency of these sources for information about {company}.\n\n"
        f"Objective: {objective}\n\n"
        f"URLs to evaluate: {json.dumps(urls)}\n\n"
        "Instructions:\n"
        "1. For each URL, identify what makes it reliable or unreliable for the specific objective\n"
        "2. Assess whether these sources are likely to provide consistent or contradictory information\n"
        "3. Identify any potential biases in these sources (e.g., company's own website may present favorable information)\n"
        "4. Recommend the final set of URLs that, when used together, will provide the most accurate and complete information\n"
        "5. IMPORTANT: Only include URLs that are DIRECTLY relevant to the specific objective\n"
        "6. Exclude any URLs that contain primarily general information about the company not related to the objective\n"
        "7. Return a JSON object with: {\"verified_urls\": [\"url1\", \"url2\"], \"verification_notes\": \"explanation\"}\n"
    )
    
    try:
        response = mistral_client.chat.complete(
            model="mistral-small-latest",
            messages=[
                {"role": "user", "content": verification_prompt}
            ]
        )
        
        # Clean the response text
        cleaned_response = response.choices[0].message.content.strip()
        if cleaned_response.startswith('```'):
            cleaned_response = cleaned_response.split('```')[1]
            if cleaned_response.startswith('json'):
                cleaned_response = cleaned_response[4:]
        cleaned_response = cleaned_response.strip()
        
        try:
            # Parse JSON response
            result = json.loads(cleaned_response)
            if isinstance(result, dict) and "verified_urls" in result:
                verified_urls = result["verified_urls"]
                verification_notes = result.get("verification_notes", "")
                
                print(f"{Colors.CYAN}Cross-verification complete:{Colors.RESET}")
                print(f"{Colors.CYAN}Notes: {verification_notes}{Colors.RESET}")
                print(f"{Colors.CYAN}Final verified URLs:{Colors.RESET}")
                for url in verified_urls:
                    print(f"- {url}")
                
                return verified_urls
            else:
                # If JSON parsing fails, return original URLs
                print(f"{Colors.YELLOW}Could not parse cross-verification result. Using original URLs.{Colors.RESET}")
                return urls
        except json.JSONDecodeError:
            # If JSON parsing fails, return original URLs
            print(f"{Colors.YELLOW}Could not parse cross-verification result. Using original URLs.{Colors.RESET}")
            return urls
            
    except Exception as e:
        print(f"{Colors.RED}Error during cross-verification: {str(e)}{Colors.RESET}")
        return urls  # Return original URLs if cross-verification fails

def extract_company_info(urls, prompt, company, api_key):
    """Use requests to call Firecrawl's extract endpoint with selected URLs."""
    print(f"{Colors.YELLOW}Extracting structured data from the provided URLs using Firecrawl...{Colors.RESET}")
    
    # Enhanced prompt for better data quality
    enhanced_prompt = (
        f"Extract accurate and verified information about {company}. "
        f"Specifically focus on: {prompt}. "
        f"IMPORTANT INSTRUCTIONS:\n"
        f"1. Only include information that is EXPLICITLY stated in the source material\n"
        f"2. Do NOT include any speculative information\n"
        f"3. If information conflicts between sources, prioritize information from the company's official website\n"
        f"4. For each piece of information, cite the specific source URL\n"
        f"5. Assign a confidence score (1-10) to each piece of information based on source reliability\n"
        f"6. ONLY include information that is DIRECTLY relevant to the specific request\n"
        f"7. EXCLUDE any tangential or general information about the company not related to the specific request\n"
        f"8. Format the response as a structured JSON with clear categories related to the request\n"
        f"9. For each data point, include both the information and its source in this format: {{\"value\": \"information\", \"source\": \"url\", \"confidence\": 8}}\n"
        f"10. If multiple sources confirm the same information, cite all sources and increase the confidence score\n"
        f"11. If you cannot find specific information requested, explicitly state that it was not found in the sources rather than providing general information"
    )
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    payload = {
        "urls": urls,
        "prompt": enhanced_prompt,
        "enableWebSearch": False  # Changed to False to rely only on verified URLs
    }
    
    try:
        # Print the payload for debugging
        print(f"{Colors.YELLOW}Request payload:{Colors.RESET}")
        print(json.dumps(payload, indent=2))
        
        response = requests.post(
            "https://api.firecrawl.dev/v1/extract",
            headers=headers,
            json=payload,
            timeout=30
        )
        
        # Print detailed response for debugging
        print(f"{Colors.YELLOW}Response status code: {response.status_code}{Colors.RESET}")
        print(f"{Colors.YELLOW}Response headers: {response.headers}{Colors.RESET}")
        
        data = response.json()
        print(f"{Colors.YELLOW}Response body:{Colors.RESET}")
        print(json.dumps(data, indent=2))
        
        if not data.get('success'):
            print(f"{Colors.RED}API returned error: {data.get('error', 'No error message')}{Colors.RESET}")
            return None
        
        extraction_id = data.get('id')
        if not extraction_id:
            print(f"{Colors.RED}No extraction ID found in response.{Colors.RESET}")
            return None

        return poll_firecrawl_result(extraction_id, api_key)

    except requests.exceptions.RequestException as e:
        print(f"{Colors.RED}Request failed: {e}{Colors.RESET}")
        return None
    except json.JSONDecodeError as e:
        print(f"{Colors.RED}Failed to parse response: {e}{Colors.RESET}")
        return None
    except Exception as e:
        print(f"{Colors.RED}Failed to extract data: {e}{Colors.RESET}")
        return None

def poll_firecrawl_result(extraction_id, api_key, interval=5, max_attempts=60):
    """Poll Firecrawl API to get the extraction result."""
    url = f"https://api.firecrawl.dev/v1/extract/{extraction_id}"
    headers = {
        'Authorization': f'Bearer {api_key}'
    }

    print(f"{Colors.YELLOW}Waiting for extraction to complete...{Colors.RESET}")
    
    # Show a simple progress indicator instead of "still processing" messages
    print(f"{Colors.YELLOW}[", end="", flush=True)
    
    for attempt in range(1, max_attempts + 1):
        try:
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()

            if data.get('success') and data.get('data'):
                print(f"]{Colors.RESET}")  # Close the progress indicator
                print(f"{Colors.GREEN}Data successfully extracted:{Colors.RESET}")
                
                # Validate and clean the extracted data
                validated_data = validate_extracted_data(data['data'])
                print(json.dumps(validated_data, indent=2))
                return validated_data
            elif data.get('success') and not data.get('data'):
                # Show a simple progress indicator
                print(f"{Colors.YELLOW}.", end="", flush=True)
                time.sleep(interval)
            else:
                print(f"]{Colors.RESET}")  # Close the progress indicator
                print(f"{Colors.RED}API Error: {data.get('error', 'No error message provided')}{Colors.RESET}")
                return None

        except requests.exceptions.RequestException as e:
            print(f"]{Colors.RESET}")  # Close the progress indicator
            print(f"{Colors.RED}Request error: {str(e)}{Colors.RESET}")
            return None
        except json.JSONDecodeError as e:
            print(f"]{Colors.RESET}")  # Close the progress indicator
            print(f"{Colors.RED}JSON parsing error: {str(e)}{Colors.RESET}")
            return None
        except Exception as e:
            print(f"]{Colors.RESET}")  # Close the progress indicator
            print(f"{Colors.RED}Unexpected error: {str(e)}{Colors.RESET}")
            return None

    print(f"]{Colors.RESET}")  # Close the progress indicator
    print(f"{Colors.RED}Max polling attempts reached. Extraction did not complete in time.{Colors.RESET}")
    return None

def validate_extracted_data(data):
    """Validate and clean the extracted data to reduce misinformation."""
    if not data or not isinstance(data, dict):
        return data
    
    # Look for confidence scores or source information if available
    validated_data = {}
    
    for key, value in data.items():
        # Skip entries that indicate uncertainty
        if isinstance(value, str) and any(term in value.lower() for term in ["unknown", "unclear", "not specified", "not found", "couldn't find"]):
            continue
            
        # Keep entries with clear information
        validated_data[key] = value
    
    return validated_data

def main():
    company = input(f"{Colors.BLUE}Enter the company name: {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter what information you want about the company: {Colors.RESET}")
    
    # Add more specific search terms for better results
    search_query = f"{company} {objective}"
    # print(f"{Colors.YELLOW}Searching Google for '{search_query}'...{Colors.RESET}")
    serp_results = search_google(search_query)
    
    if not serp_results:
        # Fallback to just company name
        print(f"{Colors.YELLOW}No results found. Trying broader search...{Colors.RESET}")
        serp_results = search_google(company)
        
    if not serp_results:
        print(f"{Colors.RED}No search results found.{Colors.RESET}")
        return
    
    # Select URLs with Mistral
    selected_urls = select_urls_with_mistral(company, objective, serp_results)
    
    if not selected_urls:
        print(f"{Colors.RED}No URLs were selected.{Colors.RESET}")
        return
    
    # Cross-verify the selected sources
    verified_urls = cross_verify_sources(selected_urls, company, objective)
    
    if not verified_urls:
        print(f"{Colors.YELLOW}No URLs were verified. Using original selected URLs.{Colors.RESET}")
        verified_urls = selected_urls
    
    data = extract_company_info(verified_urls, objective, company, firecrawl_api_key)
    
    if data:
        print(f"{Colors.GREEN}Extraction completed successfully.{Colors.RESET}")
    else:
        print(f"{Colors.RED}Failed to extract the requested information. Try refining your prompt or choosing a different company.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/o1_job_recommender/o1_job_recommender.py
================
# %%
# %%
import os
import requests
import json
from dotenv import load_dotenv
from openai import OpenAI

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'
# Load environment variables
load_dotenv()

# Initialize the FirecrawlApp with your API key
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Set the jobs page URL
jobs_page_url = "https://openai.com/careers/search"

# Resume
resume_paste = """"
Eric Ciarla
Co-Founder @ Firecrawl
San Francisco, California, United States
Summary
Building‚Ä¶
Experience
Firecrawl
Co-Founder
April 2024 - Present (6 months)
San Francisco, California, United States
Firecrawl by Mendable. Building data extraction infrastructure for AI. Used by
Amazon, Zapier, and Nvidia (YC S22)
Mendable
2 years 7 months
Co-Founder @ Mendable.ai
March 2022 - Present (2 years 7 months)
San Francisco, California, United States
- Built an AI powered search platform that that served millions of queries for
hundreds of customers (YC S22)
- We were one of the first LLM powered apps adopted by industry leaders like
Coinbase, Snap, DoorDash, and MongoDB
Co-Founder @ SideGuide
March 2022 - Present (2 years 7 months)
San Francisco, California, United States
- Built and scaled an online course platform with a community of over 50,000
developers
- Selected for Y Combinator S22 batch, 2% acceptance rate
Fracta
Data Engineer
2022 - 2022 (less than a year)
Palo Alto, California, United States
- Demoed tool during sales calls and provided technical support during the
entire customer lifecycle
Page 1 of 2
- Mined, wrangled, & visualized geospatial and water utility data for predictive
analytics & ML workflows (Python, QGIS)
Ford Motor Company
Data Scientist
2021 - 2021 (less than a year)
Dearborn, Michigan, United States
- Extracted, cleaned, and joined data from multiple sources using SQL,
Hadoop, and Alteryx
- Used Bayesian Network Structure Learning (BNLearn, R) to uncover the
relationships between survey free response verbatim topics (derived from
natural language processing models) and numerical customer experience
scores
MDRemindME
Co-Founder
2018 - 2020 (2 years)
Durham, New Hampshire, United States
- Founded and led a healthtech startup aimed at improving patient adherence
to treatment plans through an innovative engagement and retention tool
- Piloted the product with healthcare providers and patients, gathering critical
insights to refine functionality and enhance user experience
- Secured funding through National Science Foundation I-CORPS Grant and
UNH Entrepreneurship Center Seed Grant
Education
Y Combinator
S22
University of New Hampshire
Economics and Philosophy
"""

# First, scrape the jobs page using Firecrawl
try:
    response = requests.post(
        "https://api.firecrawl.dev/v1/scrape",
        headers={
            "Content-Type": "application/json",
            "Authorization": f"Bearer {firecrawl_api_key}"
        },
        json={
            "url": jobs_page_url,
            "formats": ["markdown"]
        }
    )
    
    if response.status_code == 200:
        result = response.json()
        if result.get('success'):
            html_content = result['data']['markdown']
            # Define the O1 prompt for extracting apply links
            prompt = f"""
            Extract up to 30 job application links from the given markdown content.
            Return the result as a JSON object with a single key 'apply_links' containing an array of strings (the links).
            The output should be a valid JSON object, with no additional text.
            Do not include any JSON markdown formatting or code block indicators.
            Provide only the raw JSON object as the response.

            Example of the expected format:
            {{"apply_links": ["https://example.com/job1", "https://example.com/job2", ...]}}

            Markdown content:
            {html_content[:100000]}
            """
            print(f"{Colors.GREEN}Successfully scraped the jobs page{Colors.RESET}")
        else:
            print(f"{Colors.RED}Failed to scrape the jobs page: {result.get('message', 'Unknown error')}{Colors.RESET}")
            html_content = ""
    else:
        print(f"{Colors.RED}Error {response.status_code}: {response.text}{Colors.RESET}")
        html_content = ""
except requests.RequestException as e:
    print(f"{Colors.RED}An error occurred while scraping: {str(e)}{Colors.RESET}")
    html_content = ""
except json.JSONDecodeError as e:
    print(f"{Colors.RED}Error decoding JSON response: {str(e)}{Colors.RESET}")
    html_content = ""
except Exception as e:
    print(f"{Colors.RED}An unexpected error occurred while scraping: {str(e)}{Colors.RESET}")
    html_content = ""

# Extract apply links from the scraped HTML using O1
apply_links = []
if html_content:
    try:
        completion = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )
        
        if completion.choices:
            print(completion.choices[0].message.content)
            result = json.loads(completion.choices[0].message.content.strip())
        
            apply_links = result['apply_links']
            print(f"{Colors.GREEN}Successfully extracted {len(apply_links)} apply links{Colors.RESET}")
        else:
            print(f"{Colors.RED}No apply links extracted{Colors.RESET}")
    except json.JSONDecodeError as e:
        print(f"{Colors.RED}Error decoding JSON from OpenAI response: {str(e)}{Colors.RESET}")
    except KeyError as e:
        print(f"{Colors.RED}Expected key not found in OpenAI response: {str(e)}{Colors.RESET}")
    except Exception as e:
        print(f"{Colors.RED}An unexpected error occurred during extraction: {str(e)}{Colors.RESET}")
else:
    print(f"{Colors.RED}No HTML content to process{Colors.RESET}")

# Initialize a list to store the extracted data
extracted_data = []


# %%
print(f"{Colors.CYAN}Apply links:{Colors.RESET}")
for link in apply_links:
    print(f"{Colors.YELLOW}{link}{Colors.RESET}")

# %%
# Process each apply link
for index, link in enumerate(apply_links):
    try:
        response = requests.post(
            "https://api.firecrawl.dev/v1/scrape",
            headers={
                "Content-Type": "application/json",
                "Authorization": f"Bearer {firecrawl_api_key}"
            },
            json={
                "url": link,
                "formats": ["extract"],
                "actions": [{
                    "type": "click",
                    "selector": "#job-overview"
                }],
                "extract": {
                    "schema": {
                        "type": "object",
                        "properties": {
                            "job_title": {"type": "string"},
                            "sub_division_of_organization": {"type": "string"},
                            "key_skills": {"type": "array", "items": {"type": "string"}},
                            "compensation": {"type": "string"},
                            "location": {"type": "string"},
                            "apply_link": {"type": "string"}
                        },
                        "required": ["job_title", "sub_division_of_organization", "key_skills", "compensation", "location", "apply_link"]
                    }
                }
            }
        )
        
        if response.status_code == 200:
            result = response.json()
            if result.get('success'):
                extracted_data.append(result['data']['extract'])
                print(f"{Colors.GREEN}Data extracted for job {index}{Colors.RESET}")
            else:
                print(f"")
        else:
            print(f"")
    except Exception as e:
        print(f"")


# %%
# %%
# Print the extracted data
print(f"{Colors.CYAN}Extracted data:{Colors.RESET}")
for job in extracted_data:
    print(json.dumps(job, indent=2))
    print(f"{Colors.MAGENTA}{'-' * 50}{Colors.RESET}")


# %%




# Use o1-preview to choose which jobs should be applied to based on the resume
prompt = f"""
Please analyze the resume and job listings, and return a JSON list of the top 3 roles that best fit the candidate's experience and skills. Include only the job title, compensation, and apply link for each recommended role. The output should be a valid JSON array of objects in the following format, with no additional text:

[
  {{
    "job_title": "Job Title",
    "compensation": "Compensation (if available, otherwise empty string)",
    "apply_link": "Application URL"
  }},
  ...
]

Based on the following resume:
{resume_paste}

And the following job listings:
{json.dumps(extracted_data, indent=2)}
"""

completion = client.chat.completions.create(
    model="o1-preview",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": prompt
                }
            ]
        }
    ]
)

recommended_jobs = json.loads(completion.choices[0].message.content.strip())

print(f"{Colors.CYAN}Recommended jobs:{Colors.RESET}")
print(json.dumps(recommended_jobs, indent=2))

================
File: examples/o1_web_crawler/o1_web_crawler.py
================
import os
from firecrawl import FirecrawlApp
import json
from dotenv import load_dotenv
from openai import OpenAI

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")

# Initialize the FirecrawlApp and OpenAI client
app = FirecrawlApp(api_key=firecrawl_api_key)
client = OpenAI(api_key=openai_api_key)

# Find the page that most likely contains the objective
def find_relevant_page_via_map(objective, url, app, client):
    try:
        print(f"{Colors.CYAN}Understood. The objective is: {objective}{Colors.RESET}")
        print(f"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}")
        
        map_prompt = f"""
        The map function generates a list of URLs from a website and it accepts a search parameter. Based on the objective of: {objective}, come up with a 1-2 word search parameter that will help us find the information we need. Only respond with 1-2 words nothing else.
        """

        print(f"{Colors.YELLOW}Analyzing objective to determine optimal search parameter...{Colors.RESET}")
        completion = client.chat.completions.create(
            model="o1-preview",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": map_prompt
                        }
                    ]
                }
            ]
        )

        map_search_parameter = completion.choices[0].message.content
        print(f"{Colors.GREEN}Optimal search parameter identified: {map_search_parameter}{Colors.RESET}")

        print(f"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}")
        map_website = app.map_url(url, params={"search": map_search_parameter})
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")
        print(f"{Colors.GREEN}Located {len(map_website)} relevant links.{Colors.RESET}")
        return map_website
    except Exception as e:
        print(f"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}")
        return None
    
# Scrape the top 3 pages and see if the objective is met, if so return in json format else return None
def find_objective_in_top_pages(map_website, objective, app, client):
    try:
        # Get top 3 links from the map result
        top_links = map_website[:3] if isinstance(map_website, list) else []
        print(f"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}")
        
        for link in top_links:
            print(f"{Colors.YELLOW}Initiating scrape of page: {link}{Colors.RESET}")
            # Scrape the page
            scrape_result = app.scrape_url(link, params={'formats': ['markdown']})
            print(f"{Colors.GREEN}Page scraping completed successfully.{Colors.RESET}")
     
            
            # Check if objective is met
            check_prompt = f"""
            Given the following scraped content and objective, determine if the objective is met.
            If it is, extract the relevant information in a simple and concise JSON format. Use only the necessary fields and avoid nested structures if possible.
            If the objective is not met with confidence, respond with 'Objective not met'.

            Objective: {objective}
            Scraped content: {scrape_result['markdown']}

            Remember:
            1. Only return JSON if you are confident the objective is fully met.
            2. Keep the JSON structure as simple and flat as possible.
            3. Do not include any explanations or markdown formatting in your response.
            """
        
            completion = client.chat.completions.create(
            model="o1-preview",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": check_prompt
                        }
                    ]
                }
                ]
            )
            
            result = completion.choices[0].message.content
            
            if result != "Objective not met":
                print(f"{Colors.GREEN}Objective potentially fulfilled. Relevant information identified.{Colors.RESET}")
                try:
                    return json.loads(result)
                except json.JSONDecodeError:
                    print(f"{Colors.RED}Error in parsing response. Proceeding to next page...{Colors.RESET}")
            else:
                print(f"{Colors.YELLOW}Objective not met on this page. Proceeding to next link...{Colors.RESET}")
        
        print(f"{Colors.RED}All available pages analyzed. Objective not fulfilled in examined content.{Colors.RESET}")
        return None
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during page analysis: {str(e)}{Colors.RESET}")
        return None

# Main function to execute the process
def main():
    # Get user input
    url = input(f"{Colors.BLUE}Enter the website to crawl: {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter your objective: {Colors.RESET}")
    
    print(f"{Colors.YELLOW}Initiating web crawling process...{Colors.RESET}")
    # Find the relevant page
    map_website = find_relevant_page_via_map(objective, url, app, client)
    
    if map_website:
        print(f"{Colors.GREEN}Relevant pages identified. Proceeding with detailed analysis...{Colors.RESET}")
        # Find objective in top pages
        result = find_objective_in_top_pages(map_website, objective, app, client)
        
        if result:
            print(f"{Colors.GREEN}Objective successfully fulfilled. Extracted information:{Colors.RESET}")
            print(f"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}")
        else:
            print(f"{Colors.RED}Unable to fulfill the objective with the available content.{Colors.RESET}")
    else:
        print(f"{Colors.RED}No relevant pages identified. Consider refining the search parameters or trying a different website.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/o1_web_extractor/o1_web_extractor.py
================
import os
import json
import requests
from dotenv import load_dotenv
from openai import OpenAI
from serpapi import GoogleSearch

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Initialize clients
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")

def search_google(query):
    """Search Google using SerpAPI and return top results."""
    print(f"{Colors.YELLOW}Searching Google for '{query}'...{Colors.RESET}")
    search = GoogleSearch({"q": query, "api_key": os.getenv("SERP_API_KEY")})
    return search.get_dict().get("organic_results", [])

def select_urls_with_o1(company, objective, serp_results):
    """
    Use O1 to select the most relevant URLs from SERP results for the given company and objective.
    Returns a JSON object with a "selected_urls" property that is an array of strings.
    """
    try:
        # Prepare the data for O1
        serp_data = [{"title": r.get("title"), "link": r.get("link"), "snippet": r.get("snippet")} 
                     for r in serp_results if r.get("link")]

        response = client.chat.completions.create(
            model="o1-2024-12-17",
            messages=[
                {
                    "role": "developer",
                    "content": "You select URLs from the SERP results relevant to the company and objective."
                },
                {
                    "role": "user",
                    "content": (
                        f"Company: {company}\n"
                        f"Objective: {objective}\n"
                        f"SERP Results: {json.dumps(serp_data)}\n\n"
                        "Return a JSON object with a property 'selected_urls' that contains an array "
                        "of URLs most likely to help meet the objective. Add a /* to the end of the URL if you think it should search all of the pages in the site. Do not return any social media links. For example: {\"selected_urls\": [\"https://example.com\", \"https://example2.com\"]}"
                    )
                }
            ],
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "selected_urls_object",
                    "schema": {
                        "type": "object",
                        "properties": {
                            "selected_urls": {
                                "type": "array",
                                "items": {
                                    "type": "string"
                                }
                            }
                        },
                        "required": ["selected_urls"],
                        "additionalProperties": False
                    }
                }
            }
        )

        # The response is guaranteed to follow the specified JSON schema
        result = json.loads(response.choices[0].message.content)
        urls = result.get("selected_urls", [])
        return urls

    except Exception as e:
        print(f"{Colors.RED}Error selecting URLs with O1: {e}{Colors.RESET}")
        return []



def extract_company_info(urls, prompt, company, api_key):
    """Use requests to call Firecrawl's extract endpoint with selected URLs."""
    print(f"{Colors.YELLOW}Extracting structured data from the provided URLs using Firecrawl...{Colors.RESET}")
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    payload = {
        "urls": urls,
        "prompt": prompt + " for " + company
    }
    
    try:
        response = requests.post(
            "https://api.firecrawl.dev/v1/extract",
            headers=headers,
            json=payload
        )
        response.raise_for_status()
        data = response.json()
        return data
    except Exception as e:
        print(f"{Colors.RED}Failed to extract data: {e}{Colors.RESET}")
        return None

def main():
    company = input(f"{Colors.BLUE}Enter the company name: {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter what information you want about the company: {Colors.RESET}")
    
    serp_results = search_google(f"{company}")
    if not serp_results:
        print(f"{Colors.RED}No search results found.{Colors.RESET}")
        return
    
    # Ask O1 to select URLs
    selected_urls = select_urls_with_o1(company, objective, serp_results)
    
    if not selected_urls:
        print(f"{Colors.RED}O1 did not return any URLs.{Colors.RESET}")
        return
    
    print(f"{Colors.CYAN}Selected URLs for extraction by O1:{Colors.RESET}")
    for url in selected_urls:
        print(f"- {url}")

    data = extract_company_info(selected_urls, objective, company, firecrawl_api_key)
    
    if data and data.get('success') and data.get('data'):
        print(f"{Colors.GREEN}Data successfully extracted:{Colors.RESET}")
        print(json.dumps(data['data'], indent=2))
    else:
        print(f"{Colors.RED}Failed to extract the requested information. Try refining your prompt or choosing a different company.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/o3-mini_company_researcher/o3-mini_company_researcher.py
================
import os
import json
import time
import requests
from dotenv import load_dotenv
from openai import OpenAI
from serpapi.google_search import GoogleSearch

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Initialize clients
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
serp_api_key = os.getenv("SERP_API_KEY")

def search_google(query):
    """Search Google using SerpAPI and return top results."""
    print(f"{Colors.YELLOW}Searching Google for '{query}'...{Colors.RESET}")
    search = GoogleSearch({"q": query, "api_key": serp_api_key})
    return search.get_dict().get("organic_results", [])

def select_urls_with_r1(company, objective, serp_results):
    """
    Use R1's approach with o3-mini to select URLs from SERP results.
    Returns a list of URLs.
    """
    try:
        # Prepare the data for o3-mini (keeping R1's structure)
        serp_data = [{"title": r.get("title"), "link": r.get("link"), "snippet": r.get("snippet")} 
                     for r in serp_results if r.get("link")]

        response = client.chat.completions.create(
            model="o3-mini",
            messages=[
                {
                    "role": "system",
                    "content": "You are a URL selector that always responds with valid JSON. You select URLs from the SERP results relevant to the company and objective. Your response must be a JSON object with a 'selected_urls' array property containing strings."
                },
                {
                    "role": "user",
                    "content": (
                        f"Company: {company}\n"
                        f"Objective: {objective}\n"
                        f"SERP Results: {json.dumps(serp_data)}\n\n"
                        "Return a JSON object with a property 'selected_urls' that contains an array "
                        "of URLs most likely to help meet the objective. Add a /* to the end of the URL if you think it should search all of the pages in the site. Do not return any social media links. For example: {\"selected_urls\": [\"https://example.com\", \"https://example2.com\"]}"
                    )
                }
            ]
        )

        try:
            # First try to parse as JSON
            result = json.loads(response.choices[0].message.content)
            if isinstance(result, dict) and "selected_urls" in result:
                urls = result["selected_urls"]
            else:
                # If JSON doesn't have the expected structure, fall back to text parsing
                response_text = response.choices[0].message.content
                urls = [line.strip() for line in response_text.split('\n') 
                       if line.strip().startswith(('http://', 'https://'))]
        except json.JSONDecodeError:
            # If JSON parsing fails, fall back to text parsing
            response_text = response.choices[0].message.content
            urls = [line.strip() for line in response_text.split('\n') 
                   if line.strip().startswith(('http://', 'https://'))]

        # Clean up URLs - remove wildcards and trailing slashes
        cleaned_urls = [url.replace('/*', '').rstrip('/') for url in urls]
        cleaned_urls = [url for url in cleaned_urls if url]

        if not cleaned_urls:
            print(f"{Colors.YELLOW}No valid URLs found.{Colors.RESET}")
            return []

        print(f"{Colors.CYAN}Selected URLs for extraction:{Colors.RESET}")
        for url in cleaned_urls:
            print(f"- {url}")

        return cleaned_urls

    except Exception as e:
        print(f"{Colors.RED}Error selecting URLs: {e}{Colors.RESET}")
        return []

def extract_company_info(urls, prompt, company, api_key):
    """Use requests to call Firecrawl's extract endpoint with selected URLs."""
    print(f"{Colors.YELLOW}Extracting structured data from the provided URLs using Firecrawl...{Colors.RESET}")
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    payload = {
        "urls": urls,
        "prompt": prompt + " for " + company,
        "enableWebSearch": True
    }
    
    try:
        response = requests.post(
            "https://api.firecrawl.dev/v1/extract",
            headers=headers,
            json=payload,
            timeout=30
        )
        
        data = response.json()
        
        if not data.get('success'):
            print(f"{Colors.RED}API returned error: {data.get('error', 'No error message')}{Colors.RESET}")
            return None
        
        extraction_id = data.get('id')
        if not extraction_id:
            print(f"{Colors.RED}No extraction ID found in response.{Colors.RESET}")
            return None

        return poll_firecrawl_result(extraction_id, api_key)

    except requests.exceptions.RequestException as e:
        print(f"{Colors.RED}Request failed: {e}{Colors.RESET}")
        return None
    except json.JSONDecodeError as e:
        print(f"{Colors.RED}Failed to parse response: {e}{Colors.RESET}")
        return None
    except Exception as e:
        print(f"{Colors.RED}Failed to extract data: {e}{Colors.RESET}")
        return None

def poll_firecrawl_result(extraction_id, api_key, interval=5, max_attempts=36):
    """Poll Firecrawl API to get the extraction result."""
    url = f"https://api.firecrawl.dev/v1/extract/{extraction_id}"
    headers = {
        'Authorization': f'Bearer {api_key}'
    }

    for attempt in range(1, max_attempts + 1):
        try:
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()

            if data.get('success') and data.get('data'):
                print(f"{Colors.GREEN}Data successfully extracted:{Colors.RESET}")
                print(json.dumps(data['data'], indent=2))
                return data['data']
            elif data.get('success') and not data.get('data'):
                time.sleep(interval)
            else:
                print(f"{Colors.RED}API Error: {data.get('error', 'No error message provided')}{Colors.RESET}")
                return None

        except requests.exceptions.RequestException:
            return None
        except json.JSONDecodeError:
            return None
        except Exception:
            return None

    print(f"{Colors.RED}Max polling attempts reached. Extraction did not complete in time.{Colors.RESET}")
    return None

def main():
    company = input(f"{Colors.BLUE}Enter the company name: {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter what information you want about the company: {Colors.RESET}")
    
    serp_results = search_google(f"{company}")
    if not serp_results:
        print(f"{Colors.RED}No search results found.{Colors.RESET}")
        return
    
    # Use R1's URL selection approach with o3-mini model
    selected_urls = select_urls_with_r1(company, objective, serp_results)
    
    if not selected_urls:
        print(f"{Colors.RED}No URLs were selected.{Colors.RESET}")
        return
    
    data = extract_company_info(selected_urls, objective, company, firecrawl_api_key)
    
    if data:
        print(f"{Colors.GREEN}Extraction completed successfully.{Colors.RESET}")
    else:
        print(f"{Colors.RED}Failed to extract the requested information. Try refining your prompt or choosing a different company.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/o3-mini_web_crawler/o3-mini_web_crawler.py
================
import os
from firecrawl import FirecrawlApp
import json
from dotenv import load_dotenv
from openai import OpenAI

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")

# Initialize the FirecrawlApp and OpenAI client
app = FirecrawlApp(api_key=firecrawl_api_key)
client = OpenAI(api_key=openai_api_key)

# Find the page that most likely contains the objective
def find_relevant_page_via_map(objective, url, app, client):
    try:
        print(f"{Colors.CYAN}Understood. The objective is: {objective}{Colors.RESET}")
        print(f"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}")
        
        map_prompt = f"""
        The map function generates a list of URLs from a website and it accepts a search parameter. Based on the objective of: {objective}, come up with a 1-2 word search parameter that will help us find the information we need. Only respond with 1-2 words nothing else.
        """

        print(f"{Colors.YELLOW}Analyzing objective to determine optimal search parameter...{Colors.RESET}")
        completion = client.chat.completions.create(
            model="o3-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": map_prompt
                        }
                    ]
                }
            ]
        )

        map_search_parameter = completion.choices[0].message.content
        print(f"{Colors.GREEN}Optimal search parameter identified: {map_search_parameter}{Colors.RESET}")

        print(f"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}")
        map_website = app.map_url(url, params={"search": map_search_parameter})
        
        # Debug print to see the response structure
        print(f"{Colors.MAGENTA}Debug - Map response structure: {json.dumps(map_website, indent=2)}{Colors.RESET}")
        
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")
        
        # Handle the response based on its structure
        if isinstance(map_website, dict):
            # Assuming the links are in a 'urls' or similar key
            links = map_website.get('urls', []) or map_website.get('links', [])
        elif isinstance(map_website, str):
            try:
                parsed = json.loads(map_website)
                links = parsed.get('urls', []) or parsed.get('links', [])
            except json.JSONDecodeError:
                links = []
        else:
            links = map_website if isinstance(map_website, list) else []

        if not links:
            print(f"{Colors.RED}No links found in map response.{Colors.RESET}")
            return None

        rank_prompt = f"""
        Given this list of URLs and the objective: {objective}
        Analyze each URL and rank the top 3 most relevant ones that are most likely to contain the information we need.
        Return your response as a JSON array with exactly 3 objects, each containing:
        - "url": the full URL
        - "relevance_score": number between 0-100 indicating relevance to objective
        - "reason": brief explanation of why this URL is relevant

        Example output:
        [
            {{
                "url": "https://example.com/about",
                "relevance_score": 95,
                "reason": "Main about page containing company information"
            }},
            {{
                "url": "https://example.com/team",
                "relevance_score": 80,
                "reason": "Team page with leadership details"
            }},
            {{
                "url": "https://example.com/contact",
                "relevance_score": 70,
                "reason": "Contact page with location information"
            }}
        ]

        URLs to analyze:
        {json.dumps(links, indent=2)}
        """

        print(f"{Colors.YELLOW}Ranking URLs by relevance to objective...{Colors.RESET}")
        completion = client.chat.completions.create(
            model="o3-mini",
            messages=[
                {
                    "role": "user", 
                    "content": [
                        {
                            "type": "text",
                            "text": rank_prompt
                        }
                    ]
                }
            ]
        )

        try:
            ranked_results = json.loads(completion.choices[0].message.content)
            links = [result["url"] for result in ranked_results]
            
            # Print detailed ranking info
            print(f"{Colors.CYAN}Top 3 ranked URLs:{Colors.RESET}")
            for result in ranked_results:
                print(f"{Colors.GREEN}URL: {result['url']}{Colors.RESET}")
                print(f"{Colors.YELLOW}Relevance Score: {result['relevance_score']}{Colors.RESET}")
                print(f"{Colors.BLUE}Reason: {result['reason']}{Colors.RESET}")
                print("---")

            if not links:
                print(f"{Colors.RED}No relevant links identified.{Colors.RESET}")
                return None

        except (json.JSONDecodeError, KeyError) as e:
            print(f"{Colors.RED}Error parsing ranked results: {str(e)}{Colors.RESET}")
            return None
            
        print(f"{Colors.GREEN}Located {len(links)} relevant links.{Colors.RESET}")
        return links
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}")
        return None
    
# Scrape the top 3 pages and see if the objective is met, if so return in json format else return None
def find_objective_in_top_pages(map_website, objective, app, client):
    try:
        # Get top 3 links from the map result
        if not map_website:
            print(f"{Colors.RED}No links found to analyze.{Colors.RESET}")
            return None
            
        top_links = map_website[:3]
        print(f"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}")
        
        for link in top_links:
            print(f"{Colors.YELLOW}Initiating scrape of page: {link}{Colors.RESET}")
            # Scrape the page
            scrape_result = app.scrape_url(link, params={'formats': ['markdown']})
            print(f"{Colors.GREEN}Page scraping completed successfully.{Colors.RESET}")
     
            
            # Check if objective is met
            check_prompt = f"""
            Given the following scraped content and objective, determine if the objective is met.
            If it is, extract the relevant information in a simple and concise JSON format. Use only the necessary fields and avoid nested structures if possible.
            If the objective is not met with confidence, respond with 'Objective not met'.

            Objective: {objective}
            Scraped content: {scrape_result['markdown']}

            Remember:
            1. Only return JSON if you are confident the objective is fully met.
            2. Keep the JSON structure as simple and flat as possible.
            3. Do not include any explanations or markdown formatting in your response.
            """
        
            completion = client.chat.completions.create(
                model="o3-mini",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": check_prompt
                            }
                        ]
                    }
                ]
            )
            
            result = completion.choices[0].message.content
            
            if result != "Objective not met":
                print(f"{Colors.GREEN}Objective potentially fulfilled. Relevant information identified.{Colors.RESET}")
                try:
                    return json.loads(result)
                except json.JSONDecodeError:
                    print(f"{Colors.RED}Error in parsing response. Proceeding to next page...{Colors.RESET}")
            else:
                print(f"{Colors.YELLOW}Objective not met on this page. Proceeding to next link...{Colors.RESET}")
        
        print(f"{Colors.RED}All available pages analyzed. Objective not fulfilled in examined content.{Colors.RESET}")
        return None
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during page analysis: {str(e)}{Colors.RESET}")
        return None

# Main function to execute the process
def main():
    # Get user input
    url = input(f"{Colors.BLUE}Enter the website to crawl : {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter your objective: {Colors.RESET}")
    
    print(f"{Colors.YELLOW}Initiating web crawling process...{Colors.RESET}")
    # Find the relevant page
    map_website = find_relevant_page_via_map(objective, url, app, client)
    
    if map_website:
        print(f"{Colors.GREEN}Relevant pages identified. Proceeding with detailed analysis using o3-mini...{Colors.RESET}")
        # Find objective in top pages
        result = find_objective_in_top_pages(map_website, objective, app, client)
        
        if result:
            print(f"{Colors.GREEN}Objective successfully fulfilled. Extracted information :{Colors.RESET}")
            print(f"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}")
        else:
            print(f"{Colors.RED}Unable to fulfill the objective with the available content.{Colors.RESET}")
    else:
        print(f"{Colors.RED}No relevant pages identified. Consider refining the search parameters or trying a different website.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/o3-mini-deal-finder/o3-mini-deal-finder.py
================
import os
import json
import time
import requests
from dotenv import load_dotenv
from openai import OpenAI
from serpapi.google_search import GoogleSearch

class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
serp_api_key = os.getenv("SERP_API_KEY")

def search_product(product_name):
    """Search for specific product pages."""
    amazon_search = GoogleSearch({
        "q": f"site:amazon.com/ {product_name} -renewed -refurbished",
        "num": 5,
        "api_key": serp_api_key
    })
    
    walmart_search = GoogleSearch({
        "q": f"site:walmart.com/ {product_name}",
        "num": 5,
        "api_key": serp_api_key
    })
    
    return (
        amazon_search.get_dict().get("organic_results", [])[:3],
        walmart_search.get_dict().get("organic_results", [])[:3]
    )

def get_product_urls(amazon_results, walmart_results):
    """Get the product URL from each platform and make sure it is the actual product instead of any accesories and also make sure it is from the original reseller, if not available return error.Make sure it's the same product not accesorries but the actual product."""
    amazon_url = next((r['link'] for r in amazon_results if '/dp/' in r['link']), None)
    walmart_url = next((r['link'] for r in walmart_results if '/ip/' in r['link']), None)
    
    if amazon_url or walmart_url:
        print(f"{Colors.CYAN}Found product URLs:{Colors.RESET}")
        if amazon_url:
            print(f"Amazon: {amazon_url}")
        if walmart_url:
            print(f"Walmart: {walmart_url}")
    
    return amazon_url, walmart_url

def poll_extraction(extraction_id, api_key, max_attempts=20):
    """Poll for extraction results with detailed logging."""
    url = f"https://api.firecrawl.dev/v1/extract/{extraction_id}"
    headers = {'Authorization': f'Bearer {api_key}'}
    
    for attempt in range(max_attempts):
        try:
            # print(f"{Colors.YELLOW}Polling attempt {attempt + 1}/{max_attempts}...{Colors.RESET}")
            response = requests.get(url, headers=headers)
            # print(f"Response status: {response.status_code}")
            
            print(f"Raw response: {response.text}")
            
            data = response.json()
            print(f"Parsed data: {json.dumps(data, indent=2)}")
            
            if data.get('success') and data.get('data'):
                print(f"Returning data: {json.dumps(data['data'], indent=2)}")
                return data['data']
            elif not data.get('success'):
                print(f"{Colors.RED}Polling failed: {data.get('error')}{Colors.RESET}")
            else:
                print(f"{Colors.YELLOW}Still processing...{Colors.RESET}")
            
            if attempt < max_attempts - 1:
                print(f"Waiting 10 seconds before retry...")
                time.sleep(10)
                
        except Exception as e:
            print(f"{Colors.RED}Polling error: {str(e)}{Colors.RESET}")
            if attempt < max_attempts - 1:
                print(f"Waiting 10 seconds before retry...")
                time.sleep(10)
            continue
            
    print(f"{Colors.RED}Extraction timed out after {max_attempts} attempts{Colors.RESET}")
    return None

def extract_product_info_and_reviews(urls):
    """Extract product information and reviews and the best URL from amazon or walmart according to the deal."""
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {firecrawl_api_key}'
    }
    
    payload = {
        "urls": urls,
        "prompt": """
            Extract the following information for each product URL:
            1. Current price (as a number only)
            2. Stock status (in stock or out of stock)
            3. Shipping information
            4. Overall product rating
            5. Top 15 most helpful customer reviews
            6.Which URL has the cheapest price and the best deal between amazon and walmart
            
            Format the response as JSON with these exact keys:
            {
                "price": number,
                "stock_status": string,
                "shipping": string,
                "rating": number,
                "reviews": array of strings,
                "best_deal_url": string
            }
        """,
        "enableWebSearch": False
    }
    
    try:
        print(f"{Colors.YELLOW}Starting extraction...{Colors.RESET}")
        response = requests.post(
            "https://api.firecrawl.dev/v1/extract",
            headers=headers,
            json=payload,
            timeout=45
        )
        
        print(f"Response status: {response.status_code}")
        data = response.json()
        
        if not data.get('success'):
            print(f"{Colors.RED}Extraction failed: {data.get('error')}{Colors.RESET}")
            return None
            
        extraction_id = data.get('id')
        if not extraction_id:
            print(f"{Colors.RED}No extraction ID received{Colors.RESET}")
            return None
            
        print(f"{Colors.YELLOW}Waiting for results...{Colors.RESET}")
        return poll_extraction(extraction_id, firecrawl_api_key)

    except Exception as e:
        print(f"{Colors.RED}Extraction error: {str(e)}{Colors.RESET}")
        return None

def display_comparison(data):
    """Display price and review comparison with improved formatting and the best deal providing platform name between amazon and walmart."""
    if not data:
        print(f"{Colors.RED}No data to display{Colors.RESET}")
        return
        
    print(f"\n{Colors.CYAN}=== Product Comparison ==={Colors.RESET}")
    
    # Handle the data structure returned by Firecrawl
    if isinstance(data, dict):
        product_info = data
        
        print(f"\n{Colors.GREEN}Product Information:{Colors.RESET}")
        print(f"Price: ${product_info.get('price', 'N/A')}")
        print(f"Stock: {product_info.get('stock_status', 'N/A')}")
        print(f"Shipping: {product_info.get('shipping', 'N/A')}")
        print(f"Rating: {product_info.get('rating', 'N/A')}/5")
        print(f"Best URL to Buy From: {product_info.get('best_deal_url', 'N/A')}")
        
        reviews = product_info.get('reviews', [])
        if reviews:
            print(f"\n{Colors.YELLOW}Review Analysis:{Colors.RESET}")
            
            # Analyze reviews using OpenAI
            prompt = f"""Analyze these product reviews and provide:
            1. Top 3 most frequently mentioned PROS
            2. Top 3 most frequently mentioned CONS
            3. Overall sentiment (positive/negative/mixed)
            4. Provide the best URL to choose for buying the particular product
            
            Reviews: {' | '.join(reviews)}
            
            Response format:
            {{
                "pros": ["pro1", "pro2", "pro3"],
                "cons": ["con1", "con2", "con3"],
                "sentiment": "overall sentiment",
                "best url to buy from": "best url between amazon_url and walmart_url"
            }}
            """
            
            try:
                response = client.chat.completions.create(
                    model="o3-mini",
                    messages=[{"role": "user", "content": prompt }],
                )
                
                analysis = json.loads(response.choices[0].message.content)
                
                print("\nüìà PROS:")
                for pro in analysis['pros']:
                    print(f"‚úì {pro}")
                    
                print("\nüìâ CONS:")
                for con in analysis['cons']:
                    print(f"‚úó {con}")
                    
                print(f"\nüéØ Overall Sentiment: {analysis['sentiment']}")
                
            except Exception as e:
                print(f"{Colors.RED}Error analyzing reviews: {str(e)}{Colors.RESET}")
                
            print(f"\n{Colors.YELLOW}Sample Reviews:{Colors.RESET}")
            for i, review in enumerate(reviews[:5], 1):
                print(f"{i}. {review}")
            
            print(f"Best URL to Buy From: {product_info.get('best_deal_url', 'N/A')}")


def main():
    product_name = input(f"{Colors.CYAN}Enter product name to compare: {Colors.RESET}")
    
    # Search and get URLs
    amazon_results, walmart_results = search_product(product_name)
    amazon_url, walmart_url = get_product_urls(amazon_results, walmart_results)
    
    if not (amazon_url or walmart_url):
        print(f"{Colors.RED}No valid product URLs found.{Colors.RESET}")
        return
    
    # Extract info and reviews
    urls = [url for url in [amazon_url, walmart_url] if url]
    product_data = extract_product_info_and_reviews(urls)
    
    # Display results with improved formatting
    display_comparison(product_data)

if __name__ == "__main__":
    main()

================
File: examples/o3-web-crawler/.env.example
================
# API Keys
FIRECRAWL_API_KEY=your_firecrawl_api_key_here
OPENAI_API_KEY=your_openai_api_key_here

================
File: examples/o3-web-crawler/.gitignore
================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/
.pytest_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# VS Code settings
.vscode/

# Jupyter Notebook
.ipynb_checkpoints

# MacOS
.DS_Store

# Project specific
*.log

================
File: examples/o3-web-crawler/o3-web-crawler.py
================
import os
from firecrawl import FirecrawlApp
import json
from dotenv import load_dotenv
from openai import OpenAI

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")

# Initialize the FirecrawlApp and OpenAI client
app = FirecrawlApp(api_key=firecrawl_api_key)
client = OpenAI(api_key=openai_api_key)

# Find the page that most likely contains the objective
def find_relevant_page_via_map(objective, url, app, client):
    try:
        print(f"{Colors.CYAN}Understood. The objective is: {objective}{Colors.RESET}")
        print(f"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}")
        
        map_prompt = f"""
        The map function generates a list of URLs from a website and it accepts a search parameter. Based on the objective of: {objective}, come up with a 1-2 word search parameter that will help us find the information we need. Only respond with 1-2 words nothing else.
        """

        print(f"{Colors.YELLOW}Analyzing objective to determine optimal search parameter...{Colors.RESET}")
        completion = client.chat.completions.create(
            model="o3",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": map_prompt
                        }
                    ]
                }
            ]
        )

        map_search_parameter = completion.choices[0].message.content
        print(f"{Colors.GREEN}Optimal search parameter identified: {map_search_parameter}{Colors.RESET}")

        print(f"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}")
        map_website = app.map_url(url, params={"search": map_search_parameter})
        
        print(f"{Colors.MAGENTA}Debug - Map response structure: {json.dumps(map_website, indent=2)}{Colors.RESET}")
        
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")
        
        # Handle the response based on its structure
        if isinstance(map_website, dict):
            links = map_website.get('urls', []) or map_website.get('links', [])
        elif isinstance(map_website, str):
            try:
                parsed = json.loads(map_website)
                links = parsed.get('urls', []) or parsed.get('links', [])
            except json.JSONDecodeError:
                links = []
        else:
            links = map_website if isinstance(map_website, list) else []

        if not links:
            print(f"{Colors.RED}No links found in map response.{Colors.RESET}")
            return None

        rank_prompt = f"""
        Given this list of URLs and the objective: {objective}
        Analyze each URL and rank the top 3 most relevant ones that are most likely to contain the information we need.
        Return your response as a JSON array with exactly 3 objects, each containing:
        - "url": the full URL
        - "relevance_score": number between 0-100 indicating relevance to objective
        - "reason": brief explanation of why this URL is relevant

        Example output:
        [
            {{
                "url": "https://example.com/about",
                "relevance_score": 95,
                "reason": "Main about page containing company information"
            }},
            {{
                "url": "https://example.com/team",
                "relevance_score": 80,
                "reason": "Team page with leadership details"
            }},
            {{
                "url": "https://example.com/contact",
                "relevance_score": 70,
                "reason": "Contact page with location information"
            }}
        ]

        URLs to analyze:
        {json.dumps(links, indent=2)}
        """

        print(f"{Colors.YELLOW}Ranking URLs by relevance to objective...{Colors.RESET}")
        completion = client.chat.completions.create(
            model="o3",
            messages=[
                {
                    "role": "user", 
                    "content": [
                        {
                            "type": "text",
                            "text": rank_prompt
                        }
                    ]
                }
            ]
        )

        try:
            ranked_results = json.loads(completion.choices[0].message.content)
            links = [result["url"] for result in ranked_results]
            
            # Print detailed ranking info
            print(f"{Colors.CYAN}Top 3 ranked URLs:{Colors.RESET}")
            for result in ranked_results:
                print(f"{Colors.GREEN}URL: {result['url']}{Colors.RESET}")
                print(f"{Colors.YELLOW}Relevance Score: {result['relevance_score']}{Colors.RESET}")
                print(f"{Colors.BLUE}Reason: {result['reason']}{Colors.RESET}")
                print("---")

            if not links:
                print(f"{Colors.RED}No relevant links identified.{Colors.RESET}")
                return None

        except (json.JSONDecodeError, KeyError) as e:
            print(f"{Colors.RED}Error parsing ranked results: {str(e)}{Colors.RESET}")
            return None
            
        print(f"{Colors.GREEN}Located {len(links)} relevant links.{Colors.RESET}")
        return links
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}")
        return None

def find_objective_in_top_pages(map_website, objective, app, client):
    try:
        # Get top 3 links from the map result
        if not map_website:
            print(f"{Colors.RED}No links found to analyze.{Colors.RESET}")
            return None
            
        top_links = map_website[:3]
        print(f"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}")
        
        for link in top_links:
            print(f"{Colors.YELLOW}Initiating scrape of page: {link}{Colors.RESET}")
            # Scrape the page
            scrape_result = app.scrape_url(link, params={'formats': ['markdown']})
            print(f"{Colors.GREEN}Page scraping completed successfully.{Colors.RESET}")
     
            
            check_prompt = f"""
            Given the following scraped content and objective, determine if the objective is met.
            If it is, extract the relevant information in a simple and concise JSON format. Use only the necessary fields and avoid nested structures if possible.
            If the objective is not met with confidence, respond with 'Objective not met'.

            Objective: {objective}
            Scraped content: {scrape_result['markdown']}

            Remember:
            1. Only return JSON if you are confident the objective is fully met.
            2. Keep the JSON structure as simple and flat as possible.
            3. Do not include any explanations or markdown formatting in your response.
            """
        
            completion = client.chat.completions.create(
                model="o3",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": check_prompt
                            }
                        ]
                    }
                ]
            )
            
            result = completion.choices[0].message.content
            
            if result != "Objective not met":
                print(f"{Colors.GREEN}Objective potentially fulfilled. Relevant information identified.{Colors.RESET}")
                try:
                    return json.loads(result)
                except json.JSONDecodeError:
                    print(f"{Colors.RED}Error in parsing response. Proceeding to next page...{Colors.RESET}")
            else:
                print(f"{Colors.YELLOW}Objective not met on this page. Proceeding to next link...{Colors.RESET}")
        
        print(f"{Colors.RED}All available pages analyzed. Objective not fulfilled in examined content.{Colors.RESET}")
        return None
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during page analysis: {str(e)}{Colors.RESET}")
        return None

# Main function to execute the process
def main():
    url = input(f"{Colors.BLUE}Enter the website to crawl : {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter your objective: {Colors.RESET}")
    
    print(f"{Colors.YELLOW}Initiating web crawling process...{Colors.RESET}")
    # Find the relevant page
    map_website = find_relevant_page_via_map(objective, url, app, client)
    
    if map_website:
        print(f"{Colors.GREEN}Relevant pages identified. Proceeding with detailed analysis using o3...{Colors.RESET}")
        # Find objective in top pages
        result = find_objective_in_top_pages(map_website, objective, app, client)
        
        if result:
            print(f"{Colors.GREEN}Objective successfully fulfilled. Extracted information :{Colors.RESET}")
            print(f"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}")
        else:
            print(f"{Colors.RED}Unable to fulfill the objective with the available content.{Colors.RESET}")
    else:
        print(f"{Colors.RED}No relevant pages identified. Consider refining the search parameters or trying a different website.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/o3-web-crawler/README.md
================
# O3 Web Crawler

A Python tool that uses OpenAI's o3 model and Firecrawl to intelligently crawl websites based on specific objectives.

## Features

- Maps website URLs to identify the most relevant pages for your objective
- Uses OpenAI's o3 model to analyze and rank pages by relevance
- Extracts specific information from web pages based on your objective
- Provides detailed, color-coded terminal output to track progress

## Prerequisites

- Python 3.6+
- Firecrawl API key
- OpenAI API key

## Installation

1. Clone this repository
2. Install dependencies:
   ```
   pip install -r requirements.txt
   ```
3. Create a `.env` file based on `.env.example` with your API keys

## Usage

Run the script:

```
python o3-web-crawler.py
```

You will be prompted to:

1. Enter a website URL to crawl
2. Specify your objective (what information you want to extract)

The script will:

- Analyze your objective to determine optimal search parameters
- Map the website to find relevant pages
- Rank pages by relevance to your objective
- Scrape and analyze top pages to extract the requested information
- Display results in JSON format

## Example

```
Enter the website to crawl: https://example.com
Enter your objective: Find the company's contact information and headquarters location
```

The script will intelligently crawl the website and extract the requested information.

## License

MIT

================
File: examples/o3-web-crawler/requirements.txt
================
firecrawl>=0.1.0
openai>=1.0.0
python-dotenv>=0.19.0

================
File: examples/o4-mini-web-crawler/.env.example
================
FIRECRAWL_API_KEY=your_firecrawl_api_key_here
OPENAI_API_KEY=your_openai_api_key_here

================
File: examples/o4-mini-web-crawler/.gitignore
================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# PyInstaller
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# pyenv
.python-version

# celery beat schedule file
celerybeat-schedule

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/

# IDE files
.idea/
.vscode/
*.swp
*.swo

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

================
File: examples/o4-mini-web-crawler/o4-mini-web-crawler.py
================
import os
from firecrawl import FirecrawlApp
import json
from dotenv import load_dotenv
from openai import OpenAI

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")

# Initialize the FirecrawlApp and OpenAI client
app = FirecrawlApp(api_key=firecrawl_api_key)
client = OpenAI(api_key=openai_api_key)

# Find the page that most likely contains the objective
def find_relevant_page_via_map(objective, url, app, client):
    try:
        print(f"{Colors.CYAN}Understood. The objective is: {objective}{Colors.RESET}")
        print(f"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}")
        
        map_prompt = f"""
        The map function generates a list of URLs from a website and it accepts a search parameter. Based on the objective of: {objective}, come up with a 1-2 word search parameter that will help us find the information we need. Only respond with 1-2 words nothing else.
        """

        print(f"{Colors.YELLOW}Analyzing objective to determine optimal search parameter...{Colors.RESET}")
        completion = client.chat.completions.create(
            model="o4-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": map_prompt
                        }
                    ]
                }
            ]
        )

        map_search_parameter = completion.choices[0].message.content
        print(f"{Colors.GREEN}Optimal search parameter identified: {map_search_parameter}{Colors.RESET}")

        print(f"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}")
        map_website = app.map_url(url, params={"search": map_search_parameter})
        
        print(f"{Colors.MAGENTA}Debug - Map response structure: {json.dumps(map_website, indent=2)}{Colors.RESET}")
        
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")
        
        # Handle the response based on its structure
        if isinstance(map_website, dict):
            links = map_website.get('urls', []) or map_website.get('links', [])
        elif isinstance(map_website, str):
            try:
                parsed = json.loads(map_website)
                links = parsed.get('urls', []) or parsed.get('links', [])
            except json.JSONDecodeError:
                links = []
        else:
            links = map_website if isinstance(map_website, list) else []

        if not links:
            print(f"{Colors.RED}No links found in map response.{Colors.RESET}")
            return None

        rank_prompt = f"""
        Given this list of URLs and the objective: {objective}
        Analyze each URL and rank the top 3 most relevant ones that are most likely to contain the information we need.
        Return your response as a JSON array with exactly 3 objects, each containing:
        - "url": the full URL
        - "relevance_score": number between 0-100 indicating relevance to objective
        - "reason": brief explanation of why this URL is relevant

        Example output:
        [
            {{
                "url": "https://example.com/about",
                "relevance_score": 95,
                "reason": "Main about page containing company information"
            }},
            {{
                "url": "https://example.com/team",
                "relevance_score": 80,
                "reason": "Team page with leadership details"
            }},
            {{
                "url": "https://example.com/contact",
                "relevance_score": 70,
                "reason": "Contact page with location information"
            }}
        ]

        URLs to analyze:
        {json.dumps(links, indent=2)}
        """

        print(f"{Colors.YELLOW}Ranking URLs by relevance to objective...{Colors.RESET}")
        completion = client.chat.completions.create(
            model="o4-mini",
            messages=[
                {
                    "role": "user", 
                    "content": [
                        {
                            "type": "text",
                            "text": rank_prompt
                        }
                    ]
                }
            ]
        )

        try:
            ranked_results = json.loads(completion.choices[0].message.content)
            links = [result["url"] for result in ranked_results]
            
            # Print detailed ranking info
            print(f"{Colors.CYAN}Top 3 ranked URLs:{Colors.RESET}")
            for result in ranked_results:
                print(f"{Colors.GREEN}URL: {result['url']}{Colors.RESET}")
                print(f"{Colors.YELLOW}Relevance Score: {result['relevance_score']}{Colors.RESET}")
                print(f"{Colors.BLUE}Reason: {result['reason']}{Colors.RESET}")
                print("---")

            if not links:
                print(f"{Colors.RED}No relevant links identified.{Colors.RESET}")
                return None

        except (json.JSONDecodeError, KeyError) as e:
            print(f"{Colors.RED}Error parsing ranked results: {str(e)}{Colors.RESET}")
            return None
            
        print(f"{Colors.GREEN}Located {len(links)} relevant links.{Colors.RESET}")
        return links
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}")
        return None

def find_objective_in_top_pages(map_website, objective, app, client):
    try:
        # Get top 3 links from the map result
        if not map_website:
            print(f"{Colors.RED}No links found to analyze.{Colors.RESET}")
            return None
            
        top_links = map_website[:3]
        print(f"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}")
        
        for link in top_links:
            print(f"{Colors.YELLOW}Initiating scrape of page: {link}{Colors.RESET}")
            # Scrape the page
            scrape_result = app.scrape_url(link, params={'formats': ['markdown']})
            print(f"{Colors.GREEN}Page scraping completed successfully.{Colors.RESET}")
     
            
            check_prompt = f"""
            Given the following scraped content and objective, determine if the objective is met.
            If it is, extract the relevant information in a simple and concise JSON format. Use only the necessary fields and avoid nested structures if possible.
            If the objective is not met with confidence, respond with 'Objective not met'.

            Objective: {objective}
            Scraped content: {scrape_result['markdown']}

            Remember:
            1. Only return JSON if you are confident the objective is fully met.
            2. Keep the JSON structure as simple and flat as possible.
            3. Do not include any explanations or markdown formatting in your response.
            """
        
            completion = client.chat.completions.create(
                model="o4-mini",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": check_prompt
                            }
                        ]
                    }
                ]
            )
            
            result = completion.choices[0].message.content
            
            if result != "Objective not met":
                print(f"{Colors.GREEN}Objective potentially fulfilled. Relevant information identified.{Colors.RESET}")
                try:
                    return json.loads(result)
                except json.JSONDecodeError:
                    print(f"{Colors.RED}Error in parsing response. Proceeding to next page...{Colors.RESET}")
            else:
                print(f"{Colors.YELLOW}Objective not met on this page. Proceeding to next link...{Colors.RESET}")
        
        print(f"{Colors.RED}All available pages analyzed. Objective not fulfilled in examined content.{Colors.RESET}")
        return None
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during page analysis: {str(e)}{Colors.RESET}")
        return None

# Main function to execute the process
def main():
    url = input(f"{Colors.BLUE}Enter the website to crawl : {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter your objective: {Colors.RESET}")
    
    print(f"{Colors.YELLOW}Initiating web crawling process...{Colors.RESET}")
    # Find the relevant page
    map_website = find_relevant_page_via_map(objective, url, app, client)
    
    if map_website:
        print(f"{Colors.GREEN}Relevant pages identified. Proceeding with detailed analysis using o4-mini...{Colors.RESET}")
        # Find objective in top pages
        result = find_objective_in_top_pages(map_website, objective, app, client)
        
        if result:
            print(f"{Colors.GREEN}Objective successfully fulfilled. Extracted information :{Colors.RESET}")
            print(f"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}")
        else:
            print(f"{Colors.RED}Unable to fulfill the objective with the available content.{Colors.RESET}")
    else:
        print(f"{Colors.RED}No relevant pages identified. Consider refining the search parameters or trying a different website.{Colors.RESET}")

if __name__ == "__main__":
    main()

================
File: examples/o4-mini-web-crawler/README.md
================
# O4 Mini Web Crawler

A simple web crawler that uses Firecrawl and OpenAI's o4-mini model to search websites based on user objectives.

## Features

- Maps websites to find relevant URLs
- Uses AI to rank URLs by relevance to the objective
- Scrapes content and analyzes it with o4-mini
- Returns structured data when objectives are met

## Prerequisites

- Python 3.6+
- Firecrawl API key
- OpenAI API key

## Installation

1. Clone this repository
2. Install the required packages:
   ```
   pip install -r requirements.txt
   ```
3. Copy `.env.example` to `.env` and fill in your API keys:
   ```
   cp .env.example .env
   ```

## Usage

Run the script:

```
python o4-mini-web-crawler.py
```

You will be prompted to:

1. Enter a website URL to crawl
2. Define your objective (what information you're looking for)

The crawler will then:

- Map the website to find relevant URLs
- Rank the most relevant pages
- Scrape and analyze the content
- Return structured data if the objective is met

## Example

```
Enter the website to crawl: https://example.com
Enter your objective: Find the company's headquarters address
```

The crawler will search for pages likely to contain this information, analyze them, and return the address in a structured format.

## License

[MIT](LICENSE)

================
File: examples/o4-mini-web-crawler/requirements.txt
================
firecrawl==1.0.0
openai==1.16.0
python-dotenv==1.0.0

================
File: examples/openai_swarm_firecrawl/.env.example
================
OPENAI_API_KEY=
FIRECRAWL_API_KEY=

================
File: examples/openai_swarm_firecrawl/main.py
================
import os
from firecrawl import FirecrawlApp
from swarm import Agent
from swarm.repl import run_demo_loop
import dotenv
from openai import OpenAI

dotenv.load_dotenv()

# Initialize FirecrawlApp and OpenAI
app = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def scrape_website(url):
    """Scrape a website using Firecrawl."""
    scrape_status = app.scrape_url(
        url,
        params={'formats': ['markdown']}
    )
    return scrape_status

def generate_completion(role, task, content):
    """Generate a completion using OpenAI."""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": f"You are a {role}. {task}"},
            {"role": "user", "content": content}
        ]
    )
    return response.choices[0].message.content

def analyze_website_content(content):
    """Analyze the scraped website content using OpenAI."""
    analysis = generate_completion(
        "marketing analyst",
        "Analyze the following website content and provide key insights for marketing strategy.",
        content
    )
    return {"analysis": analysis}

def generate_copy(brief):
    """Generate marketing copy based on a brief using OpenAI."""
    copy = generate_completion(
        "copywriter",
        "Create compelling marketing copy based on the following brief.",
        brief
    )
    return {"copy": copy}

def create_campaign_idea(target_audience, goals):
    """Create a campaign idea based on target audience and goals using OpenAI."""
    campaign_idea = generate_completion(
        "marketing strategist",
        "Create an innovative campaign idea based on the target audience and goals provided.",
        f"Target Audience: {target_audience}\nGoals: {goals}"
    )
    return {"campaign_idea": campaign_idea}

def handoff_to_copywriter():
    """Hand off the campaign idea to the copywriter agent."""
    return copywriter_agent

def handoff_to_analyst():
    """Hand off the website content to the analyst agent."""
    return analyst_agent

def handoff_to_campaign_idea():
    """Hand off the target audience and goals to the campaign idea agent."""
    return campaign_idea_agent

def handoff_to_website_scraper():
    """Hand off the url to the website scraper agent."""
    return website_scraper_agent

user_interface_agent = Agent(
    name="User Interface Agent",
    instructions="You are a user interface agent that handles all interactions with the user. You need to always start with a URL that the user wants to create a marketing strategy for. Ask clarification questions if needed. Be concise.",
    functions=[handoff_to_website_scraper],
)

website_scraper_agent = Agent(
    name="Website Scraper Agent",
    instructions="You are a website scraper agent specialized in scraping website content.",
    functions=[scrape_website, handoff_to_analyst],
)

analyst_agent = Agent(
    name="Analyst Agent",
    instructions="You are an analyst agent that examines website content and provides insights for marketing strategies. Be concise.",
    functions=[analyze_website_content, handoff_to_campaign_idea],
)

campaign_idea_agent = Agent(
    name="Campaign Idea Agent",
    instructions="You are a campaign idea agent that creates innovative marketing campaign ideas based on website content and target audience. Be concise.",
    functions=[create_campaign_idea, handoff_to_copywriter],
)

copywriter_agent = Agent(
    name="Copywriter Agent",
    instructions="You are a copywriter agent specialized in creating compelling marketing copy based on website content and campaign ideas. Be concise.",
    functions=[generate_copy],
)

if __name__ == "__main__":
    # Run the demo loop with the user interface agent
    run_demo_loop(user_interface_agent, stream=True)

================
File: examples/openai_swarm_firecrawl/README.md
================
# Swarm Firecrawl Marketing Agent

A multi-agent system using [OpenAI Swarm](https://github.com/openai/swarm) for AI-powered marketing strategies using [Firecrawl](https://firecrawl.dev) for web scraping.

## Agents

1. User Interface: Manages user interactions
2. Website Scraper: Extracts clean LLM-ready content via Firecrawl API
3. Analyst: Provides marketing insights
4. Campaign Idea: Generates marketing campaign concepts
5. Copywriter: Creates compelling marketing copy

## Requirements

- [Firecrawl](https://firecrawl.dev) API key
- [OpenAI](https://platform.openai.com/api-keys) API key

## Setup

1. Install the required packages:
   ```
   pip install -r requirements.txt
   ```

2. Set up your environment variables in a `.env` file:
   ```
   OPENAI_API_KEY=your_openai_api_key
   FIRECRAWL_API_KEY=your_firecrawl_api_key
   ```

## Usage

Run the main script to start the interactive demo:

```
python main.py
```

================
File: examples/openai_swarm_firecrawl/requirements.txt
================
firecrawl-py
openai

================
File: examples/openai_swarm_firecrawl_web_extractor/.env.example
================
OPENAI_API_KEY=
FIRECRAWL_API_KEY=
SERP_API_KEY=

================
File: examples/openai_swarm_firecrawl_web_extractor/main.py
================
import os
from firecrawl import FirecrawlApp
from swarm import Agent
from swarm.repl import run_demo_loop
import dotenv
from serpapi import GoogleSearch
from openai import OpenAI

dotenv.load_dotenv()

# Initialize FirecrawlApp and OpenAI
app = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def search_google(query, objective):
    """Search Google using SerpAPI."""
    print(f"Parameters: query={query}, objective={objective}")
    search = GoogleSearch({"q": query, "api_key": os.getenv("SERP_API_KEY")})
    results = search.get_dict().get("organic_results", [])
    return {"objective": objective, "results": results}

def map_url_pages(url, objective):
    """Map a website's pages using Firecrawl."""
   
    search_query = generate_completion(
        "website search query generator",
        f"Generate a 1-2 word search query for the website: {url} based on the objective",
        "Objective: " + objective
    )
    print(f"Parameters: url={url}, objective={objective}, search_query={search_query}")
    map_status = app.map_url(url, params={'search': search_query})
    if map_status.get('status') == 'success':
        links = map_status.get('links', [])
        top_link = links[0] if links else None
        return {"objective": objective, "results": [top_link] if top_link else []}
    else:
        return {"objective": objective, "results": []}

def scrape_url(url, objective):
    """Scrape a website using Firecrawl."""
    print(f"Parameters: url={url}, objective={objective}")
    scrape_status = app.scrape_url(
        url,
        params={'formats': ['markdown']}
    )
    return {"objective": objective, "results": scrape_status}

def analyze_website_content(content, objective):
    """Analyze the scraped website content using OpenAI."""
    print(f"Parameters: content={content[:50]}..., objective={objective}")
    analysis = generate_completion(
        "website data extractor",
        f"Analyze the following website content and extract a JSON object based on the objective.",
        "Objective: " + objective + "\nContent: " + content
    )
    return {"objective": objective, "results": analysis}

def generate_completion(role, task, content):
    """Generate a completion using OpenAI."""
    print(f"Parameters: role={role}, task={task[:50]}..., content={content[:50]}...")
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": f"You are a {role}. {task}"},
            {"role": "user", "content": content}
        ]
    )
    return response.choices[0].message.content

def handoff_to_search_google():
    """Hand off the search query to the search google agent."""
    return google_search_agent

def handoff_to_map_url():
    """Hand off the url to the map url agent."""
    return map_url_agent

def handoff_to_website_scraper():
    """Hand off the url to the website scraper agent."""
    return website_scraper_agent

def handoff_to_analyst():
    """Hand off the website content to the analyst agent."""
    return analyst_agent



user_interface_agent = Agent(
    name="User Interface Agent",
    instructions="You are a user interface agent that handles all interactions with the user. You need to always start with an web data extraction objective that the user wants to achieve by searching the web, mapping the web pages, and extracting the content from a specific page. Be concise.",
    functions=[handoff_to_search_google],
)

google_search_agent = Agent(
    name="Google Search Agent",
    instructions="You are a google search agent specialized in searching the web. Only search for the website not any specific page. When you are done, you must hand off to the map agent.",
    functions=[search_google, handoff_to_map_url],
)

map_url_agent = Agent(
    name="Map URL Agent",
    instructions="You are a map url agent specialized in mapping the web pages. When you are done, you must hand off the results to the website scraper agent.",
    functions=[map_url_pages, handoff_to_website_scraper],
)

website_scraper_agent = Agent(
    name="Website Scraper Agent",
    instructions="You are a website scraper agent specialized in scraping website content. When you are done, you must hand off the website content to the analyst agent to extract the data based on the objective.",
    functions=[scrape_url, handoff_to_analyst],
)

analyst_agent = Agent(
    name="Analyst Agent",
    instructions="You are an analyst agent that examines website content and returns a JSON object. When you are done, you must return a JSON object.",
    functions=[analyze_website_content],
)

if __name__ == "__main__":
    # Run the demo loop with the user interface agent
    run_demo_loop(user_interface_agent, stream=True)

================
File: examples/openai_swarm_firecrawl_web_extractor/requirements.txt
================
firecrawl-py
openai
google-search-results
git+https://github.com/openai/swarm.git

================
File: examples/openai-realtime-firecrawl/README.md
================
# OpenAI Realtime API with Firecrawl

This project is a demo of the OpenAI Realtime API with Firecrawl integrated.

Here is a link to the Realtime console fork:

[https://github.com/nickscamara/firecrawl-openai-realtime](https://github.com/nickscamara/firecrawl-openai-realtime)

================
File: examples/R1_company_researcher/r1_company_researcher.py
================
import os
import json
import time
import requests
from dotenv import load_dotenv
from openai import OpenAI
from serpapi.google_search import GoogleSearch

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Initialize clients
client = OpenAI(api_key=os.getenv("DEEPSEEK_API_KEY"), base_url="https://api.deepseek.com")
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
serp_api_key = os.getenv("SERP_API_KEY")

def search_google(query):
    """Search Google using SerpAPI and return top results."""
    print(f"{Colors.YELLOW}Searching Google for '{query}'...{Colors.RESET}")
    search = GoogleSearch({"q": query, "api_key": serp_api_key})
    return search.get_dict().get("organic_results", [])

def select_urls_with_r1(company, objective, serp_results):
    """
    Use R1 to select the most relevant URLs from SERP results for the given company and objective.
    Returns a list of URLs.
    """
    try:
        # Prepare the data for R1
        serp_data = [{"title": r.get("title"), "link": r.get("link"), "snippet": r.get("snippet")} 
                     for r in serp_results if r.get("link")]

        response = client.chat.completions.create(
            model="deepseek-reasoner",
            messages=[
                {
                    "role": "system",
                    "content": "You are a URL selector that always responds with valid JSON. You select URLs from the SERP results relevant to the company and objective. Your response must be a JSON object with a 'selected_urls' array property containing strings."
                },
                {
                    "role": "user",
                    "content": (
                        f"Company: {company}\n"
                        f"Objective: {objective}\n"
                        f"SERP Results: {json.dumps(serp_data)}\n\n"
                        "Return a JSON object with a property 'selected_urls' that contains an array "
                        "of URLs most likely to help meet the objective. Add a /* to the end of the URL if you think it should search all of the pages in the site. Do not return any social media links. For example: {\"selected_urls\": [\"https://example.com\", \"https://example2.com\"]}"
                    )
                }
            ]
        )

        try:
            # First try to parse as JSON
            result = json.loads(response.choices[0].message.content)
            if isinstance(result, dict) and "selected_urls" in result:
                urls = result["selected_urls"]
            else:
                # If JSON doesn't have the expected structure, fall back to text parsing
                response_text = response.choices[0].message.content
                urls = [line.strip() for line in response_text.split('\n') 
                       if line.strip().startswith(('http://', 'https://'))]
        except json.JSONDecodeError:
            # If JSON parsing fails, fall back to text parsing
            response_text = response.choices[0].message.content
            urls = [line.strip() for line in response_text.split('\n') 
                   if line.strip().startswith(('http://', 'https://'))]

        # Clean up URLs - remove wildcards and trailing slashes
        cleaned_urls = [url.replace('/*', '').rstrip('/') for url in urls]
        cleaned_urls = [url for url in cleaned_urls if url]

        if not cleaned_urls:
            print(f"{Colors.YELLOW}No valid URLs found.{Colors.RESET}")
            return []

        print(f"{Colors.CYAN}Selected URLs for extraction by R1:{Colors.RESET}")
        for url in cleaned_urls:
            print(f"- {url}")

        return cleaned_urls

    except Exception as e:
        print(f"{Colors.RED}Error selecting URLs with R1: {e}{Colors.RESET}")
        return []

def extract_company_info(urls, prompt, company, api_key):
    """Use requests to call Firecrawl's extract endpoint with selected URLs."""
    print(f"{Colors.YELLOW}Extracting structured data from the provided URLs using Firecrawl...{Colors.RESET}")
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    payload = {
        "urls": urls,
        "prompt": prompt + " for " + company,
        "enableWebSearch": True
    }
    
    try:
        response = requests.post(
            "https://api.firecrawl.dev/v1/extract",
            headers=headers,
            json=payload,
            timeout=30
        )
        
        data = response.json()
        
        if not data.get('success'):
            print(f"{Colors.RED}API returned error: {data.get('error', 'No error message')}{Colors.RESET}")
            return None
        
        # Assuming Firecrawl provides a way to retrieve data with 'id'
        extraction_id = data.get('id')
        if not extraction_id:
            print(f"{Colors.RED}No extraction ID found in response.{Colors.RESET}")
            return None

        # Polling for the extraction result
        return poll_firecrawl_result(extraction_id, api_key)

    except requests.exceptions.RequestException as e:
        print(f"{Colors.RED}Request failed: {e}{Colors.RESET}")
        return None
    except json.JSONDecodeError as e:
        print(f"{Colors.RED}Failed to parse response: {e}{Colors.RESET}")
        return None
    except Exception as e:
        print(f"{Colors.RED}Failed to extract data: {e}{Colors.RESET}")
        return None

def poll_firecrawl_result(extraction_id, api_key, interval=5, max_attempts=36):
    """Poll Firecrawl API to get the extraction result."""
    url = f"https://api.firecrawl.dev/v1/extract/{extraction_id}"
    headers = {
        'Authorization': f'Bearer {api_key}'
    }

    for attempt in range(1, max_attempts + 1):
        try:
            # print(f"{Colors.YELLOW}Polling for extraction result (Attempt {attempt}/{max_attempts})...{Colors.RESET}")
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()

            if data.get('success') and data.get('data'):
                print(f"{Colors.GREEN}Data successfully extracted:{Colors.RESET}")
                print(json.dumps(data['data'], indent=2))
                return data['data']
            elif data.get('success') and not data.get('data'):
                time.sleep(interval)
            else:
                print(f"{Colors.RED}API Error: {data.get('error', 'No error message provided')}{Colors.RESET}")
                return None

        except requests.exceptions.RequestException:
            return None
        except json.JSONDecodeError:
            return None
        except Exception:
            return None

    print(f"{Colors.RED}Max polling attempts reached. Extraction did not complete in time.{Colors.RESET}")
    return None

def main():
    company = input(f"{Colors.BLUE}Enter the company name: {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter what information you want about the company: {Colors.RESET}")
    
    serp_results = search_google(f"{company}")
    if not serp_results:
        print(f"{Colors.RED}No search results found.{Colors.RESET}")
        return
    
    # Ask R1 to select URLs
    selected_urls = select_urls_with_r1(company, objective, serp_results)
    
    if not selected_urls:
        print(f"{Colors.RED}R1 did not return any URLs.{Colors.RESET}")
        return
    
    data = extract_company_info(selected_urls, objective, company, firecrawl_api_key)
    
    if data:
        print(f"{Colors.GREEN}Extraction completed successfully.{Colors.RESET}")
    else:
        print(f"{Colors.RED}Failed to extract the requested information. Try refining your prompt or choosing a different company.{Colors.RESET}")

if __name__ == "__main__":
        main()

================
File: examples/R1_web_crawler/R1_web_crawler.py
================
import os
import json
import time
import requests
from dotenv import load_dotenv
from openai import OpenAI
from serpapi.google_search import GoogleSearch

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Initialize clients
client = OpenAI(api_key=os.getenv("DEEPSEEK_API_KEY"), base_url="https://api.deepseek.com")
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
serp_api_key = os.getenv("SERP_API_KEY")

def search_google(query):
    """Search Google using SerpAPI and return top results."""
    print(f"{Colors.YELLOW}Searching Google for '{query}'...{Colors.RESET}")
    search = GoogleSearch({"q": query, "api_key": serp_api_key})
    return search.get_dict().get("organic_results", [])

def select_urls_with_r1(company, objective, serp_results):
    """
    Use R1 to select the most relevant URLs from SERP results for the given company and objective.
    Returns a list of URLs.
    """
    try:
        # Prepare the data for R1
        serp_data = [{"title": r.get("title"), "link": r.get("link"), "snippet": r.get("snippet")} 
                     for r in serp_results if r.get("link")]

        response = client.chat.completions.create(
            model="deepseek-reasoner",
            messages=[
                {
                    "role": "system",
                    "content": "You are a URL selector that always responds with valid JSON. You select URLs from the SERP results relevant to the company and objective. Your response must be a JSON object with a 'selected_urls' array property containing strings."
                },
                {
                    "role": "user",
                    "content": (
                        f"Company: {company}\n"
                        f"Objective: {objective}\n"
                        f"SERP Results: {json.dumps(serp_data)}\n\n"
                        "Return a JSON object with a property 'selected_urls' that contains an array "
                        "of URLs most likely to help meet the objective. Add a /* to the end of the URL if you think it should search all of the pages in the site. Do not return any social media links. For example: {\"selected_urls\": [\"https://example.com\", \"https://example2.com\"]}"
                    )
                }
            ]
        )

        try:
            # First try to parse as JSON
            result = json.loads(response.choices[0].message.content)
            if isinstance(result, dict) and "selected_urls" in result:
                urls = result["selected_urls"]
            else:
                # If JSON doesn't have the expected structure, fall back to text parsing
                response_text = response.choices[0].message.content
                urls = [line.strip() for line in response_text.split('\n') 
                       if line.strip().startswith(('http://', 'https://'))]
        except json.JSONDecodeError:
            # If JSON parsing fails, fall back to text parsing
            response_text = response.choices[0].message.content
            urls = [line.strip() for line in response_text.split('\n') 
                   if line.strip().startswith(('http://', 'https://'))]

        # Clean up URLs - remove wildcards and trailing slashes
        cleaned_urls = [url.replace('/*', '').rstrip('/') for url in urls]
        cleaned_urls = [url for url in cleaned_urls if url]

        if not cleaned_urls:
            print(f"{Colors.YELLOW}No valid URLs found.{Colors.RESET}")
            return []

        print(f"{Colors.CYAN}Selected URLs for extraction by R1:{Colors.RESET}")
        for url in cleaned_urls:
            print(f"- {url}")

        return cleaned_urls

    except Exception as e:
        print(f"{Colors.RED}Error selecting URLs with R1: {e}{Colors.RESET}")
        return []

def extract_company_info(urls, prompt, company, api_key):
    """Use requests to call Firecrawl's extract endpoint with selected URLs."""
    print(f"{Colors.YELLOW}Extracting structured data from the provided URLs using Firecrawl...{Colors.RESET}")
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }
    
    payload = {
        "urls": urls,
        "prompt": prompt + " for " + company,
        "enableWebSearch": True
    }
    
    try:
        response = requests.post(
            "https://api.firecrawl.dev/v1/extract",
            headers=headers,
            json=payload,
            timeout=30
        )
        
        data = response.json()
        
        if not data.get('success'):
            print(f"{Colors.RED}API returned error: {data.get('error', 'No error message')}{Colors.RESET}")
            return None
        
        # Assuming Firecrawl provides a way to retrieve data with 'id'
        extraction_id = data.get('id')
        if not extraction_id:
            print(f"{Colors.RED}No extraction ID found in response.{Colors.RESET}")
            return None

        # Polling for the extraction result
        return poll_firecrawl_result(extraction_id, api_key)

    except requests.exceptions.RequestException as e:
        print(f"{Colors.RED}Request failed: {e}{Colors.RESET}")
        return None
    except json.JSONDecodeError as e:
        print(f"{Colors.RED}Failed to parse response: {e}{Colors.RESET}")
        return None
    except Exception as e:
        print(f"{Colors.RED}Failed to extract data: {e}{Colors.RESET}")
        return None

def poll_firecrawl_result(extraction_id, api_key, interval=5, max_attempts=12):
    """Poll Firecrawl API to get the extraction result."""
    url = f"https://api.firecrawl.dev/v1/extract/{extraction_id}"
    headers = {
        'Authorization': f'Bearer {api_key}'
    }

    for attempt in range(1, max_attempts + 1):
        try:
            # print(f"{Colors.YELLOW}Polling for extraction result (Attempt {attempt}/{max_attempts})...{Colors.RESET}")
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()

            if data.get('success') and data.get('data'):
                print(f"{Colors.GREEN}Data successfully extracted:{Colors.RESET}")
                print(json.dumps(data['data'], indent=2))
                return data['data']
            elif data.get('success') and not data.get('data'):
                time.sleep(interval)
            else:
                print(f"{Colors.RED}API Error: {data.get('error', 'No error message provided')}{Colors.RESET}")
                return None

        except requests.exceptions.RequestException:
            return None
        except json.JSONDecodeError:
            return None
        except Exception:
            return None

    print(f"{Colors.RED}Max polling attempts reached. Extraction did not complete in time.{Colors.RESET}")
    return None

def main():
    company = input(f"{Colors.BLUE}Enter the company name: {Colors.RESET}")
    objective = input(f"{Colors.BLUE}Enter what information you want about the company: {Colors.RESET}")
    
    serp_results = search_google(f"{company}")
    if not serp_results:
        print(f"{Colors.RED}No search results found.{Colors.RESET}")
        return
    
    # Ask R1 to select URLs
    selected_urls = select_urls_with_r1(company, objective, serp_results)
    
    if not selected_urls:
        print(f"{Colors.RED}R1 did not return any URLs.{Colors.RESET}")
        return
    
    data = extract_company_info(selected_urls, objective, company, firecrawl_api_key)
    
    if data:
        print(f"{Colors.GREEN}Extraction completed successfully.{Colors.RESET}")
    else:
        print(f"{Colors.RED}Failed to extract the requested information. Try refining your prompt or choosing a different company.{Colors.RESET}")

if __name__ == "__main__":
        main()

================
File: examples/sales_web_crawler/.env.example
================
OPENAI_API_KEY=
FIRECRAWL_API_KEY=
SERP_API_KEY=

================
File: examples/sales_web_crawler/app.py
================
import csv
import json
import os

from dotenv import load_dotenv
from firecrawl import FirecrawlApp
from openai import OpenAI
from serpapi import GoogleSearch
from swarm import Agent
from swarm.repl import run_demo_loop

load_dotenv()

# Initialize FirecrawlApp and OpenAI
app = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def crawl_and_analyze_url(url, objective):
    """Crawl a website using Firecrawl and analyze the content."""
    print(f"Parameters: url={url}, objective={objective}")
    # Crawl the website
    crawl_status = app.crawl_url(
        url,
        params={'limit': 10, 'scrapeOptions': {'formats': ['markdown']}},
        poll_interval=5
    )
    crawl_status = crawl_status['data']
    # Process each 'markdown' element individually
    combined_results = []
    for item in crawl_status:
        if 'markdown' in item:
            content = item['markdown']
            # Analyze the content
            analysis = generate_completion(
                "website data extractor",
                f"Analyze the following website content and extract a JSON object based on the objective. Do not write the ```json and ``` to denote a JSON when returning a response",
                "Objective: " + objective + "\nContent: " + content
            )
            # Parse the JSON result
            try:
                result = json.loads(analysis)
                combined_results.append(result)
            except json.JSONDecodeError:
                print(f"Could not parse JSON from analysis: {analysis}")
    # Combine the results
    return {"objective": objective, "results": combined_results}

def generate_completion(role, task, content):
    """Generate a completion using OpenAI."""
    print(f"Parameters: role={role}, task={task[:50]}..., content={content[:50]}...")
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": f"You are a {role}. {task}"},
            {"role": "user", "content": content}
        ]
    )
    return response.choices[0].message.content

def handoff_to_crawl_url():
    """Hand off the url to the crawl url agent."""
    return crawl_website_agent

user_interface_agent = Agent(
    name="User Interface Agent",
    instructions="You are a user interface agent that handles all interactions with the user. You need to always start by asking for a URL to crawl and the web data extraction objective. Be concise.",
    functions=[handoff_to_crawl_url],
)

crawl_website_agent = Agent(
    name="Crawl Website Agent",
    instructions="You are a crawl URL agent specialized in crawling web pages and analyzing their content. When you are done, you must print the results to the console.",
    functions=[crawl_and_analyze_url],
)

if __name__ == "__main__":
    # Run the demo loop with the user interface agent
    run_demo_loop(user_interface_agent, stream=True)

================
File: examples/sales_web_crawler/requirements.txt
================
firecrawl-py
openai
google-search-results
git+https://github.com/openai/swarm.git

================
File: examples/scrape_and_analyze_airbnb_data_e2b/.env.template
================
# TODO: Get your E2B API key from https://e2b.dev/docs
E2B_API_KEY=""

# TODO: Get your Firecrawl API key from https://firecrawl.dev
FIRECRAWL_API_KEY=""

# TODO: Get your Anthropic API key from https://anthropic.com
ANTHROPIC_API_KEY=""

================
File: examples/scrape_and_analyze_airbnb_data_e2b/.prettierignore
================
# Ignore artifacts:
node_modules

================
File: examples/scrape_and_analyze_airbnb_data_e2b/airbnb_listings.json
================
[
  {
    "title": "2br Victorian House with Breathtaking views",
    "price_per_night": 356,
    "location": "Potrero Hill",
    "rating": 4.98,
    "reviews": 184
  },
  {
    "title": "543c - convenient cozy private bedroom for 1 person",
    "price_per_night": 52,
    "location": "Inner Richmond",
    "rating": 4.72,
    "reviews": 68
  },
  {
    "title": "Clean Bright Airy Private Apt in the Heart of SF",
    "price_per_night": 269,
    "location": "Marina District",
    "rating": 4.95,
    "reviews": 239
  },
  {
    "title": "Garden Suite by Golden Gate Park, Private Bathrm",
    "price_per_night": 79,
    "location": "Outer Richmond",
    "rating": 4.82,
    "reviews": 1113
  },
  {
    "title": "#2 private bathroom next to The Ritz- Carlton",
    "price_per_night": 98,
    "location": "Union Square",
    "rating": 4.96,
    "reviews": 494
  },
  {
    "title": "Central, cozy one-bedroom condo in San Francisco",
    "price_per_night": 262,
    "location": "San Francisco",
    "rating": 4.98,
    "reviews": 46
  },
  {
    "title": "Large Light Filled Quiet Artist built 2BR Apt",
    "price_per_night": 273,
    "location": "Mission District",
    "rating": 4.99,
    "reviews": 132
  },
  {
    "title": "Oceanside Getaway",
    "price_per_night": 160,
    "location": "Outer Sunset",
    "rating": 4.93,
    "reviews": 559
  },
  {
    "title": "Perfect getaway near Japantown",
    "price_per_night": 159,
    "location": "Japantown",
    "rating": 4.92,
    "reviews": 515
  },
  {
    "title": "Style & Comfort-Private Suite near UCSF and GGPark",
    "price_per_night": 155,
    "location": "Inner Sunset",
    "rating": 4.98,
    "reviews": 439
  },
  {
    "title": "Central quiet Victorian Flat",
    "price_per_night": 224,
    "location": "San Francisco",
    "rating": 5,
    "reviews": 15
  },
  {
    "title": "Palm Trees private room near Ocean Beach Zoo GGPK",
    "price_per_night": 76,
    "location": "San Francisco",
    "rating": 4.95,
    "reviews": 200
  },
  {
    "title": "Spacious 1BR in the Mission w/ huge living room",
    "price_per_night": 195,
    "location": "San Francisco",
    "rating": 5,
    "reviews": 7
  },
  {
    "title": "Modern Hilltop Studio - Private Entry and Garden",
    "price_per_night": 230,
    "location": "San Francisco",
    "rating": 4.94,
    "reviews": 196
  },
  {
    "title": "Bright Modern Private Soma Studio Street Entrance",
    "price_per_night": 125,
    "location": "San Francisco",
    "rating": 4.9,
    "reviews": 214
  },
  {
    "title": "Castro private room & bath VIEW (no cleaning fee)",
    "price_per_night": 112,
    "location": "San Francisco",
    "rating": 4.94,
    "reviews": 440
  },
  {
    "title": "Nob Hill Studio",
    "price_per_night": 148,
    "location": "San Francisco",
    "rating": 5,
    "reviews": 42
  },
  {
    "title": "Spacious and Sunny Noe Valley Gem!",
    "price_per_night": 115,
    "location": "San Francisco",
    "rating": 5,
    "reviews": 68
  },
  {
    "title": "SF Ocean Beach In-law Suite",
    "price_per_night": 162,
    "location": "San Francisco",
    "rating": 4.91,
    "reviews": 646
  },
  {
    "title": "Comfortable, cozy, private studio - Bernal Heights",
    "price_per_night": 145,
    "location": "San Francisco",
    "rating": 4.9,
    "reviews": 866
  },
  {
    "title": "Casa Pinudo Queen Bed Room & City-View Roofdeck",
    "price_per_night": 100,
    "location": "San Francisco",
    "rating": 4.87,
    "reviews": 47
  },
  {
    "title": "Bright bedroom in Victorian home",
    "price_per_night": 114,
    "location": "San Francisco",
    "rating": 4.95,
    "reviews": 183
  },
  {
    "title": "#1 SF 24th ave& kirkham st Master king room",
    "price_per_night": 104,
    "location": "San Francisco",
    "rating": 4.95,
    "reviews": 59
  },
  {
    "title": "Cheerful 1 bedroom",
    "price_per_night": 137,
    "location": "San Francisco",
    "rating": 4.79,
    "reviews": 111
  },
  {
    "title": "Luxury Studio Near SFO, SFSU ,BART, Walk to shops!",
    "price_per_night": 116,
    "location": "San Francisco",
    "rating": 4.96,
    "reviews": 139
  },
  {
    "title": "#4 SF Sunset 24th ave&kirkham st Deluxe king room",
    "price_per_night": 104,
    "location": "San Francisco",
    "rating": 4.96,
    "reviews": 74
  },
  {
    "title": "Modern room & loft, private entrance",
    "price_per_night": 78,
    "location": "San Bruno",
    "rating": 4.88,
    "reviews": 868
  },
  {
    "title": "1 Queen bedded room w/full bath",
    "price_per_night": 117,
    "location": "San Francisco",
    "rating": 4.93,
    "reviews": 120
  },
  {
    "title": "Charming Noe Valley Garden Oasis",
    "price_per_night": 249,
    "location": "San Francisco",
    "rating": 4.89,
    "reviews": 199
  },
  {
    "title": "Beautiful Garden Studio in heart of Nopa",
    "price_per_night": 343,
    "location": "San Francisco",
    "rating": 4.76,
    "reviews": 259
  },
  {
    "title": "#4 SF Sunset 24th ave&kirkham st Deluxe king room",
    "price_per_night": 175,
    "location": "San Francisco",
    "rating": 4.96,
    "reviews": 74
  },
  {
    "title": "Noteworthy Large Private Bedroom - Best Location",
    "price_per_night": 159,
    "location": "San Francisco",
    "rating": 4.86,
    "reviews": 63
  },
  {
    "title": "Primary Suite Golden Gate Bridge view Private Deck",
    "price_per_night": 317,
    "location": "San Francisco",
    "rating": 4.93,
    "reviews": 445
  },
  {
    "title": "Private Room: Escape to the Mission",
    "price_per_night": 186,
    "location": "San Francisco",
    "rating": 4.72,
    "reviews": 501
  },
  {
    "title": "#1 SF 24th ave& kirkham st Master king room",
    "price_per_night": 176,
    "location": "San Francisco",
    "rating": 4.95,
    "reviews": 59
  },
  {
    "title": "Private Suite",
    "price_per_night": 154,
    "location": "San Francisco",
    "rating": 4.91,
    "reviews": 77
  },
  {
    "title": "501 Post road, San Francisco 94102",
    "price_per_night": 267,
    "location": "San Francisco"
  },
  {
    "title": "Most Desired Vacation Spot in San Francisco.",
    "price_per_night": 650,
    "location": "San Francisco",
    "rating": 5,
    "reviews": 78
  },
  {
    "title": "The House Protects The Dreamer - 2BR/1BA Victorian",
    "price_per_night": 376,
    "location": "San Francisco",
    "rating": 4.92,
    "reviews": 179
  },
  {
    "title": "Private Junior Room in Artist's Flat",
    "price_per_night": 130,
    "location": "San Francisco",
    "rating": 4.7,
    "reviews": 165
  },
  {
    "title": "Serenity by the Park , Your Golden Gate Getaway",
    "price_per_night": 238,
    "location": "San Francisco",
    "rating": 4.86,
    "reviews": 21
  },
  {
    "title": "Golden Getaway ‚Ä¢ Spacious Private Room 15m to SFO",
    "price_per_night": 113,
    "location": "San Francisco",
    "rating": 4.91,
    "reviews": 169
  },
  {
    "title": "Modern studio next to beach, G.G Park & transport",
    "price_per_night": 300,
    "location": "San Francisco",
    "rating": 4.87,
    "reviews": 318
  },
  {
    "title": "Affordable Cozy Private Room near San Francisco",
    "price_per_night": 88,
    "location": "San Francisco"
  },
  {
    "title": "Affordable Room w/ Great View in San Francisco",
    "price_per_night": 134,
    "location": "San Francisco",
    "rating": 4.79,
    "reviews": 121
  },
  {
    "title": "Nob Hill Studio",
    "price_per_night": 250,
    "location": "San Francisco",
    "rating": 5,
    "reviews": 42
  },
  {
    "title": "Cozy Sunset suite",
    "price_per_night": 120,
    "location": "San Francisco",
    "rating": 4.66,
    "reviews": 98
  },
  {
    "title": "Stay with Dongmei",
    "price_per_night": 48,
    "location": "San Francisco",
    "rating": 4.76,
    "reviews": 343
  },
  {
    "title": "Mediterranean style private studio",
    "price_per_night": 139,
    "location": "San Francisco",
    "rating": 4.93,
    "reviews": 489
  },
  {
    "title": "Cozy Garden Unit with All Amenities",
    "price_per_night": 122,
    "location": "San Francisco",
    "rating": 4.95,
    "reviews": 446
  },
  {
    "title": "Private Sunset Getaway",
    "price_per_night": 88,
    "location": "San Francisco",
    "rating": 4.85,
    "reviews": 26
  },
  {
    "title": "Pacifica studio- ocean view from deck",
    "price_per_night": 91,
    "location": "Pacifica",
    "rating": 4.76,
    "reviews": 352
  },
  {
    "title": "Sweet Suite w/ EV charger & Parking, 5 min to SFSU",
    "price_per_night": 167,
    "location": "San Francisco",
    "rating": 4.96,
    "reviews": 141
  },
  {
    "title": "Light-Filled Hillside Studio Apartment",
    "price_per_night": 128,
    "location": "San Francisco",
    "rating": 4.79,
    "reviews": 148
  },
  {
    "title": "Stay with Beno√Æte",
    "price_per_night": 72,
    "location": "San Francisco",
    "rating": 4.87,
    "reviews": 131
  },
  {
    "title": "Serene King Suite with Jacuzzi & Fireplace",
    "price_per_night": 225,
    "location": "San Francisco",
    "rating": 4.89,
    "reviews": 18
  },
  {
    "title": "Stay with Ryan",
    "price_per_night": 225,
    "location": "San Francisco",
    "rating": 4.89,
    "reviews": 18
  },
  {
    "title": "Perfectly located Castro",
    "price_per_night": 99,
    "location": "San Francisco",
    "rating": 4.93,
    "reviews": 488
  },
  {
    "title": "Sweet garden suite with free parking",
    "price_per_night": 169,
    "location": "San Francisco",
    "rating": 4.99,
    "reviews": 226
  },
  {
    "title": "Bright+Modern Brand New Guest House-Great Location",
    "price_per_night": 118,
    "location": "South San Francisco",
    "rating": 5,
    "reviews": 37
  },
  {
    "title": "Garden studio - Presidio, Baker Beach",
    "price_per_night": 194,
    "location": "San Francisco",
    "rating": 4.91,
    "reviews": 68
  },
  {
    "title": "Single Small private 1br 1ba no clean fee!",
    "price_per_night": 106,
    "location": "San Francisco",
    "rating": 4.78,
    "reviews": 310
  },
  {
    "title": "Private Cozy room 3",
    "price_per_night": 53,
    "location": "San Francisco",
    "rating": 4.86,
    "reviews": 332
  },
  {
    "title": "Central Mission Potrero 1BED-1BATH",
    "price_per_night": 164,
    "location": "San Francisco",
    "rating": 4.81,
    "reviews": 324
  },
  {
    "title": "Cozy Remodeled Suite in Oceanview With Parking",
    "price_per_night": 151,
    "location": "San Francisco",
    "rating": 4.95,
    "reviews": 310
  }
]

================
File: examples/scrape_and_analyze_airbnb_data_e2b/codeInterpreter.ts
================
import { CodeInterpreter } from '@e2b/code-interpreter'

export async function codeInterpret(
  codeInterpreter: CodeInterpreter,
  code: string
) {
  console.log(
    `\n${'='.repeat(50)}\n> Running following AI-generated code:\n${code}\n${'='.repeat(50)}`
  )

  const exec = await codeInterpreter.notebook.execCell(code, {
    // You can stream logs from the code interpreter
    // onStderr: (stderr: string) => console.log("\n[Code Interpreter stdout]", stderr),
    // onStdout: (stdout: string) => console.log("\n[Code Interpreter stderr]", stdout),
    //
    // You can also stream additional results like charts, images, etc.
    // onResult: ...
  })

  if (exec.error) {
    console.log('[Code Interpreter error]', exec.error) // Runtime error
    return undefined
  }

  return exec
}

================
File: examples/scrape_and_analyze_airbnb_data_e2b/index.ts
================
// @ts-ignore
import * as fs from 'fs'

import 'dotenv/config'
import { CodeInterpreter, Execution } from '@e2b/code-interpreter'
import Anthropic from '@anthropic-ai/sdk'
import { Buffer } from 'buffer'

import { MODEL_NAME, SYSTEM_PROMPT, tools } from './model'

import { codeInterpret } from './codeInterpreter'
import { scrapeAirbnb } from './scraping'

const anthropic = new Anthropic()

async function chat(
  codeInterpreter: CodeInterpreter,
  userMessage: string
): Promise<Execution | undefined> {
  console.log('Waiting for Claude...')

  const msg = await anthropic.beta.tools.messages.create({
    model: MODEL_NAME,
    system: SYSTEM_PROMPT,
    max_tokens: 4096,
    messages: [{ role: 'user', content: userMessage }],
    tools,
  })

  console.log(
    `\n${'='.repeat(50)}\nModel response: ${msg.content}\n${'='.repeat(50)}`
  )
  console.log(msg)

  if (msg.stop_reason === 'tool_use') {
    const toolBlock = msg.content.find((block) => block.type === 'tool_use')
    // @ts-ignore
    const toolName = toolBlock?.name ?? ''
    // @ts-ignore
    const toolInput = toolBlock?.input ?? ''

    console.log(
      `\n${'='.repeat(50)}\nUsing tool: ${toolName}\n${'='.repeat(50)}`
    )

    if (toolName === 'execute_python') {
      const code = toolInput.code
      return codeInterpret(codeInterpreter, code)
    }
    return undefined
  }
}

async function run() {
  // Load the Airbnb prices data from the JSON file
  let data
  const readDataFromFile = () => {
    try {
      return fs.readFileSync('airbnb_listings.json', 'utf8')
    } catch (err) {
      if (err.code === 'ENOENT') {
        console.log('File not found, scraping data...')
        return null
      } else {
        throw err
      }
    }
  }

  const fetchData = async () => {
    data = readDataFromFile()
    if (!data || data.trim() === '[]') {
      console.log('File is empty or contains an empty list, scraping data...')
      data = await scrapeAirbnb()
    }
  }

  await fetchData()

  // Parse the JSON data
  const prices = JSON.parse(data)

  // Convert prices array to a string representation of a Python list
  const pricesList = JSON.stringify(prices)

  const userMessage = `
  Load the Airbnb prices data from the airbnb listing below and visualize the distribution of prices with a histogram. Listing data: ${pricesList}
`

  const codeInterpreter = await CodeInterpreter.create()
  const codeOutput = await chat(codeInterpreter, userMessage)
  if (!codeOutput) {
    console.log('No code output')
    return
  }

  const logs = codeOutput.logs
  console.log(logs)

  if (codeOutput.results.length == 0) {
    console.log('No results')
    return
  }

  const firstResult = codeOutput.results[0]
  console.log(firstResult.text)

  if (firstResult.png) {
    const pngData = Buffer.from(firstResult.png, 'base64')
    const filename = 'airbnb_prices_chart.png'
    fs.writeFileSync(filename, pngData)
    console.log(`‚úÖ Saved chart to ${filename}`)
  }

  await codeInterpreter.close()
}

run()

================
File: examples/scrape_and_analyze_airbnb_data_e2b/model.ts
================
import { Tool } from '@anthropic-ai/sdk/src/resources/beta/tools'

export const MODEL_NAME = 'claude-3-opus-20240229'

export const SYSTEM_PROMPT = `
## your job & context
you are a python data scientist. you are given tasks to complete and you run python code to solve them.
- the python code runs in jupyter notebook.
- every time you call \`execute_python\` tool, the python code is executed in a separate cell. it's okay to multiple calls to \`execute_python\`.
- display visualizations using matplotlib or any other visualization library directly in the notebook. don't worry about saving the visualizations to a file.
- you have access to the internet and can make api requests.
- you also have access to the filesystem and can read/write files.
- you can install any pip package (if it exists) if you need to but the usual packages for data analysis are already preinstalled.
- you can run any python code you want, everything is running in a secure sandbox environment.
`

export const tools: Tool[] = [
  {
    name: 'execute_python',
    description:
      'Execute python code in a Jupyter notebook cell and returns any result, stdout, stderr, display_data, and error.',
    input_schema: {
      type: 'object',
      properties: {
        code: {
          type: 'string',
          description: 'The python code to execute in a single cell.',
        },
      },
      required: ['code'],
    },
  },
]

================
File: examples/scrape_and_analyze_airbnb_data_e2b/package.json
================
{
  "name": "hello-world",
  "version": "1.0.0",
  "description": "",
  "main": "index.js",
  "scripts": {
    "start": "tsx index.ts",
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "devDependencies": {
    "@types/node": "^20.12.12",
    "prettier": "3.2.5",
    "tsx": "^4.7.3",
    "typescript": "^5.4.5"
  },
  "dependencies": {
    "@anthropic-ai/sdk": "^0.20.7",
    "@e2b/code-interpreter": "^0.0.2",
    "@mendable/firecrawl-js": "^0.0.21",
    "buffer": "^6.0.3",
    "dotenv": "^16.4.5"
  }
}

================
File: examples/scrape_and_analyze_airbnb_data_e2b/prettier.config.mjs
================
// prettier.config.js, .prettierrc.js, prettier.config.mjs, or .prettierrc.mjs

/** @type {import("prettier").Config} */
const config = {
  trailingComma: 'es5',
  tabWidth: 2,
  semi: false,
  singleQuote: true,
}

export default config

================
File: examples/scrape_and_analyze_airbnb_data_e2b/README.md
================
# Scrape and Analyze Airbnb Data with Firecrawl and E2B

This example demonstrates how to scrape Airbnb data and analyze it using [Firecrawl](https://www.firecrawl.dev/) and the [Code Interpreter SDK](https://github.com/e2b-dev/code-interpreter) from E2B.

## Prerequisites

- Node.js installed on your machine
- An E2B API key
- A Firecrawl API key
- A Anthropic API key

## Setup & run

### 1. Install dependencies

```
npm install
```

### 2. Set up `.env`

1. Copy `.env.template` to `.env`
2. Get [E2B API key](https://e2b.dev/docs/getting-started/api-key)
3. Get [Firecrawl API key](https://firecrawl.dev)
4. Get [Anthropic API key](https://anthropic.com)

### 3. Run the example

```
npm run start
```

================
File: examples/scrape_and_analyze_airbnb_data_e2b/scraping.ts
================
//@ts-ignore
import * as fs from 'fs'
import FirecrawlApp from '@mendable/firecrawl-js'
import 'dotenv/config'
import { config } from 'dotenv'
import { z } from 'zod'

config()

export async function scrapeAirbnb() {
  try {
    // Initialize the FirecrawlApp with your API key
    const app = new FirecrawlApp({ apiKey: process.env.FIRECRAWL_API_KEY })

    // Define the URL to crawl
    const listingsUrl =
      'https://www.airbnb.com/s/San-Francisco--CA--United-States/homes'

    const baseUrl = 'https://www.airbnb.com'
    // Define schema to extract pagination links
    const paginationSchema = z.object({
      page_links: z
        .array(
          z.object({
            link: z.string(),
          })
        )
        .describe('Pagination links in the bottom of the page.'),
    })

    const params2 = {
      pageOptions: {
        onlyMainContent: false,
      },
      extractorOptions: { extractionSchema: paginationSchema },
      timeout: 50000, // if needed, sometimes airbnb stalls...
    }

    // Start crawling to get pagination links
    const linksData = await app.scrapeUrl(listingsUrl, params2)
    console.log(linksData.data['llm_extraction'])

    let paginationLinks = linksData.data['llm_extraction'].page_links.map(
      (link) => baseUrl + link.link
    )

    // Just in case is not able to get the pagination links
    if (paginationLinks.length === 0) {
      paginationLinks = [listingsUrl]
    }

    // Define schema to extract listings
    const schema = z.object({
      listings: z
        .array(
          z.object({
            title: z.string(),
            price_per_night: z.number(),
            location: z.string(),
            rating: z.number().optional(),
            reviews: z.number().optional(),
          })
        )
        .describe('Airbnb listings in San Francisco'),
    })

    const params = {
      pageOptions: {
        onlyMainContent: false,
      },
      extractorOptions: { extractionSchema: schema },
    }

    // Function to scrape a single URL
    const scrapeListings = async (url) => {
      const result = await app.scrapeUrl(url, params)
      return result.data['llm_extraction'].listings
    }

    // Scrape all pagination links in parallel
    const listingsPromises = paginationLinks.map((link) => scrapeListings(link))
    const listingsResults = await Promise.all(listingsPromises)

    // Flatten the results
    const allListings = listingsResults.flat()

    // Save the listings to a file
    fs.writeFileSync(
      'airbnb_listings.json',
      JSON.stringify(allListings, null, 2)
    )
    // Read the listings from the file
    const listingsData = fs.readFileSync('airbnb_listings.json', 'utf8')
    return listingsData
  } catch (error) {
    console.error('An error occurred:', error.message)
  }
}

================
File: examples/simple_web_data_extraction_with_claude/simple_web_data_extraction_with_claude.ipynb
================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping and Extraction with Firecrawl and Claude\n",
    "\n",
    "This notebook demonstrates how to use Firecrawl to scrape web content and Claude to extract structured data from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from firecrawl import FirecrawlApp\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up API Keys and URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL to scrape: https://mendable.ai\n"
     ]
    }
   ],
   "source": [
    "# Retrieve API keys from environment variables\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "firecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\")\n",
    "\n",
    "# Set the URL to scrape\n",
    "url = \"https://mendable.ai\"  # Replace with the actual URL you want to scrape\n",
    "\n",
    "print(f\"URL to scrape: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Firecrawl and Anthropic Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firecrawl and Anthropic clients initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize FirecrawlApp and Anthropic client\n",
    "firecrawl_app = FirecrawlApp(api_key=firecrawl_api_key)\n",
    "anthropic_client = Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "print(\"Firecrawl and Anthropic clients initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Scrape the URL using Firecrawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page content scraped. Length: 16199 characters\n"
     ]
    }
   ],
   "source": [
    "# Scrape the URL using Firecrawl\n",
    "page_content = firecrawl_app.scrape_url(url, params={\"pageOptions\": {\"onlyMainContent\": True}})\n",
    "\n",
    "print(f\"Page content scraped. Length: {len(page_content['content'])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare the Prompt for Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt prepared for Claude.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the prompt for Claude\n",
    "prompt = f\"\"\"Analyze the following webpage content and extract the following information:\n",
    "1. The title of the page\n",
    "2. Whether the company is part of Y Combinator (YC)\n",
    "3. Whether the company/product is open source\n",
    "\n",
    "Return the information in JSON format with the following schema:\n",
    "{{\n",
    "    \"main_header_title\": string,\n",
    "    \"is_yc_company\": boolean,\n",
    "    \"is_open_source\": boolean\n",
    "}}\n",
    "\n",
    "Webpage content:\n",
    "{page_content['content']}\n",
    "\n",
    "Return only the JSON, nothing else.\"\"\"\n",
    "\n",
    "print(\"Prompt prepared for Claude.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Query Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude response received.\n"
     ]
    }
   ],
   "source": [
    "# Query Claude\n",
    "response = anthropic_client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Claude response received.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Parse and Display the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"Just in time answers for Sales and Support\",\n",
      "  \"is_yc_company\": true,\n",
      "  \"is_open_source\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Parse and print the result\n",
    "result = json.loads(response.content[0].text)\n",
    "print(json.dumps(result, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

================
File: examples/sonnet_web_crawler/sonnet_web_crawler.py
================
import os
from firecrawl import FirecrawlApp
import json
from dotenv import load_dotenv
import anthropic
import agentops

# ANSI color codes
class Colors:
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    GREEN = '\033[92m'
    RED = '\033[91m'
    MAGENTA = '\033[95m'
    BLUE = '\033[94m'
    RESET = '\033[0m'

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")
anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")

# Initialize the FirecrawlApp and OpenAI client
app = FirecrawlApp(api_key=firecrawl_api_key)
client = anthropic.Anthropic(api_key=anthropic_api_key)

# Find the page that most likely contains the objective
def find_relevant_page_via_map(objective, url, app, client):
    try:
        print(f"{Colors.CYAN}Understood. The objective is: {objective}{Colors.RESET}")
        print(f"{Colors.CYAN}Initiating search on the website: {url}{Colors.RESET}")
        
        map_prompt = f"""
        The map function generates a list of URLs from a website and it accepts a search parameter. Based on the objective of: {objective}, come up with a 1-2 word search parameter that will help us find the information we need. Only respond with 1-2 words nothing else.
        """

        print(f"{Colors.YELLOW}Analyzing objective to determine optimal search parameter...{Colors.RESET}")
        completion = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1000,
            temperature=0,
            system="You are an expert web crawler. Respond with the best search parameter.",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": map_prompt
                        }
                    ]
                }
            ]
        )

        map_search_parameter = completion.content[0].text
        print(f"{Colors.GREEN}Optimal search parameter identified: {map_search_parameter}{Colors.RESET}")

        print(f"{Colors.YELLOW}Mapping website using the identified search parameter...{Colors.RESET}")
        map_website = app.map_url(url, params={"search": map_search_parameter})
        print(f"{Colors.GREEN}Website mapping completed successfully.{Colors.RESET}")
        print(f"{Colors.GREEN}Located {len(map_website['links'])} relevant links.{Colors.RESET}")
        return map_website['links']
    except Exception as e:
        print(f"{Colors.RED}Error encountered during relevant page identification: {str(e)}{Colors.RESET}")
        return None
    
# Scrape the top 3 pages and see if the objective is met, if so return in json format else return None
def find_objective_in_top_pages(map_website, objective, app, client):
    try:
        # Get top 2 links from the map result
        top_links = map_website[:2]
        print(f"{Colors.CYAN}Proceeding to analyze top {len(top_links)} links: {top_links}{Colors.RESET}")
        
        # Scrape the pages in batch
        batch_scrape_result = app.batch_scrape_urls(top_links, {'formats': ['markdown']})
        print(f"{Colors.GREEN}Batch page scraping completed successfully.{Colors.RESET}")
        
        
        for scrape_result in batch_scrape_result['data']:

            # Check if objective is met
            check_prompt = f"""
            Given the following scraped content and objective, determine if the objective is met.
            If it is, extract the relevant information in a simple and concise JSON format. Use only the necessary fields and avoid nested structures if possible.
            If the objective is not met with confidence, respond with 'Objective not met'.

            Objective: {objective}
            Scraped content: {scrape_result['markdown']}

            Remember:
            1. Only return JSON if you are confident the objective is fully met.
            2. Keep the JSON structure as simple and flat as possible.
            3. Do not include any explanations or markdown formatting in your response.
            """
        
            completion = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1000,
                temperature=0,
                system="You are an expert web crawler. Respond with the relevant information in JSON format.",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": check_prompt
                            }
                        ]
                    }
                ]
            )
            
            result = completion.content[0].text
            
            if result != "Objective not met":
                print(f"{Colors.GREEN}Objective potentially fulfilled. Relevant information identified.{Colors.RESET}")
                try:
                    return json.loads(result)
                except json.JSONDecodeError:
                    print(f"{Colors.RED}Error in parsing response. Proceeding to next page...{Colors.RESET}")
            else:
                print(f"{Colors.YELLOW}Objective not met on this page. Proceeding to next link...{Colors.RESET}")
        
        print(f"{Colors.RED}All available pages analyzed. Objective not fulfilled in examined content.{Colors.RESET}")
        return None
    
    except Exception as e:
        print(f"{Colors.RED}Error encountered during page analysis: {str(e)}{Colors.RESET}")
        return None

# Main function to execute the process
def main():
    # Get user input
    url = input(f"{Colors.BLUE}Enter the website to crawl: {Colors.RESET}")
    if not url.strip():
        url = "https://www.firecrawl.dev/"
    
    objective = input(f"{Colors.BLUE}Enter your objective: {Colors.RESET}")
    if not objective.strip():
        objective = "find me the pricing plans"
    
    print(f"{Colors.YELLOW}Initiating web crawling process...{Colors.RESET}")
    # Find the relevant page
    map_website = find_relevant_page_via_map(objective, url, app, client)
    print(map_website)
    
    if map_website:
        print(f"{Colors.GREEN}Relevant pages identified. Proceeding with detailed analysis...{Colors.RESET}")
        # Find objective in top pages
        result = find_objective_in_top_pages(map_website, objective, app, client)
        
        if result:
            print(f"{Colors.GREEN}Objective successfully fulfilled. Extracted information:{Colors.RESET}")
            print(f"{Colors.MAGENTA}{json.dumps(result, indent=2)}{Colors.RESET}")
        else:
            print(f"{Colors.RED}Unable to fulfill the objective with the available content.{Colors.RESET}")
    else:
        print(f"{Colors.RED}No relevant pages identified. Consider refining the search parameters or trying a different website.{Colors.RESET}")

if __name__ == "__main__":
    agentops.init(os.getenv("AGENTOPS_API_KEY"))
    main()

================
File: examples/turning_docs_into_api_specs/turning_docs_into_api_specs.py
================
# %%
import os
import datetime
import time
from firecrawl import FirecrawlApp
import json
import google.generativeai as genai
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Retrieve API keys from environment variables
google_api_key = os.getenv("GOOGLE_API_KEY")
firecrawl_api_key = os.getenv("FIRECRAWL_API_KEY")

# Configure the Google Generative AI module with the API key
genai.configure(api_key=google_api_key)
model = genai.GenerativeModel("gemini-1.5-pro-001")

# Set the docs URL
docs_url = "https://docs.firecrawl.dev/api-reference"

# Initialize the FirecrawlApp with your API key
app = FirecrawlApp(api_key=firecrawl_api_key)

# %%
# Crawl all pages on docs
crawl_result = app.crawl_url(docs_url)
print(f"Total pages crawled: {len(crawl_result['data'])}")

# %%
# Define the prompt instructions for generating OpenAPI specs
prompt_instructions = """
Given the following API documentation content, generate an OpenAPI 3.0 specification in JSON format ONLY if you are 100% confident and clear about all details. Focus on extracting the main endpoints, their HTTP methods, parameters, request bodies, and responses. The specification should follow OpenAPI 3.0 structure and conventions. Include only the 200 response for each endpoint. Limit all descriptions to 5 words or less.

If there is ANY uncertainty, lack of complete information, or if you are not 100% confident about ANY part of the specification, return an empty JSON object {{}}.

Do not make anything up. Only include information that is explicitly provided in the documentation. If any detail is unclear or missing, do not attempt to fill it in.

API Documentation Content:
{{content}}

Generate the OpenAPI 3.0 specification in JSON format ONLY if you are 100% confident about every single detail. Include only the JSON object, no additional text, and ensure it has no errors in the JSON format so it can be parsed. Remember to include only the 200 response for each endpoint and keep all descriptions to 5 words maximum.

Once again, if there is ANY doubt, uncertainty, or lack of complete information, return an empty JSON object {{}}.

To reiterate: accuracy is paramount. Do not make anything up. If you are not 100% clear or confident about the entire OpenAPI spec, return an empty JSON object {{}}.
"""

# %%
# Initialize a list to store all API specs
all_api_specs = []

# Process each page in crawl_result
for index, page in enumerate(crawl_result['data']):
    if 'markdown' in page:
        # Update prompt_instructions with the current page's content
        current_prompt = prompt_instructions.replace("{content}", page['markdown'])
        try:
            # Query the model
            response = model.generate_content([current_prompt])
            response_dict = response.to_dict()
            response_text = response_dict['candidates'][0]['content']['parts'][0]['text']
            
            # Remove the ```json code wrap if present
            response_text = response_text.strip().removeprefix('```json').removesuffix('```').strip()
            
            # Parse JSON
            json_data = json.loads(response_text)
            
            # Add non-empty API specs to the list
            if json_data != {}:
                all_api_specs.append(json_data)
                print(f"API specification generated for page {index}")
            else:
                print(f"No API specification found for page {index}")
            
        except json.JSONDecodeError:
            print(f"Error parsing JSON response for page {index}")
        except Exception as e:
            print(f"An error occurred for page {index}: {str(e)}")

# Print the total number of API specs collected
print(f"Total API specifications collected: {len(all_api_specs)}")

# %%
# Combine all API specs and keep the most filled out spec for each path and method
combined_spec = {
    "openapi": "3.0.0",
    "info": {
        "title": f"{docs_url} API Specification",
        "version": "1.0.0"
    },
    "paths": {},
    "components": {
        "schemas": {}
    }
}

# Helper function to count properties in an object
def count_properties(obj):
    if isinstance(obj, dict):
        return sum(count_properties(v) for v in obj.values()) + len(obj)
    elif isinstance(obj, list):
        return sum(count_properties(item) for item in obj)
    else:
        return 1

# Combine specs, keeping the most detailed version of each path and schema
for spec in all_api_specs:
    # Combine paths
    if "paths" in spec:
        for path, methods in spec["paths"].items():
            if path not in combined_spec["paths"]:
                combined_spec["paths"][path] = {}
            for method, details in methods.items():
                if method not in combined_spec["paths"][path] or count_properties(details) > count_properties(combined_spec["paths"][path][method]):
                    combined_spec["paths"][path][method] = details

    # Combine schemas
    if "components" in spec and "schemas" in spec["components"]:
        for schema_name, schema in spec["components"]["schemas"].items():
            if schema_name not in combined_spec["components"]["schemas"] or count_properties(schema) > count_properties(combined_spec["components"]["schemas"][schema_name]):
                combined_spec["components"]["schemas"][schema_name] = schema

# Print summary of combined spec
print(f"Combined API specification generated")
print(f"Total paths in combined spec: {len(combined_spec['paths'])}")
print(f"Total schemas in combined spec: {len(combined_spec['components']['schemas'])}")

# Save the combined spec to a JSON file in the same directory as the Python file
output_file = os.path.join(os.path.dirname(__file__), "combined_api_spec.json")
with open(output_file, "w") as f:
    json.dump(combined_spec, f, indent=2)

print(f"Combined API specification saved to {output_file}")

================
File: examples/visualize_website_topics_e2b/claude-visualize-website-topics.ipynb
================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Website Topics (Claude + Firecrawl + E2B)\n",
    "\n",
    "**Powered by [Claude 3.5 Sonnet](https://www.anthropic.com/news/claude-3-5-sonnet), [Firecrawl](https://www.firecrawl.dev/), and [Code Interpreter SDK](https://github.com/e2b-dev/code-interpreter) by [E2B](https://e2b.dev/docs)**\n",
    "\n",
    "Scrape a website with Firecrawl and then plot the most common topics using Claude and Code Interpreter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install e2b_code_interpreter anthropic firecrawl-py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "from firecrawl import FirecrawlApp\n",
    "import json\n",
    "\n",
    "# TODO: Get your Anthropic API key from https://anthropic.com\n",
    "anthropic_api_key = \"your-anthropic-api-key\"\n",
    "# TODO: Get your Firecrawl API key from https://www.firecrawl.dev\n",
    "firecrawl_api_key = \"your-firecrawl-api-key\"\n",
    "# TODO: Get your E2B API key from https://e2b.dev/docs\n",
    "e2b_api_key = \"your-e2b-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the FirecrawlApp with your API key\n",
    "app = FirecrawlApp(api_key=firecrawl_api_key)\n",
    "\n",
    "# Crawl a website\n",
    "crawl_url = 'https://python.langchain.com/v0.2/docs'\n",
    "params = {\n",
    "    'crawlerOptions': {\n",
    "        'limit': 5\n",
    "    }\n",
    "}\n",
    "crawl_result = app.crawl_url(crawl_url, params=params)\n",
    "cleaned_crawl_result = []\n",
    "if crawl_result is not None:\n",
    "    # Convert crawl results to JSON format, excluding 'content' field from each entry\n",
    "    cleaned_crawl_result = [{k: v for k, v in entry.items() if k != 'content'} for entry in crawl_result]\n",
    "else:\n",
    "    print(\"No data returned from crawl.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "## your job & context\n",
    "you are a python data scientist. you are given tasks to complete and you run python code to solve them.\n",
    "- the python code runs in jupyter notebook.\n",
    "- every time you call `execute_python` tool, the python code is executed in a separate cell. it's okay to multiple calls to `execute_python`.\n",
    "- display visualizations using matplotlib or any other visualization library directly in the notebook. don't worry about saving the visualizations to a file.\n",
    "- you have access to the internet and can make api requests.\n",
    "- you also have access to the filesystem and can read/write files.\n",
    "- you can install any pip package (if it exists) if you need to but the usual packages for data analysis are already preinstalled.\n",
    "- you can run any python code you want, everything is running in a secure sandbox environment.\n",
    "\n",
    "## style guide\n",
    "tool response values that have text inside \"[]\"  mean that a visual element got rended in the notebook. for example:\n",
    "- \"[chart]\" means that a chart was generated in the notebook.\n",
    "\"\"\"\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"name\": \"execute_python\",\n",
    "        \"description\": \"Execute python code in a Jupyter notebook cell and returns any result, stdout, stderr, display_data, and error.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"code\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The python code to execute in a single cell.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"code\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_interpret(e2b_code_interpreter, code):\n",
    "  print(\"Running code interpreter...\")\n",
    "  exec = e2b_code_interpreter.notebook.exec_cell(\n",
    "    code,\n",
    "    on_stderr=lambda stderr: print(\"[Code Interpreter]\", stderr),\n",
    "    on_stdout=lambda stdout: print(\"[Code Interpreter]\", stdout),\n",
    "    # You can also stream code execution results\n",
    "    # on_result=...\n",
    "  )\n",
    "\n",
    "  if exec.error:\n",
    "    print(\"[Code Interpreter ERROR]\", exec.error)\n",
    "  else:\n",
    "    return exec.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "client = Anthropic(\n",
    "    api_key=anthropic_api_key,\n",
    ")\n",
    "\n",
    "def process_tool_call(e2b_code_interpreter, tool_name, tool_input):\n",
    "    if tool_name == \"execute_python\":\n",
    "        return code_interpret(e2b_code_interpreter, tool_input[\"code\"])\n",
    "    return []\n",
    "\n",
    "def chat_with_claude(e2b_code_interpreter, user_message):\n",
    "    print(f\"\\n{'='*50}\\nUser Message: {user_message}\\n{'='*50}\")\n",
    "\n",
    "    message = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        system=SYSTEM_PROMPT,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_message}],\n",
    "        max_tokens=4096,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nInitial Response:\")\n",
    "    print(f\"Stop Reason: {message.stop_reason}\")\n",
    "    print(f\"Content: {message.content}\")\n",
    "\n",
    "    if message.stop_reason == \"tool_use\":\n",
    "        tool_use = next(block for block in message.content if block.type == \"tool_use\")\n",
    "        tool_name = tool_use.name\n",
    "        tool_input = tool_use.input\n",
    "\n",
    "        print(f\"\\nTool Used: {tool_name}\")\n",
    "        print(f\"Tool Input: {tool_input}\")\n",
    "\n",
    "        code_interpreter_results = process_tool_call(e2b_code_interpreter, tool_name, tool_input)\n",
    "\n",
    "        print(f\"Tool Result: {code_interpreter_results}\")\n",
    "        return code_interpreter_results\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2b_code_interpreter import CodeInterpreter\n",
    "\n",
    "with CodeInterpreter(api_key=e2b_api_key) as code_interpreter:\n",
    "  code_interpreter_results = chat_with_claude(\n",
    "    code_interpreter,\n",
    "    \"Use python to identify the most common topics in the crawl results. For each topic, count the number of times it appears in the crawl results and plot them. Here is the crawl results: \" + str(cleaned_crawl_result),\n",
    "  )\n",
    "print(code_interpreter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = code_interpreter_results[0]\n",
    "print(result)\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

================
File: examples/web_data_extraction/web-data-extraction-using-llms.mdx
================
# Extract website data using LLMs

Learn how to use Firecrawl and Groq to extract structured data from a web page in a few lines of code. With Groq fast inference speeds and firecrawl parellization, you can extract data from web pages *super* fast.

## Setup

Install our python dependencies, including groq and firecrawl-py. 

```bash
pip install groq firecrawl-py
```

## Getting your Groq and Firecrawl API Keys

To use Groq and Firecrawl, you will need to get your API keys. You can get your Groq API key from [here](https://groq.com) and your Firecrawl API key from [here](https://firecrawl.dev).   

## Load website with Firecrawl

To be able to get all the data from a website page and make sure it is in the cleanest format, we will use [FireCrawl](https://firecrawl.dev). It handles by-passing JS-blocked websites, extracting the main content, and outputting in a LLM-readable format for increased accuracy.

Here is how we will scrape a website url using Firecrawl. We will also set a `pageOptions` for only extracting the main content (`onlyMainContent: True`) of the website page - excluding the navs, footers, etc.

```python
from firecrawl import FirecrawlApp  # Importing the FireCrawlLoader

url = "https://about.fb.com/news/2024/04/introducing-our-open-mixed-reality-ecosystem/"

firecrawl = FirecrawlApp(
    api_key="fc-YOUR_FIRECRAWL_API_KEY",
)
page_content = firecrawl.scrape_url(url=url,  # Target URL to crawl
    params={
        "pageOptions":{
            "onlyMainContent": True # Ignore navs, footers, etc.
        }
    })
print(page_content)
```

Perfect, now we have clean data from the website - ready to be fed to the LLM for data extraction.

## Extraction and Generation

Now that we have the website data, let's use Groq to pull out the information we need. We'll use Groq Llama 3 model in JSON mode and pick out certain fields from the page content.

We are using LLama 3 8b model for this example. Feel free to use bigger models for improved results.

```python
import json
from groq import Groq

client = Groq(
    api_key="gsk_YOUR_GROQ_API_KEY",  # Note: Replace 'API_KEY' with your actual Groq API key
)

# Here we define the fields we want to extract from the page content
extract = ["summary","date","companies_building_with_quest","title_of_the_article","people_testimonials"]

completion = client.chat.completions.create(
    model="llama3-8b-8192",
    messages=[
        {
            "role": "system",
            "content": "You are a legal advisor who extracts information from documents in JSON."
        },
        {
            "role": "user",
            # Here we pass the page content and the fields we want to extract
            "content": f"Extract the following information from the provided documentation:\Page content:\n\n{page_content}\n\nInformation to extract: {extract}"
        }
    ],
    temperature=0,
    max_tokens=1024,
    top_p=1,
    stream=False,
    stop=None,
    # We set the response format to JSON object
    response_format={"type": "json_object"}
)


# Pretty print the JSON response
dataExtracted = json.dumps(str(completion.choices[0].message.content), indent=4)

print(dataExtracted)
```

## And Voila!

You have now built a data extraction bot using Groq and Firecrawl. You can now use this bot to extract structured data from any website.

If you have any questions or need help, feel free to reach out to us at [Firecrawl](https://firecrawl.dev).

================
File: examples/web_data_rag_with_llama3/web-data-rag--with-llama3.mdx
================
---
title: "Build a 'Chat with website' using Groq Llama 3"
description: "Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot."
---

## Setup

Install our python dependencies, including langchain, groq, faiss, ollama, and firecrawl-py. 

```bash
pip install --upgrade --quiet langchain langchain-community groq faiss-cpu ollama firecrawl-py
```

We will be using Ollama for the embeddings, you can download Ollama [here](https://ollama.com/). But feel free to use any other embeddings you prefer.

## Load website with Firecrawl

To be able to get all the data from a website and make sure it is in the cleanest format, we will use FireCrawl. Firecrawl integrates very easily with Langchain as a document loader.

Here is how you can load a website with FireCrawl:

```python
from langchain_community.document_loaders import FireCrawlLoader  # Importing the FireCrawlLoader

url = "https://firecrawl.dev"
loader = FireCrawlLoader(
    api_key="fc-YOUR_API_KEY", # Note: Replace 'YOUR_API_KEY' with your actual FireCrawl API key
    url=url,  # Target URL to crawl
    mode="crawl"  # Mode set to 'crawl' to crawl all accessible subpages
)
docs = loader.load()
```

## Setup the Vectorstore

Next, we will setup the vectorstore. The vectorstore is a data structure that allows us to store and query embeddings. We will use the Ollama embeddings and the FAISS vectorstore.
We split the documents into chunks of 1000 characters each, with a 200 character overlap. This is to ensure that the chunks are not too small and not too big - and that it can fit into the LLM model when we query it.
 
```python
from langchain_community.embeddings import OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)
vectorstore = FAISS.from_documents(documents=splits, embedding=OllamaEmbeddings())
```

## Retrieval and Generation

Now that our documents  are loaded and the vectorstore is setup, we can, based on user's question, do a similarity search to retrieve the most relevant documents. That way we can use these documents to be fed to the LLM model.


```python
question = "What is firecrawl?"
docs = vectorstore.similarity_search(query=question)
```

## Generation
Last but not least, you can use the Groq to generate a response to a question based on the documents we have loaded.

```python
from groq import Groq

client = Groq(
    api_key="YOUR_GROQ_API_KEY",
)

completion = client.chat.completions.create(
    model="llama3-8b-8192",
    messages=[
        {
            "role": "user",
            "content": f"You are a friendly assistant. Your job is to answer the users question based on the documentation provided below:\nDocs:\n\n{docs}\n\nQuestion: {question}"
        }
    ],
    temperature=1,
    max_tokens=1024,
    top_p=1,
    stream=False,
    stop=None,
)

print(completion.choices[0].message)
```

## And Voila!

You have now built a 'Chat with your website' bot using Llama 3, Groq Llama 3, Langchain, and Firecrawl. You can now use this bot to answer questions based on the documentation of your website.

If you have any questions or need help, feel free to reach out to us at [Firecrawl](https://firecrawl.dev).

================
File: examples/website_qa_with_gemini_caching/website_qa_with_gemini_caching.ipynb
================
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericciarla/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import caching\n",
    "from dotenv import load_dotenv\n",
    "from firecrawl import FirecrawlApp\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "firecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\")\n",
    "\n",
    "# Configure the Google Generative AI module with the API key\n",
    "genai.configure(api_key=google_api_key)\n",
    "\n",
    "# Initialize the FirecrawlApp with your API key\n",
    "app = FirecrawlApp(api_key=firecrawl_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data returned from crawl.\n"
     ]
    }
   ],
   "source": [
    "# Crawl a website\n",
    "crawl_url = 'https://dify.ai/'\n",
    "params = {\n",
    "   \n",
    "    'crawlOptions': {\n",
    "        'limit': 100\n",
    "    }\n",
    "}\n",
    "crawl_result = app.crawl_url(crawl_url, params=params)\n",
    "\n",
    "if crawl_result is not None:\n",
    "    # Convert crawl results to JSON format, excluding 'content' field from each entry\n",
    "    cleaned_crawl_result = [{k: v for k, v in entry.items() if k != 'content'} for entry in crawl_result]\n",
    "\n",
    "    # Save the modified results as a text file containing JSON data\n",
    "    with open('crawl_result.txt', 'w') as file:\n",
    "        file.write(json.dumps(cleaned_crawl_result, indent=4))\n",
    "else:\n",
    "    print(\"No data returned from crawl.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the video using the Files API\n",
    "text_file = genai.upload_file(path=\"crawl_result.txt\")\n",
    "\n",
    "# Wait for the file to finish processing\n",
    "while text_file.state.name == \"PROCESSING\":\n",
    "    print('Waiting for file to be processed.')\n",
    "    time.sleep(2)\n",
    "    text_file = genai.get_file(text_file.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cache with a 5 minute TTL\n",
    "cache = caching.CachedContent.create(\n",
    "    model=\"models/gemini-1.5-pro-002\",\n",
    "    display_name=\"website crawl testing again\", # used to identify the cache\n",
    "    system_instruction=\"You are an expert at this website, and your job is to answer user's query based on the website you have access to.\",\n",
    "    contents=[text_file],\n",
    "    ttl=datetime.timedelta(minutes=15),\n",
    ")\n",
    "# Construct a GenerativeModel which uses the created cache.\n",
    "model = genai.GenerativeModel.from_cached_content(cached_content=cache)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dify.AI utilizes the **Firecrawl** service for website scraping. This service can crawl and convert any website into clean markdown or structured data that's ready for use in building RAG applications. \n",
      "\n",
      "Here's how Firecrawl helps:\n",
      "\n",
      "* **Crawling and Conversion:** Firecrawl crawls the website and converts the content into a format that is easily understood by LLMs, such as markdown or structured data.\n",
      "* **Clean Output:**  Firecrawl ensures the data is clean and free of errors, making it easier to use in Dify's RAG engine.\n",
      "* **Parallel Crawling:**  Firecrawl efficiently crawls web pages in parallel, delivering results quickly.\n",
      "\n",
      "You can find Firecrawl on their website: [https://www.firecrawl.dev/](https://www.firecrawl.dev/)\n",
      "\n",
      "Firecrawl offers both a cloud service and an open-source software (OSS) edition. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the model\n",
    "response = model.generate_content([\"What powers website scraping with Dify?\"])\n",
    "response_dict = response.to_dict()\n",
    "response_text = response_dict['candidates'][0]['content']['parts'][0]['text']\n",
    "print(response_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

================
File: examples/website_qa_with_gemini_caching/website_qa_with_gemini_flash_caching.ipynb
================
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericciarla/projects/python_projects/agents_testing/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import caching\n",
    "from dotenv import load_dotenv\n",
    "from firecrawl import FirecrawlApp\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "firecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\")\n",
    "\n",
    "# Configure the Google Generative AI module with the API key\n",
    "genai.configure(api_key=google_api_key)\n",
    "\n",
    "# Initialize the FirecrawlApp with your API key\n",
    "app = FirecrawlApp(api_key=firecrawl_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data returned from crawl.\n"
     ]
    }
   ],
   "source": [
    "# Crawl a website\n",
    "crawl_url = 'https://dify.ai/'\n",
    "params = {\n",
    "   \n",
    "    'crawlOptions': {\n",
    "        'limit': 100\n",
    "    }\n",
    "}\n",
    "crawl_result = app.crawl_url(crawl_url, params=params)\n",
    "\n",
    "if crawl_result is not None:\n",
    "    # Convert crawl results to JSON format, excluding 'content' field from each entry\n",
    "    cleaned_crawl_result = [{k: v for k, v in entry.items() if k != 'content'} for entry in crawl_result]\n",
    "\n",
    "    # Save the modified results as a text file containing JSON data\n",
    "    with open('crawl_result.txt', 'w') as file:\n",
    "        file.write(json.dumps(cleaned_crawl_result, indent=4))\n",
    "else:\n",
    "    print(\"No data returned from crawl.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the video using the Files API\n",
    "text_file = genai.upload_file(path=\"crawl_result.txt\")\n",
    "\n",
    "# Wait for the file to finish processing\n",
    "while text_file.state.name == \"PROCESSING\":\n",
    "    print('Waiting for file to be processed.')\n",
    "    time.sleep(2)\n",
    "    text_file = genai.get_file(text_file.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cache with a 5 minute TTL\n",
    "cache = caching.CachedContent.create(\n",
    "    model=\"models/gemini-1.5-flash-002\",\n",
    "    display_name=\"website crawl testing again\", # used to identify the cache\n",
    "    system_instruction=\"You are an expert at this website, and your job is to answer user's query based on the website you have access to.\",\n",
    "    contents=[text_file],\n",
    "    ttl=datetime.timedelta(minutes=15),\n",
    ")\n",
    "# Construct a GenerativeModel which uses the created cache.\n",
    "model = genai.GenerativeModel.from_cached_content(cached_content=cache)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dify.AI utilizes the **Firecrawl** service for website scraping. This service can crawl and convert any website into clean markdown or structured data that's ready for use in building RAG applications. \n",
      "\n",
      "Here's how Firecrawl helps:\n",
      "\n",
      "* **Crawling and Conversion:** Firecrawl crawls the website and converts the content into a format that is easily understood by LLMs, such as markdown or structured data.\n",
      "* **Clean Output:**  Firecrawl ensures the data is clean and free of errors, making it easier to use in Dify's RAG engine.\n",
      "* **Parallel Crawling:**  Firecrawl efficiently crawls web pages in parallel, delivering results quickly.\n",
      "\n",
      "You can find Firecrawl on their website: [https://www.firecrawl.dev/](https://www.firecrawl.dev/)\n",
      "\n",
      "Firecrawl offers both a cloud service and an open-source software (OSS) edition. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the model\n",
    "response = model.generate_content([\"What powers website scraping with Dify?\"])\n",
    "response_dict = response.to_dict()\n",
    "response_text = response_dict['candidates'][0]['content']['parts'][0]['text']\n",
    "print(response_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
